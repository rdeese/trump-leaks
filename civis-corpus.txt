It's a truism bordering on a cliche to say that every day, there's more and more data being collected. As a data scientist, this is a great situation in which to find myself: so much data for training machine learning algorithms! But at a certain point, the amount of data out there starts to become overwhelming, and it's not clear what data will be most useful for solving a given problem. Say I want to build a model of political behavior, like how liberal or conservative someone is-what's the best dataset to go to when I am building this model? Demographic data? Data from previous elections? Social media? Recent campaign contributions? My goal as a data scientist is to live in a world where, when an interesting new problem walks in the door, I can quickly reach for the datasets that I think will be most useful for solving that problem.
I recently spoke at SciPy about one approach I find interesting for bringing order and scientific methodology to assessing which data is most relevant for a given problem. It's called Item Response Theory (IRT), and I think it's one of the best-kept secrets of quantitative social science.
Before we dig into IRT, some background would be helpful. Let me start by challenging the idea that predictive modeling is just about "building a model." At Civis we have to start upstream of the modeling step, because we're often solving problems that don't have ready-made datasets available for training machine learning algorithms-we have to collect, build, aggregate, clean, and assess these datasets ourselves. Only once that hard work is done, and we have the data in hand, does it make sense to start building models. But there are trade-offs. Once those first-pass models are built, we often find that they don't do as well as we would like, which usually means going back and trying different algorithms or tuning hyperparameters. However, our predictions aren't just a function of the algorithm, but of the the underlying data that was used for training.
Years of experience with building models has taught me two related heuristics about the importance of algorithms vs. data when building a predictive model:
With these simple rules in mind, a reasonable response to a poor model might not be to invest lots of time into the algorithm. Instead, we should revisit the training data! How do we really know that the data we're using captures the trend that we want to extract from it?
Put another way, the question I'm interested in is how we can develop the same intuition for our data that we have for our models. Is there an analysis that I can devise that studies the data itself, and allows me to know the best dataset for training a predictive model?
This brings me to IRT. IRT is a framework that comes originally from education and psychometrics. One well-known example is the SAT exams, where many students take a multiple-choice test which assesses their scholastic ability; another interesting use case comes from political science, where researchers rank-order congressional representatives in terms of partisanship (liberal/conservative) based on how they've voted on legislation. These examples, and IRT in general, can be thought of as latent trait models, where there is a characteristic of a person (scholastic ability, partisanship) which might be difficult to measure directly but which we can indirectly access via many measurements (test questions, congressional votes) performed by many examinees (students, representatives).
Here's the gist of IRT (for much more detail, 
 is an excellent reference). Say we have a student, with some unknown ability, who takes a test. We already have information on how easy or difficult each test question is, and we anticipate that the student will get easy questions correct and difficult questions wrong. Then we can imagine plotting each answer from the student, where the x coordinate is the question difficulty and the y coordinate is 1 (correct) or 0 (incorrect). If we fit a logistic curve to this data, we'll find a turn-on where, as a function of question difficulty, the student goes from usually getting questions right to usually getting questions wrong. The location of this turn-on is our estimate of student ability-for smarter students, this turn-on will be far toward the "difficult" end of the spectrum, indicating that a high-ability student mostly gets questions right, even the hard ones. Likewise for low-ability students; they will miss a lot of questions, even easier ones. And of course, there's a continuum in between those two extremes, where most students will end up.
Here's the second part of IRT: take that scenario and flip it around, so that we imagine having many students of known ability and we have a new question being added to the test. We want to quantify how easy or difficult the new question is. Again, we can imagine a plot of the resulting data, where we plot correct/incorrect on the y axis as before but now the x axis is the student ability. Similarly, if we fit a logistic curve to this data, there will be a turn-on, and the location of that turn-on now quantifies how difficult the question is. Difficult questions are questions that many students get wrong, even the high-ability ones; medium questions will be answered correctly by high-ability students and incorrectly by low-ability students, and easy questions will be answered correctly by most students of all abilities.
Of course, we are usually not in the luxurious position of quantitatively knowing student ability or question difficulty (or whatever latent traits are of interest), but we usually do have many students taking a test and many questions on the test. That allows us to build a big matrix, where each student is a row and each question is a column, so each entry in the matrix records whether student X gets question Y correct or not. There are several methodologies within the IRT framework that allows us to solve for both student ability and question difficulty at the same time, generally by estimating one, using those preliminary results to estimate the other, then using those estimates to refine the first set of estimates we made, and so on iteratively until the results converge and we have our answers.
Let's take our student-test example, but now the student isn't a person: it's a dataset. Specifically, it's the dataset used to train a predictive model. Then that model will be validated by using it to make predictions on our test set, which is a subset of the data which was NOT used in training but for which we do have the correct answers. If we keep track of which predictions the model gets wrong, and which ones it gets correct, we've basically made a vector of "correct" and "incorrect" predictions.
Now imagine that we can repeat that process many times, with a different type of data used each time. We're keeping the algorithm the same, as well as the dependent variable; the only thing that changes from one model to another is the training data.  Each model will get some predictions correct, and some of them wrong, and we can record this information for each model. With this process we can start to build a matrix of datasets and predictions, where would not be a huge stretch to compare each training dataset to a student, and each prediction to a test question. Then that matrix can be sent into the formalism of IRT, which will then produce best-fit estimates of prediction difficulty (based on how many and which datasets get a given prediction right/wrong) and dataset predictive ability (based on how many questions, and of what difficulty, are answered correctly by a given dataset).
These latent trait models are well known people-based data science but nothing in the math requires person-level analysis. So I'm suggesting to think outside of the box to use this type of workflow to understand the quality of the data you are working with.
You can find the slides 
 or watch my full talk below.Data science is an interdisciplinary endeavor. It requires a variety of technical knowledge, from computer science to statistics to data visualization. While conferences are a great way to gain such knowledge,
many conferences focus only on particular aspects of data science (e.g., 
 for machine learning, or

 for Python programming), and so one would have to go to several different conferences to keep up with the latest data science.
That's one reason why we at Civis were excited that 
 recently hosted its first conference in Chicago, 
. PyData is great because it brings together people who are developing and using data science tools to talk about both theoretical and practical challenges. Also, the scope is fairly broad (e.g., with talks on data engineering, distributed computing, machine learning, time series analysis, etc.), so it's a bit like going to several conferences all at once.
Civis played a big role in the conference: 
 and 
 helped organize it; several other people from Civis attended;
we 
 with some other great organizations; and we had four presentations (listed below).
We had a great time, learned a lot, and are looking forward to the next PyData Chicago conference. Hopefully, we'll see you there!
"Using Dask for Parallel Computing in Python" by 
 [
]
"Deconstructing Feather" by 
 [
]
"Implementing distributed grid search for deep learning using TensorFlow, scikit-learn, and joblib" by 
 [
]
"Evolutionary Algorithms: Perfecting the Art of 'Good Enough'" by 
 (who was doing an internship at Civis at the time) [
]Last week, the Chicagoland Entrepreneurial Center and the City of Chicago awarded our company, Civis Analytics, the 'Merrick Momentum Rising Star Award'. In receiving this award, we're proud to join co-winner @properties and previous Chicago winners including kCura, SMS Assist, Cleversafe and more.
For us, this award is near unbelievable.
This time just three years ago, our small founding team shared a couple of back tables at 1871. We had used laptops, a little seed funding, and a vision - but not much else. Many of the companies we started with didn't make it. We were lucky for many reasons, though, and we started with a brilliant team. That first year, we split our time between the short term, practical day-to-day business of a day-one company and the long-term construction of our data science technology.
Truthfully, though, we couldn't absolutely know if we'd break out or fall short. It was hard.
Today, three short years later, we support some of the world's greatest causes and companies with our software and insights. We work with some of the brightest, most creative people. We have two growing offices in Chicago and Washington, D.C.
We're honored by the public recognition for what we've achieved in so little time, the pace at which we've done it, and our commitment to Chicago - our headquarters home. We've worked hard to get this far, and we're grateful to 1871 and the entire Chicago startup community who continue to pave the way for great companies to succeed. We couldn't do it without you.
You can read more about the award on 
.Last Monday night, more Americans than ever before tuned in to watch the first presidential debate, and we at Civis did too. And while we were as interested in the back-and-forth between Clinton and Trump as anyone, we were most excited to see voters' reactions in the following days. That's when we turned to our social science team to field a survey so we could find out. As the survey responses rolled in, it became clear that voters believe Clinton trounced Trump, describing Clinton as 'prepared' and Trump as 'rude'.
Three fourths of respondents in our 
, 
 and phone surveys said they watched the debate. Of those who watched, Clinton was described as prepared, professional, and calm, while Donald Trump was described as unprepared, rude, and a bully.
However, it is no surprise that Democrats viewed Clinton's performance as presidential; Republicans, likewise, fell into party lines for the most part, describing Trump as strong (though even some said Trump was 'unprepared'!). Among voters who don't identify with either party, Trump does not fare much better: every single one of the ten words most associated with Trump are negative.
We were able to determine which words were most unique to each candidate scientifically by using a suite of tools built out by our Data Science Research and Development team to analyze survey responses. One such natural language model allows us to cut through the noise we typically see in word clouds to determine whether respondents are using different language to describe each candidate; when they are, our modeling identifies the specific words that are used to describe one candidate's performance and not the other's.
Of the 73\% of respondents who said they watched the debate, voters who have yet to make up their minds were the least likely to report watching.
As other polling directly after the debate showed, a majority - 61\% - of voters who watched believe Hillary Clinton won the debate. Only 21\% believe Donald Trump won and 18\% think neither won. In fact, only half of Trump supporters think their  candidate won and even one in five was willing to concede that Clinton routed Trump! Moreover, almost everyone who believes Trump won already supports him: fewer than one in ten undecided voters believe Trump won, versus half who said the same about Clinton. Clearly, voters think Clinton routed Trump in their first match-up.
What effect this debate will have on the polls, and eventually on the election, remains to be seen- we will be watching closely. In the meantime, Civis is going to continue developing new tools like the natural language processing ones featured here to cut through the noise and deliver insights to our clients.This post was written by 
. It highlights a project she worked on during her internship with the Data Science R\&D department in Summer 2016.
Neural networks have become the 
, but they are also notorious for their obscurity. Recently, we applied a neural network to the task of predicting numerical ratings for text-based consumer reviews, training the model to learn the ratings directly from the words in each review. It turns out that our model had decent prediction accuracy, but this wasn't our objective, especially since we already knew the ratings for these reviews. Instead, we wanted to understand why those particular ratings were assigned. Interpreting what a neural network has learned from data is a separate endeavor from applying it to make predictions. In this post, we'll explore some strategies for bringing the inside of a neural network to light, using our ratings prediction model to demonstrate.
For this task we used a simple feed-forward neural network called a multilayer perceptron. The below image shows its basic architecture; it has an input layer (in this case, the reviews text), one or more hidden layers, and an output layer that makes predictions (here, ratings). There are weights connecting each of these layers, and the values of these weights are updated during training as the model observes reviews and tries to correctly predict their ratings. Mathematically the network is represented as 
, where here the input 
 is, for each review, a vector of indicator features for words, the output 
 is for predicted ratings, and 
 is the nonlinear sigmoid function. What makes a neural network unique from other machine learning models is that the hidden layers allow it to learn complex interactions between variables (in our case, words) that can more precisely predict output.
For this work, we examined two different reviews datasets, both publically available. The first consists of 
 that appeared on imdb.com from 1998-2009. Ratings for these reviews range from 1-10, with 5 as the average rating. The second dataset is composed of approximately 15,000 reviews of Scotch posted in the /r/Scotch forum on reddit.com. The reviews were curated by this subreddit community into a 
 that contains the item names, ratings, and links to the review text. We then scraped the review text from the links. These reviews are rated from 1-100, with the average rating at 84. In both datasets we found that many reviews mentioned ratings within the text itself, so we replaced all numeric characters with a placeholder (0) to prevent the model from "cheating" using this direct mention. The trade-off was that a few meaningful numbers were replaced (e.g. "100 proof" scotch became "000 proof").
For each set of reviews, we trained a multilayer perceptron with one hidden layer to predict ratings based on review text. We recently 
 our implementation of this model, which was developed in Python with 
. Each review was tokenized into individual words using the 
 Python library. Words occurring at least 10 times across all reviews were added to the model's vocabulary. We also threw in phrases in addition to words by using the 
 library  to identify frequent collocations like "top shelf" and "thumbs up", as well as proper full names like "eva langoria" and "johnnie walker red label". These phrases were subsequently represented as individual words (e.g. "top shelf" became "top\_shelf"). To simplify the vocabulary, we converted all words to lowercase. We then encoded each review as a "bag-of-words", meaning the unordered set of all words weighted by how often each occurs in the review. Ultimately each review is represented in the model as a 1-D tensor (i.e., vector) with a dimension for each word in the vocabulary, whose values are equal to the number of times each word appears in the given review.
Given this structure to the data, we can interpret trained models by quantifying how each word influences predicted ratings. We will demonstrate some strategies for visualizing influence scores. There are three dimensions that characterize each of the examples we'll show:
It'll become clear what these dimensions mean as we go through each example.
We'll first focus on word influence at the 
 layer, meaning how words affect the ratings predicted by the model. In a model without a hidden layer like logistic regression, each word the model knows has a weight connecting it from the input layer to the prediction layer, so the influence of a word on the ratings is simply its weight value. In a neural network, on the other hand, the input weights for each word connect to the hidden layer instead of the output. A different set of weights connects the hidden layer and output layer, and these weights are based on the nonlinear values of the hidden units instead of the words themselves. By only looking at these weights, we don't know the overall influence of each word on the model's predictions.
It turns out that a bit of differential calculus can help solve this problem. In particular, we can compute the derivative of a layer in the network with respect to any layer below it. This happens to be a similar technique that was used to train the network, referred to as 
. You can think of this like retracing a path from the output back to the input to get clues about the original route. This path can traverse any number of hidden layers, where here it's just one. The derivative of the output layer with respect to the input layer tells us how sensitive the predicted rating is to each word in the review. Words with positive values influence the rating to be higher while negative words influence it to be lower, with the magnitude indicating how strongly they influence the rating in that direction.
Let's say we want to do an 
 analysis to find out which words have the greatest influence on ratings overall in the model. We're looking for a 
 score for each word: rather than asking the model to tell us what it knows about particular reviews, we're asking the model what it knows in general. To find this, we calculate the derivative of the network's output 
 with respect to the representation of a review 
: 
. In this case, 
 is a vector with only one nonzero value for a particular word, meaning that we're taking the derivative of a review that contains only one word. TensorFlow can actually calculate this in one call: tf.gradients(
, 
). The result is a vector with a derivative value for that particular word, which signifies its independent effect on predicted ratings. We repeat this for every word in the model's vocabulary. Here is a list of the 20 words that most negatively (red) and positively (blue) predict scotch ratings in our dataset.
For many words, it's probably intuitive why they appear on one side or the other of this list. Some of them generically express negative or positive sentiment ("awful", "fantastic"). Others have positive or negative meaning to people who are aware of scotch, or alcoholic beverages in general ("spoiled", "flat", "nicely balanced"). To those who aren't scotch experts, a few words may need additional context to understand their negative or positive influence ("stagg" and "coke" are two that stumped us, for example). This list is not sensitive to word frequency, so some words may have strong influence despite only showing up in a few reviews. For instance, only 86 of the 11832 reviews used to train the model discuss "nail polish remover", but these few reviews do have low ratings. Apparently this is enough for the model to know that scotch that resembles "nail polish remover" is bad scotch.
The word list shows which words are most influential in the model at large, but it is also helpful to visualize word influence for an 
 review. For instance, we can highlight influential words in a review and vary their color intensity according to their context-independent influence values.
We now get to see the context surrounding words in the word list, making it easier to understand their influence. This review suggests that "stagg" is a favorable brand of scotch, and "coke" is a mixer for less favorable scotch. Even though we're visualizing these words in context, the highlighting still reflects their context-independent scores in the word list. Alternatively, we can compute the 
 influence of words on the ratings for this particular review. To do this, we again take the derivative of the output 
 with respect to the review 
, 
, where now 
 is a vector of word counts for this review.
Here the focus shifts to the words that most influence the prediction for this specific review. The word "complex", for instance, has high context-independent influence, but it is not particularly important for assigning a rating to this review. If it were omitted, the rating would not change very much. In this analysis, the highlighted words are those that would impact the predicted rating if they didn't appear in the review. This is a little counterintuitive, because it seems like words with strong overall influence should have proportionate influence within a given review. Look at what happens when we truncate this review to the first few sentences.
Now the word "complex" becomes important, and the words "shipping" and "cayenne" increase their influence as well. This is why: in the original review, these words only contributed moderately to the positive rating since there were several other positive words. In this shortened review, there are fewer words with positive influence, so the remaining ones become even more influential.
One way to think about the hidden layer in a neural network is that each hidden unit corresponds to a topic dimension of the reviews. For instance, a movie review model could have a hidden unit for each genre as the topics (e.g. action, suspense, comedy), although this doesn't seem to be the case in our model. Instead of looking at how words affect the output, we'll now focus on the 
 layer to find the words that characterize the topic dimensions of these reviews.
In the previous analysis we took the derivative of the predictions with respect to the input to find influential words. We can do the same with any layer of a model, by taking the derivative of that layer with respect to the input layer. When there's only one hidden layer, its derivative will be proportional to the weights between the input words and each hidden unit, so we actually don't need to compute the derivative. The weights of each unit already tell us which corresponding words are most influential to that unit. Turning to the movie reviews dataset, the 
 and 
 analysis shows the top 20 words with the highest weights for each of the five hidden units in the model:
Neural networks usually have far more than 5 hidden units, but for the purpose of visualizing the units as topic dimensions in this post, we kept the model very small. Note that for the hidden layer, it only makes sense to show words with positive influence, since negative weights indicate the word does not activate that topic. Examining this list, we can start to see a few patterns among the words in each dimension. The yellow unit is almost exclusively composed of strongly negative sentiment words. The purple words seem to express a more moderate dissatisfaction. The red unit has more positive sentiment words, and the brown unit contains words suggesting surprise. Still, it seems difficult to assign an abstract topic label that summarizes the words in each dimension. Just as with the output analysis, we can try to disambiguate this list by viewing it from the perspective of specific reviews.
Interestingly, "johnny depp" is the most influential word in the blue topic, so let's look its influence within an 
 review.
Each word is colored by the unit where it has the highest 
 influence. From this perspective, it looks like all the topics are relevant to some degree in this review. Alternatively, to visualize the 
 influence, we can take the derivative of the hidden layer's output 
 with respect to this specific review 
: 
. The derivative is a 2-D tensor (i.e., matrix) with rows for input elements (i.e. word counts) and columns for hidden units, and we can use it to find the most strongly associated words for each hidden dimension.
Now each word is colored by the unit with the highest derivative for that word, so we only see the dimensions that are "turned on" (i.e. have high values) for this review. Despite that this review contains words with red and yellow context-indepedent influence, only the blue and purple units are activated. It's still a bit mysterious what these colors represent. We can possibly get some further clues by comparing the colors of different reviews side by side. Here's the context-sensitive analysis for a different review of the same movie (Sleepy Hollow):
And here's a comparison of their hidden activation scores, which confirms the second review has more yellow activation and less blue than the first review, as suggested by the context-sensitive highlighting.
While we can't draw any hard conclusions from this comparison, we can hypothesize. The similarity in the purple activation of the reviews may reflect their shared disappointment about the movie. Moreover, the model seems to have detected that the second review expresses more outrage, given its higher yellow activation. And perhaps the higher blue activity in the first review comes from its initial optimism about promising features of the movie ("johnny depp"?).
Neural networks are powerful models for text processing, but they don't naturally explain their decision-making to people. We demonstrated some straightforward ways to visualize how a neural network perceives words in the context of a prediction task. Future work will look for strategies to better explain how words influence the latent representation of text inside the model.Over the past few months, a team at Civis led by our CEO, Dan Wagner, has worked on researching and writing a report for Vice President Biden's Cancer Moonshot Initiative. In this report, we analyzed how cancer research might benefit from better use of data and analytics and provided recommendations around three topic areas:
This post will summarize our recommendations in the final category. You can read more about our work with the Moonshot 
, as well as read about our recommendations for 
 and 
. The full report is also 
.
Much of the research that informed our final recommendations took the form of in-depth interviews with a wide range of individuals in the field. Our conversations with researchers were particularly helpful in developing our recommendations about technical skill development. We spoke with scientists at different institutions, across various career stages, and in different subfields (basic research, clinical research, and biostatistics). It was immediately clear that the cancer research community contains talented, skilled, and driven individuals and that incredibly promising research is being done. However, we saw room for greater communication and interdisciplinary collaboration in the subfields that involve analysis of large volumes of data, particularly genomic data.
While there are many researchers in the field with biostatistics and data science experience, the specific technical skill sets needed to work with data at a large scale are not universally common. Individuals we talked to indicated that graduate biology programs are only recently starting to provide students with robust data science and statistical inference training that meets the needs of the big data era. Consequently, it can be hard for research teams without that training to take advantage of the large scale analysis made possible by large datasets like 
. This has an especially negative impact on research institutions outside of the leading universities and cancer research centers, and for teams that are not primarily computational biology labs. We spoke to one researcher who said there was an analytical bottleneck that results from a lack of skilled analysts with the technical capabilities to process data at the rate at which it is being generated.
We recognize that not everyone in the cancer research space needs to (or realistically should) be able to do advanced data science. Moreover, pure data science experience -- without any foundational understanding of biology -- is also not extremely helpful. The field needs subject matter experts in biology who are able to understand relevant data science at a high level, as well as biostatisticians who are able to work with large data and understand the biological context and implications of their analyses. This will allow teams to collaborate more effectively and broaden the pool of researchers who can benefit from and contribute to big data-driven cancer research.
In an ideal future, domain experts with training in biological methods and technical experts with training in biostatistics and data science would work together. At present, these two bodies of knowledge are somewhat siloed; researchers at either extreme of the spectrum don't always know what questions to ask of one another and how existing methods might help them answer their research questions.
We see two important steps to bring data science into all aspects of cancer research:
As more highly skilled people enter the cancer field, today and in the future, we also need to foster a better set of career options for them so they're incentivized to remain in the field.   Traditionally, individuals doing advanced biostatistics in an academic context have filled a supporting role that assists the work of other researchers. People we interviewed suggested that there should be more roles for full-time scientists in academia as "staff scientists" -- people with domain experience, more autonomy, and a viable long-term career path that is competitive with available alternatives in the private sector. These roles -- which already exist in other academic fields, like physics -- play an important role in distributing methodological knowledge among the field, advocating for data and correct use of statistical methodology, and doing independent work to advance the methodological cutting edge.
Additionally, the government should support and encourage partnerships between the cancer research community and the tech industry to encourage the sharing of information and skills between these two sectors.
Training the next-generation of biological researchers, creating new interdisciplinary partnerships, and establishing enduring and valuable channels of communication between the tech industry and the cancer research community will take time and effort. At the end of the day, the people working in this field will be the number one determinant of the pace at which new therapies are developed. Already, the cancer research space is filled with talented, hard-working researchers; the next critical step is to create the conditions for their success by enabling more researchers to participate in the cancer big data revolution.
This post was co-authored by 
, 
, 
 and 
.Over the past few months, we worked on a project that's a little different from our usual work: researching and writing a report for Vice President Biden's Cancer Moonshot Initiative. In this report, we analyzed how cancer research might benefit from better use of data and analytics. Our recommendations were organized around three major themes:
You can read more about the infrastructure recommendations 
, a note from our 
, or download the full report 
. Later this week, we'll share a deep dive on the people and skills recommendations.
In this post, I'll summarize some of the challenges we identified surrounding data sharing.
So why focus so much on data sharing?
Data sharing is very tricky because there are good reasons to constrain it. Laws governing sharing of sensitive health information, like HIPAA (Health Insurance Portability and Accountability Act) and other related laws and regulations at the state and federal level, are in place to protect patients. These laws spell out how personal health information must be kept private and secure by researchers, hospitals, and doctors. Even the strongest advocates of data sharing among us can agree that confidentiality of medical data needs to be reasonably and responsibly protected.
However, in our interviews with many hospital administrators, doctors, and researchers, we heard again and again how privacy laws are vague, difficult to interpret, and don't always keep the data as secure as one might think. HIPAA was passed by Congress in 1996 --before data science and affordable genomic sequencing were widespread. As the volume of digitized medical data has increased, data confidentiality rules haven't always kept up. The Cancer Moonshot provides a great opportunity to modernize privacy regulations for better data science in all kinds of medical fields.
One of the most compelling arguments for better data sharing came to us from patients and doctors themselves. In our interviews, we heard from patients both directly and indirectly (through their doctors) that they want to have the option to voluntarily contribute certain pieces of their own medical data for research. Currently, a patient legally has the right to open up their (anonymized) medical record up to accredited researchers for study, but in practicality there's no way to get their data to the researcher. 
.
A scalable process would require several components: software that can format and export the data, a database that could keep it both secure and accessible, and a clear legal framework surrounding ownership, liability, and rights for the data. We recommend several initiatives targeted at solving specific sub-problems related to patient data sharing, and to allow those initiatives the time and space they would need to find the exact best policy changes to make.
Data sharing isn't all about privacy regulations and laws, though. A second barrier to data sharing is that there isn't an agreed-upon set of conventions about how to format cancer data, particularly electronic medical record (EMR) data. Currently, there are many different ways that data gets recorded and formatted, which causes huge headaches every time that data needs to be aggregated or moved from one system to another. There is work underway right now to define a shared data format; we recommended that the government continue to encourage that work and enforce the standards once they are finalized.
Third, there's lots of great data that is being generated in research studies all over the country, from genomic sequencing to basic research to clinical trials. You can see where I'm going with this - that research data isn't always shared effectively either. To be fair to the researchers generating and using that data, it takes a lot of work to share data and there are not many incentives to nudge them in that direction. Researchers invest a lot of time, effort, and resources into developing their datasets, so it makes sense that they wouldn't want to release it before they are able to fully utilize it for publishing new findings. However, since much of the research in the United States is funded by the government, changes in government policy about data sharing could be attached to research grants, effectively saying "the government will pay for your research but the data must be made public after the work is done." (There are some policies like this already in place, so our recommendation is to expand their scope and enforcement, and to make it easier to access the datasets that are already being shared by consolidating them in a centralized location.) When this is coupled with 
 like the 
 for storing, finding, and distributing datasets, the incentives around academic data sharing will start to point in the right direction.
As I'm sure you now appreciate, it will take a lot of work to build the legal, regulatory, and incentive structure that we need for better data sharing. But it's worth it: our work with rich and diverse datasets at Civis Analytics has taught us that the degree to which we are able to make accurate, actionable predictions about the world depends on the quality of the data that goes into those predictions. Sharing data effectively is crucial for making the best use of all our resources in the fight against cancer, and we look forward to the advances that our brilliant researchers will be able to make once we start liberating cancer data.
This post was co-authored by 
, 
, and 
.
Later this week, we'll share a deep dive on the 
 recommendations.Working at Civis, I'm able to use my PhD both on data science research and development and real world problems. Recently, I teamed up with my colleagues, including our CEO, 
, to provide data recommendations for Vice President Biden's Cancer Moonshot initiative (you can read Dan's thoughts 
). Our efforts resulted in a full report that was presented at the 
 in June - you can download the report 
 - and this blog is the first in a three part series where we'll summarize our findings.
To start, we needed to understand the landscape. That meant wrapping our heads around the large and complex set of interacting pieces (hospitals, regulations, research programs, scientific context, historical background, etc.) of the fight against cancer. To this end, we conducted over 40 interviews with groups and individuals in relevant fields including doctors, researchers, data experts, legal experts, patients, hospital administrators, and many others.
Our objective was to understand how data was being used in the field, find where it was falling short of its potential, and form a set of policy recommendations to make cancer data science better. The final result is a report outlining three main recommendations:
This post focuses on computational infrastructure.
When we set out to describe the current computational infrastructure we expected that our primary task would be to find out about it, and how it could be made better. The reality was quite different: our interviews quickly made clear that "the cancer data infrastructure" was more like a very loosely confederated set of systems, each with its own peculiarities and nuances, and a major barrier to progress was the lack of coherence and interoperability among all these systems. There aren't technical interfaces between the systems, each one has a different user experience (interfaces, software requirements, technical assumptions about the users), they're expensive to build and maintain, and the people who build and maintain them don't always do a great job of communicating or coordinating their work. The data had simply grown too big and complex, too quickly, for any system to keep up with the deluge.
Instead of recommending small changes to an existing system, we decided that a major focus of our recommendations for the Moonshot effort should be building and maintaining the computational infrastructure for large-scale cancer data science. Our interviews made clear that there's a huge diversity of data that brilliant researchers are using to do great work, but generally that data isn't as useful as it could be because of all the problems outlined above. The government is in a unique position to coordinate this work, and take the lead in building it as a kind of public good for cancer research, in the same way that the government built the interstate highway system as a public good. The question then becomes, what should the system look like? How might we build it?
Let's start with a summary of the data, the driving factor for many of our recommendations about infrastructure. There are far too many datasets to summarize comprehensively; broadly speaking, though, we found that most of the data could be described as belonging to public or government organizations, academic research groups, or individual patients/hospitals (most commonly in the form of electronic medical records, or EMRs).
With all that in mind, we set to work thinking about what features of an infrastructure would be most important for making all this data as useful as possible. When we talk about "infrastructure", we think about it in two ways: first is the storage capacity to hold all the data, and the tools for getting data into and out of the system; second is the computational capacity to run intensive algorithms and analytics on that data to get insights out of it. Our experience working as data scientists has taught us a few general things about building infrastructure for working with big and complex data.
One very bright spot in our work was learning about the (NCI Genomic Data Commons) 
 a project that's been under construction for some time and officially launched in June. As it currently stands, the GDC hosts several large public datasets (TCGA and TARGET) and will be the future home of datasets from publicly funded academic research studies. This is a large collection of datasets, and while it's not comprehensive by any means, we think it holds great potential as a central gathering place for cancer-associated genomic data. The GDC also put an impressive amount of work into their GUI and API, and into harmonizing their data, making it appealing and useful from a user perspective. We know from building our own 
 that building a database is one thing, but building a tool that is intuitive, useful, and powerful is considerably harder. As a result of all this, many of our infrastructure recommendations build upon the foundation of the GDC, and explore what additional upgrades or capabilities should be considered.
If the GDC is a great start for a publicly-funded cancer data storage solution, its counterpart for cancer data analysis would probably be the 
. These three sibling programs-run out of the Broad Institute, Seven Bridges Genomics and the Institute for Systems Biology-provide a set of cloud-based analytics tools for cancer data. Each of these pilot programs work side-by-side with the GDC, offering researchers access to its datasets and computational resources. The Cloud Pilots are still growing, but we see potential for them to develop into widely-used tools for large-scale analysis of genomic data. Regardless of whether future data analysis tools arise out of these programs or elsewhere, we recommended that the government continue to look for ways to lead in data analysis infrastructure to grow alongside the data storage infrastructure.
All this infrastructure work will take some time. Even if we started today, it would take years to start seeing returns. That said, we've seen the payoff of these investments in the work that we do at Civis, where we've spent a lot of time and effort building a solid computational infrastructure that enables us to use data to solve really hard and important problems.
Infrastructure only gets us part of the way there, though. In our next blog posts we'll talk about the second and third pieces of the puzzle: how to think about data sharing and technical skills within the context of cancer data science.
This post was co-authored by 
, 
, and 
.
Infrastructure only gets us part of the way there, though. In our next blog posts we'll talk about the second and third pieces of the puzzle: how to think about 
 and 
 within the context of cancer data science.One aspect of the people-centered data science that we do at Civis is social network analysis.  Connections between people in the online or physical world can give us insight into how a population can be segmented or how information spreads in a community.
As we look to get the most extensive understanding of these network connections, we have turned to one open-source project, 
, a java-based tool for interactively exploring networks and creating attractive visualizations. For instance, below is a diagram we made to illustrate the polarization of the discussion around climate change on Twitter. There is a clearly separable block of users who are generally skeptical of climate science, and have relatively few connections to the larger community.
Gephi's GUI allows users to explore a network and determine the layout parameters that work best, but it doesn't allow the process to be automated once the best parameters have been determined. In furthering the usefulness of Gephi for our needs, we've created 
, a command-line application that provides access to core functionality for creating force-directed graph diagrams using Gephi.  And today, we're happy to announce that we're open sourcing it. The 
 allows users to access the underlying Java API for Gephi's layout and rendering algorithms, but using the toolkit effectively requires a good deal of knowledge about Gephi's internal data representation and processing model. Our goal with the GephiForceDiagramTool is to allow users to more easily automate the creation of force-directed graphs, providing a command-line interface that is simple to use, yet exposes the most important configuration options for diagrams of this type.
For instance, you can download a network data file representing adjacent adjective-noun pairs in the novel David Copperfield from M. E. J. Newman's 
, and create a diagram of the relationships using a relatively simple command:
Check out the 
 to learn more about all of these options. The resulting diagram looks like this:
We've open-sourced GephiForceDiagramTool under the 
, and made the source available on 
. We encourage you to use it in your own work, and we welcome community contributions.I've learned many things since I joined Civis. Least expected though is a new appreciation for simple linear regression and classification models. Shortly after I started, I was asked to evaluate a collection of modeling pipelines on a sample of typical prediction problems at Civis. Glancing at the list, I distinctly remember thinking the real task here was to measure how much better the tree-based models like XGBoost would be compared to some of the "lesser" models like logistic regression.
I set to work installing and running each of the tools on the list, collecting accuracy and runtime metrics. While most were written in Python, using Scikit-Learn, NumPy and SciPy, there was a two-stage model that used the R package glmnet and Scikit-Learn. Thinking it would be easier to have a tool that was written in a single language, I started looking for the Scikit-Learn analog of glmnet, specifically the cv.glmnet function in this R package. Unfortunately, I could not find anything written in Python that emulated the functionality we needed from glmnet.
Still, calling R from Python bothered me, so I started reading the glmnet code to see if I could copy and paste my way to success. As it turns out, the majority of the code was written in FORTRAN--a language so old it's from a time when computers had only uppercase letters
. Believe it or not, this was actually good news, I just needed to write some Python code to convert the data to the shape and format expected by the FORTRAN glmnet functions and everything would just work. A few weeks later and we had a Python implementation of glmnet. And today, we're open sourcing the code.
So what is glmnet and why do we like it at Civis? From the package documentation:
"
."
In less formal terms, glmnet fits lasso, ridge, and elastic net versions of linear and logistic regression. For the moment, we are just going to discuss lasso and its feature selection properties, but for a more complete explanation, see two of our favorte text books 
 and 
.
Feature selection - the process of identifying which features to include in a modeling task - is a challenging problem. Often, the problem is sufficiently complex that the solution to this is not at all obvious. My colleague Katie discussed feature selection 
 while building a model to predict whether water wells need maintenance. In this particular example, she used SelectKBest which selects the best features based on univariate statistical tests--a measure of how much each feature is related to the outcome of interest.
The lasso is another tool we can use to accomplish the same objective. The lasso works by fitting a normal logistic regression, but imposing a penalty on the absolute value of the coefficients of the model. This has the effect of shrinking the coefficients, some all the way to zero, meaning they are effectively excluded from the model, producing a sparse model. We could stop here if our task was simply producing a classification model, or we could use the selected features--those with non-zero coefficients--in a Scikit-Learn pipeline as one step in a modeling process.
I omitted one important detail above, and this turns out to be one of the key reasons we use glmnet instead of other lasso solvers. I mentioned that lasso imposes a cost on including features in the model, but not how we balance this against the desire to find an accurate model. This balance--the regularization strength--is actually a parameter of the model we must choose. Typically we run grid search, fitting many models to different values of the regularization strength. As you can imagine, this isn't particularly fast; one of the innovations made by the glmnet authors was making this process of fitting many models to different values of the regularization strength fast and efficient through some clever math tricks. A single call to the glmnet solver returns many model solutions for a range of values for the regularization strength (referred to as the regularization path). Scikit-Learn has a few solvers that are similar to glmnet, ElasticNetCV and LogisticRegressionCV, but they have some limitations. The first one only works for linear regression and the latter does not handle the elastic net penalty. They also require the user to supply the full sequence of regularization parameters whereas glmnet will determine a suitable sequence from the input data.
A brief example with synthetic data:
Generate some synthetic data:
Fitting a model should be familiar to anyone who has used Scikit-Learn. First, we instantiate the estimator, supplying any data-independent parameters. In our case, a few relevant options are:
: the lasso vs ridge strength, 1 being lasso, 0 ridge


: the number of cross validation folds for computing model performance
Additional options are documented in the class docstring. 
 will show this if you happen to be using the IPython interpreter or a Jupyter notebook. The 
 and 
 functions in the Python package are similar to 
 in the R package in that they run k-fold cross-validation to evaluate the model performance for each value of regularization parameter and automatically select the best.
Next, we call the fit method of the estimator, passing our covariates X and our labels y.
We can also plot the coefficient path. This is a plot of the coefficient of each feature in the model as a function of the regularization parameter. When this parameter is very big, all the coefficients are zero, as it's lowered, features start entering the model. Note, the x-axis here is reversed.
glmnet may be installed from PyPI and coming soon to conda-forge. The source code is also available on 
. We encourage you to try it for your projects, and welcome issues or pull requests.A few weeks ago I had the opportunity to speak at SciPy about how we use both Python and R at Civis. Why go all the way to a Python conference to talk about R? Was I fanning the flames of yet another Python vs R language war? No! It turns out arguing which language is "better" is not a very good use of our time. At Civis, we happily work in both languages--not only for our daily work solving data science problems, but also in writing tools. The how and why of this is what I discussed in my SciPy talk.
My colleagues at Civis come from many different academic backgrounds. The R\&D team I work on is composed of a physicist, an economist, two statisticians, and a civil engineer. Everyone at Civis learned the tools and techniques of data science in different contexts, some were formally trained in fields where R is the popular language, others Python, and some even used Matlab. Officially supporting a single language seems a silly choice in this context. Developing expertise in a new language takes quite a bit of time, throwing away skills gained over many years in academia or industry, allowing people to use the tools they are most comfortable with allows us to be generally more productive.
The other reason we use both languages is the availability of statistical tools/packages. In the course of solving a data science problem, we often encounter steps in the pipeline that require a tool that happens to be available in only one language. Our survey pipeline is a perfect example. Ensuring a random sample is representative of a desired universe requires a process called raking, and traditionally, Python has not been popular in the social sciences, so the weighting tools we use are only implemented in R. Surveys also include free-text response questions on occasion--you can see where this is going--R has not been popular with the NLP community, so the tools we use here are implemented in Python. And analyzing survey data is just one of the many steps in solving problems at Civis.
Stringing together workflows implemented in many different languages is challenging. Our data science platform helps in that we can submit a sequence of jobs/steps and the infrastructure takes care of calling each step in turn, passing the data emitted by the previous steps. But this is not the ideal situation for a two reasons. First, it is a bit fragile, more moving parts always means more opportunities for failure, and anyone who has done work in (or used) distributed systems knows this fact only too well. Second, this is inefficient. Switching between languages in the middle of a pipeline requires dumping writing the data to a format that can be read by both--csv files have been the least bad option. Not only are these expensive to parse, they also lose type information.
I've outlined the problem at Civis, but what is the ideal situation? In a perfect world, we would use any tool from any language. People comfortable with Python could do all their data analysis using Python and similar for R users. It turns out this is entirely possible to achieve, there are quite a number of projects that have started as cross-language tools: TensorFlow, XGBoost, and Stan to name a few of the popular ones at Civis. Porting or generalizing an existing tool is also possible, and we successfully did this with the R package glmnet.
For readers who write data science tools for others, consider making these cross-language from the start. There are a few ways to do this, but my personal favorite (and the bulk of my SciPy talk) focuses on using the C language and writing bindings/wrappers for Python and R using their respective C APIs. Both Python and R are actually implemented in C, making this the path of least resistance. Although C is a very old language, the community has made some tremendous strides in tooling the past few years. Gone are the days of obscure compiler error messages, both GCC and Clang (the most popular compilers) give nice messages (see the 
 for examples). There are also various "sanitizers" that help catch common bugs like memory leaks or undefined behavior (
).
Below we'll work through a small example, writing a function in C and making this callable from both Python and R. The code as well as the slides from my SciPy talk are on 
.
We are going to convert the following Python function to C:
This is what the same function looks like in C:
Notice it does not look all that different from the Python function. Of course, there are some type annotations and extra syntactic noise like curly braces and we have to keep track of the length of the array, but the overall logic is the same.
Next, we need to implement a Python binding, allowing users to call this function as they would any other Python function.
There's a lot going on here, but most of it is just boilerplate code that's part of any Python module. At the very top, we have a function that takes a Python object, checks that it's an array of the proper type, calls our tally function, and then returns the result. The rest of the code is the module definition, telling the Python interpreter the name of our tally function and its argument types.
The process for R is very similar:
There is a bit less code here because R doesn't have as extensive type system as Python, there are not really scalar types so we do not need to do the same level of checking/verifying user input as we do in the Python example above. The remainder of the code is roughly the same, we define a table of functions we want to be available in the R interpreter.
A real world example will necessarily be more complex, but the overall process is not all that difficult. A few things to keep in mind when writing cross-language tools that will make things easier:
Python and R are both going to remain important in the data science world, neither language is going to "win" the language war. What this means for tool builders and package authors is that we can't ignore the other language, building useful tools that have a big impact requires making them usable in both languages. One easy route is writing the bulk of the code in C or C++ and using the C API of both languages to provide native bindings.
You can watch my full talk below or view the slides on 
.
We'll be sharing my colleague Katie's talk from SciPy in the coming weeks. Be sure to check back to learn how to give your data an entrance exam.Recently, I participated in my first design sprint. For those who are unfamiliar with the concept, a design sprint is an intense, week long program where designers, engineers, and subject matter experts focus on creating a prototype of a new product and then testing the product on potential customers. In the past, I'd consulted on product development projects, but only in my capacity as an insights researcher, helping to collect feedback from potential buyers. This activity was my first venture into designing an actual product, and, though it was challenging, it was a lot of fun.
At Civis, we rely on staff of all levels, across a variety of teams to participate in design sprints, even those who aren't necessarily product designers. I loved the structure of the activity. I've always been fascinated by different time and activity management programs, so the sprint format was inherently interesting to me. Our sprints have a defined set of tasks to be performed in a defined time, allowing enough time for discussion but limiting time for avoiding tough decisions. Sprints also have planned work together and work alone time; it's really nice to have space to think quietly and then discuss with engaged collaborators.
As someone who doesn't design products, the act of designing a product is also surprisingly fulfilling to me. As a researcher, I usually work on a defined project that is used by stakeholders to make a key decision or two. A product is something that will be used by (hopefully) many customers for a long time. It's nice to feel connected to something more enduring.
Of course, being away from work for most of a week has consequences. Heavy, intense focus and concentration can be draining. And decision making is even more exhausting. Therefore, if I were to participate in another sprint (which I hope I do!), I would try to arrange my personal schedule to give me more mental free space. During the sprint week, we closed on a house and were coordinating move-in logistics. I nearly ran out of steam trying to manage it all. Luckily, I had my sprint teammates there to boost me up.
Overall, the best part of the sprint was our clickable prototype. We had something to show for all our work! I was surprised at how nervous I was to have testers come in to provide feedback on the product; I was so invested and hopeful they would love it. Sprint design is rewarding because there is something to show, get feedback on, and work to improve. I never saw myself as someone involved with product testing, but I'm really happy to have been included on this type of project. I learned a lot, worked with a great team, and now get to watch the product develop.
While this was my first experience in a design sprint, I loved the cross-functional collaboration that took place. I've also been happy to find out how I can get involved in unexpected projects and processes while continuing to contribute my skills and expertise to the survey science team.We love learning new things at Civis so we find ways-almost weekly-to teach our colleagues about things that we specialize in. From lightning talks to brown bags and many things in between, we thrive on learning from each other. With that learning culture in mind, I decided to host a brown bag during lunch a couple of weeks ago.
As Lead Designer at Civis, I'm focused not only on building easy-to-use data science products but also on building a design culture that keeps users at its core and leverages iterative implementation to adapt to their needs. There's a lot that goes into that internally, but it's really important to get outside of our own walls and get other opinions and expertise.
So I invited some design leaders I admire from the Chicago tech community to join us for lunch and discuss the evolving role of design in a technical company:
Big takeaways from the discussion:
The way we think about design is changing. Once viewed as merely a glossy finish on a product, design is now understood as a mindset that pilots how we create technology. Design is not a person or a singular role within an organization, but instead a process to approach problems and create solutions. We call this problem-solving process "
." Matt broke down the process into four elementary steps:
At General Assembly, Shilpa and Jason are training the next generation of design thinkers, teaching them the process above tools or discrete skills that change so frequently.
For a tech company (or any company for that matter) to be successful, it must embrace design thinking. At Spothero, design is "a core pillar to business and a conversation at every level and throughout everything we do," says Anthony Broad-Crawford. After all, products that aren't designed well-that aren't easy-to-use-will fail. Both Spothero and Context Media use 
 to iterate quickly and engage stakeholders across the company to transform an idea into a testable prototype in just five days or less. Sprints welcome non-designers to become active participants in the design process while constraining time, tools, and decision-making to get quick answers. General Assembly recognizes the importance of design sprints and uses them as their curriculum structure-some one-week projects, most two-weeks. (We use sprints at Civis, too-I'll talk about those in a future post.)
Our work at Civis revolves around the power of data. Data makes decisions less subjective, which helps with democratization. A really important thing to note when talking about data-driven design is that data isn't just numbers. Designers must balance quantitative with qualitative data-both must be incorporated and neither overemphasized. There are multiple ways of knowing and understanding human behavior and, ultimately, that is how we make great products.
This was our first internal panel and it was a great way for our team to understand how design plays into the entire product development cycle-and not just the finishing touches. I look forward to hosting more in the future.
Want to hear more from Alissa? She'll be teaching a UX workshop (including a design sprint!) on August 20-21 in Chicago. For any organization, growing your customer base, recruiting new employees, finding new donors, or expanding your user base can be daunting tasks. If you already have a good sense of what types of people your current users are, you might focus acquisition efforts on finding others who "look most like" those people, which can be done with a look-alike modeling exercise that identifies consumers most similar to your current customer base. While I could talk about that process more in-depth, I'm going to focus this post on how you can apply data science to target a different population - those who have expressed interest in or are good potential fits for your organization, but who don't necessarily "look like" your current user base.
Acquisition of interested prospects can be a more costly and risky undertaking than acquisition of "look-alikes." However, these acquisition prospects often lead to greater gains in the long run, as they allow you to expand into new markets, both geographically and demographically, in order to find new, untapped sources of users.
Finding these people might not seem like a difficult problem, but there's quite a bit that goes into identifying and reaching those who have expressed interest.
First, how do you define and measure interest in a product or service? Simply asking a person their level of interest for a product, service, or willingness to donate doesn't quite pass the full test for a variety of reasons: people don't always tell the truth, they may be unclear about what you are asking, or they might have biases based on past experiences, misinformation, or a host of other things. While some of these people might not express interest in a product or service when asked, you shouldn't be too quick to discount them as potential customers as some of their misconceptions can be changed with marketing and other advocacy tactics.
At Civis, we solve this problem with a complete data science workflow, identifying those that would be good targets for acquisition by surveying a sample of people who are within your organization's market, yet are not part of your current user base. In our surveys, we ask respondents a variety of questions - some of which are about expressed interest in your product, service, or organization - but others about their lifestyle, behaviors, and preferences that you believe would deem them a good fit as a user of your product.
Each of these individual questions are designed to capture slightly different populations, as they measure different concepts of prospective behavior. The challenge then becomes how do we combine these various measures into a single indicator of whether or not a person is a good acquisition target? We answer this question by using our data science platform to build models for all of these measures as predictors, creating an acquisition prospect score that is then a composite measure for whether or not a customer would be a good fit. This new composite measure takes into account not only stated interest and demographic information of the consumer, but also behaviors and preferences that would lead them to be an ideal prospect for acquisition.
By using this method of customer acquisition modeling you can unlock new opportunities for your business to expand and grow, rather than limiting marketing efforts to only consumers who look like your current user base or consumers who express interest in your product. Now, you can begin to reach past the low hanging fruit to tap new markets of consumers who are great fits - they just don't know it yet.With a culture of responsibility, transparency and acceptance, our attention to and appreciation of our employees is in the forefront of our minds. As a growing organization, we work hard but at the same time, we know people have lives outside of the office - from jetting off to explore a new place to major milestones like expanding their family. These are things in life we all cherish and we've created policies so that each employee can take the time they need to experience their life holistically.
Many of our employees are adventurers and world-travellers and our vacation policy was created with that in mind. Jonathan Alvares, who's taking a two-week long trip to Europe, said he's thankful for the "supportive team structures that [allow for us] to take the time we need." We expect and want our employees to take advantage of their vacation time. Akshaya Suresh, an applied data scientist who spent time recently in India with her family, said she "took a full week of vacation earlier this year because I was able to "borrow" from the future." Our policy is designed to be flexible and encourage you to use your vacation during the year.
To support our employees who are expanding their families, we created a flexible parental leave policy that all full-time employees have access to. We're proud to offer twelve weeks of continuous paid leave from the time of the birth for birth mothers and four weeks of continuous paid leave from the time of the birth along with an additional 20 days of working from home time within the first year after the birth for non-birth parents. We know that it's important to have work-life balance and to be able to connect with your family.
While we could go into the reasons we've created our policy, we thought it would be best to ask some of our employees what it has meant for their families:
Michael Heilman, Data Scientist
"I'm pretty sure having kids never gets easy, but the first few weeks are really difficult, and I'm really glad that I didn't have to worry about both family 
 work during that time. It really helped my family adjust, and since I didn't have to use up vacation days, we were still able to visit friends and family later in the year for the holidays."
Christine Campigotto, Applied Data Science Manager
"Truthfully, while I knew that parental leave was important, until I started thinking about having a child, I didn't put a lot of thought into it! I used to think parental leave was primarily about the parent - when are they ready to go back to work, when is the mother healthy again, etc. Now that I have my own future child to think about, I realize that parental leave is really about the baby - who will be there to take care of them every day and when do they go into daycare? I find myself caring much more about how my decisions will impact our baby, rather than how they impact me personally. I feel really good knowing that Civis wants me to feel comfortable and give me the resources and flexibility that I need to make the right decisions for my family. Everyone, from HR to my colleagues, has been nothing but positive and supportive."
Saranga Komanduri, Software Engineer
"Paternity leave allowed sufficient time for my wife and I to settle in with our new baby, without feeling rushed or pressured to return to work. The Civis parental leave policy also gave me the flexibility to take time as I needed to help support my family's needs."
We love hearing how our employees take advantage of our benefits and are always exploring new ways we can make Civis Analytics a great place to work!Behind the scenes at Civis, we take protecting client data and 
 very seriously. We invest substantial resources and Security/DevOps engineering time in technology and processes to protect our systems and client data. In addition to our own engineers, we rely on a variety of security technologies, including:
Being an analytics company, we have a soft spot for analytics products such as Sumo Logic. We use it to collect and securely transport security and application logs from networking devices, applications, laptops, 
 agents, and AWS EC2 instances to the Sumo Logic application. We then use Sumo Logic to search for issues and attacks, as well as provide necessary audit controls as we pursue 
 and 
 compliance and certification.
One such audit control is detecting when engineers login and logout of our AWS EC2 instances. Civis utilizes a group account called "civis-ec2" and unique SSH public/private key pairs for these Civis staff members in the civis-ec2 user's 
 file. This is done for simplicity of administration in DevOps playbooks and deployed EC2 instances, versus having to manage local accounts for every user on every instance.
However, the OSSEC agent login/logout events (below) sent to Sumo Logic do not by themselves contain the necessary user details to map the civis-ec2 user to the unique Civis staff member who created the SSH session. We do have two pieces of data to work with to determine who the user was and meet HIPAA and SOC2 compliance requirements: the user's SSH public key fingerprint in the login event and the sshd process ID (PID) in both events (all in bold text below).
To correlate the unique Civis user to the user that logged in as "civis-ec2", we'll use the list of all SSH public keys in our EC2 servers' 
 file. This file contains the full public SSH key for each user and their associated username in the SSH public key (e.g. jhollandcivis).
Using this data, we wrote a script to generate a nested if-then-else statement to use in a Sumo Logic query. The script takes each user's public key and generates the MD5 formatted fingerprint along with their associated username, and then creates the nested if-then-else format with the proper number of closing parenthesis (as shown below). The else condition "SSH Key Not Mapped" is then applied to the key\_name variable in the event a new key was added to the 
 file but is not mapped to a user in our sumo query.
Having a script that generates the nested if statement is useful as we add more staff and rotate keys.
We then use this statement within a larger query we'll run against our 
 logs in Sumo Logic:
Running this query in Sumo Logic, we get the following table that correlates SSH logins and subsequent logouts for the user "jhollandcivis". Note that each event contains the login and logout times, the user who logged in, source and target IP addresses, target hostname, and the duration of the SSH session (in minutes). Also note the fact that the login and logout events both have the same SSH PID value, thus correlating the login event to the logout event.
Now that we have this data, we can answer questions such as:
Additionally, we can see when a user logs into an EC2 instance with a new SSH public key our query does not know about (note the login\_key\_name value is "SSH Key Not Mapped"):
Finally, we'll want to know when this happens, so we schedule a query to look for "SSH Key Not Mapped" events and run it every 15 minutes and have it email us if more than zero (0) results are returned:
When we receive the email with the query results, we can run our script again to generate a new nested if-then-else statement with the new user's SSH key fingerprint and username, update our search query, and resolve "SSH Key Not Mapped" events.
Utilizing Sumo Logic, we now have the requisite audit information and auditing capability to react to security issues as well as satisfy our compliance requirements.It's no secret that it's becoming harder to field surveys in a way that's both accurate and cost effective. Fewer people have landline phones, and surveying cell phones comes with high costs and regulatory issues. Because of this, market researchers and pollsters are turning to online surveys. But online surveys don't come without challenges: it's hard to know where exactly your sample is coming from and whether you can trust it. We're changing that. Today, Civis is enabling you to conduct your own 
 using our powerful scientific methodology, developed by our data and survey scientists.
It's important to keep in mind that not all web surveys are created equal. In a recent report, the Pew Research Center compared the accuracy of nine web panel surveys against traditional benchmarks, and their analysis shows that results vary quite a bit. The 
, which uses a method of matched sampling and weighting using data from the census and other federal government sources. What this demonstrates is that it's possible to reap the benefits of the web as a means of conducting surveys while maintaining a high standard of accuracy through rigorous application of sampling and weighting techniques. This is no surprise to us: Civis uses a similar approach in our web surveys.
At a glance:
As we work to advance the state of the art in survey research, we'll also continue to work with the best tried and true methods. That means using phones still makes sense in a lot of cases and we continue to rely on them along with web surveys for our research, but now you can use that same methodology to conduct your own web surveys.At Civis, we're always looking for new ways to use existing data. Even though I primarily spend my time working with clients to collect new data through surveys, we know that some of the best data to solve our clients' problems may already exist, and often in unlikely places. A good case in point comes from my days before Civis, as a grad student in Political Science. At the time, I wasn't just short on beer money - many students who do quantitative social science are data poor, and looking for ways to squeeze the most insights possible out of whatever data they can find that has been collected by someone else.
The US Census Bureau does the mother of all data collection projects every ten years. It's in the Constitution: the federal government needs a headcount of every resident in the country. It's a massive undertaking, mostly done by mail. But not everyone responds in a timely manner; the Bureau must allocate its resources to make sure that it's able to reach everyone, even those who are hardest to count. As part of this effort, the Bureau publishes a 
 that compiles the past response rates of a given geographic area (that is, the number of people who return a census form by their deadline out of all those who received one). These numbers vary quite a bit across different neighborhoods within cities and across the country as a whole, and serve as a good predictor of which areas will require more follow-up in future censuses.
For the Census Bureau, this solves an operational problem: using past data to determine where to allocate resources for the next time around. For a data poor grad student, the Planning Database was a massive trove of data that I happened upon and had never seen anyone use - I just needed to find a problem to solve with it!
My idea (which resulted in a 
 with my then-colleague Benjamin Newman) was to think about the behavior of responding to the census not just as a means to solving an immediate organizational problem, but as an indicator of a broader phenomenon.
It's worth thinking about what responding to a survey actually entails: In order to respond to a survey, you must be able to find the person you want to talk to, but when you do, they have to be able and willing to respond themselves. People without fixed residences may be hard to track down in the first place and people who work long hours may be less inclined to put the energy into responding, but survey response often come down to motivation: the willingness to do something for no clear reward other than curiosity, social obligation or - more in this case than elsewhere - civic duty. What if Census response data could be used as a way to measure the inclination of a community's members to contribute to the collective good?
Harvard scholar 
 (among others) has argued that a well-functioning democracy isn't just a product of having regular elections, a well-designed Constitution and good laws: it requires a culture of civic engagement and community-orientation in its citizens, a quality Putnam calls social capital. Using survey data collected through Putnam's research, we were able to show that areas with higher census response rates were also significantly more likely to report that they trust their neighbors; that they interact with their neighbors more; they are more likely to believe that their neighbors will be likely to cooperate on common goals; and they are more likely to believe they can have an impact in bettering their community. Even controlling for common correlates of social capital (aggregate measures of age distribution, income, family structure, ethnic composition), places where people respond to the Census more are communities where people are more socially and civically engaged.
A measurement like this can be potentially very powerful, above and beyond the purpose for which the data were originally collected. Without administrative data like this, it can be cost prohibitive to field conventional surveys to measure these concepts at a level of detail that allows us to distinguish among individual neighborhoods and communities. And if we take the work of scholars of social capital seriously, then these response rates become a map of the health of communities and neighborhoods, distinct from measures we commonly use. If areas with a surplus of social capital are places where neighbors are most trusting, capable and willing to cooperate with one another to solve their problems collectively, then areas with low social capital are those that are least able to resolve their collective problems or organize effectively to seek government intervention. For sociologists and political scientists, it can help clarify the perhaps different ways that communities engage in collective action where mutual trust and civic engagement is rare. While it's sometimes accepted as a truism that racially diverse communities have less social capital, with detailed data at this level of aggregation, researchers could better assess how (and whether) some communities can better escape this fate. And by studying the ways in which communities with low social capital respond to crisis (for example, natural disaster), we may better understand the degree to which such intangible forces as social capital serve to make communities more resilient. And perhaps best of all for the researcher, it's rich data that's already paid for, free for anyone to put to another use.
A few things have changed since my grad school days: the beer budget isn't quite as tight as it used to be, and at Civis, I have access to cutting edge 
, my colleagues and I are continuing to raise the bar on existing analytics techniques, and of course we have access to a wealth of data far beyond what I had as a student. With all these assets, we're able to address this kind of research question with even greater precision and depth of insight. That said, we know there's always more data out there just waiting to be leveraged against the world's biggest problems - sometimes where you wouldn't expect it.During my interviews at Civis, I asked every Civi about workload. Against the guidance of interview tip listicles, career counselors, and even my own mother, I did not couch my questions in interview-speak. Instead of asking "What does your typical day look like?", hoping someone would give me a clue as to work/life balance at Civis, I was (almost too) straightforward: "When do you get here, when do you leave? How many hours a week would you say you work? How often do you take work home?"
I was afraid to end up in a workplace where being present was the proxy for working hard. Where loyalty and potential are measured in billable hours. Conversely, I was equally wary of a workplace where afternoons are spent staring at the clock, waiting for 5 o'clock. Eighty hours a week or eight hours a day; two mutually exclusive and unsatisfying options. As a data scientist, I viewed it as a simple classification problem. Collect as much data as possible in order to place Civis in the proper end node.
My interviewers didn't give me a consistent answer that day, because at Civis, we offer a flexible work environment. It's not about clocking in and out. For most of us, the work is what you make it. You end your day when the work is done to your satisfaction. I personally have worked a few late nights, but I do not begrudge them. I wouldn't be able to tell you how many 50+ hour weeks I've worked, because I honestly don't keep track. When a job doesn't measure your worth in hours, you stop measuring hours, and start measuring work you are proud of.
What I can tell you is: I cook or bake almost every day. I go out with friends. I read books. I've got more paid vacation time than I know what to do with. I take sick days when I'm ill and need to take care of myself. I work from home when I need to, though I prefer to come in to see the 100+ coworkers I have the joy to know. I graduated with my Master's degree and a gaggle of Cives showed up to toast me. I know if and when I choose to start a family, I will be supported 100\%.
Most importantly: I have meaningful, rewarding work. I'm challenged, and being challenged means I can't be an island. I get constructive feedback, my manager cares about me, and every single person I work with is immensely helpful. I'm happy I didn't take the advice of everyone else out there when I was interviewing. I was true to who I am and what I need. I've been fortunate to find at Civis, I'm given the autonomy I thrive on with an incredible amount of support when needed.In March, I got the chance to speak at O'Reilly's 
 about how we use Docker to power our data science work at Civis.  Now that we have the video of the talk, we thought this would be a great time to share it.
As data scientists, we inhabit an ever-changing landscape of languages, packages, and frameworks.  Given that it seems like something new pops up every day, it can be easy to succumb to tool fatigue. If this sounds familiar, you may have missed the increasing popularity of Linux containers in the DevOps world, in particular the rise of Docker. In the talk, I showcase Docker's many benefits to the data scientist, from making data science code and environments more portable and shareable to making the transition from development to production more seamless to giving data scientists a common basis for collaborating with software engineers.
I go through a total beginner's tutorial on containers, on Docker, and on the Docker tool ecosystem using a "real-life" end-to-end data science example. Well, sort of real-life, if you're interested in training a deep neural network to distinguish photos of pugs from photos of golden retrievers. You can find the slides 
, and all of the code is up on 
. Enjoy!Tuesday night all but officially certified Donald Trump as the Republican nominee for president. While Cruz and Kasich's announcements were surprising to many (including us), they confirm what the data has been telling us since we started polling on the Republican primary in August of last year: Donald Trump was always going to be the nominee.
The time-series above shows estimated (more on methods below) candidate support week-by-week over the primary race. The trends largely bear out what our data showed 
 back in August: Donald Trump won't fold.
For the more than 8 months of our polling, Donald Trump led the field, with the only exception of a six week period in mid-fall of 2015 when Carson lead Mr. Trump by, at his peak, 4 percentage points. At all other times, Trump earned more support, often by double-digit margins.
Even as Cruz picked up steam after winning in Wisconsin when we were conducting our final polls (which concluded on 4/17/2016), Trump was still leading Cruz, albeit narrowly. This does not even account for Trump's subsequent surge in the last few weeks with big wins in New York and Pennsylvania.
While Carson was the only candidate able to top Trump, his support was fleeting in a way that Trump's was not. By the end of 2015, Carson's once-nation-wide support receded to only a few districts. And by March, when Carson dropped out, he won a plurality of support in only one Congressional district.
As we observed Mr. Carson's bubble of support internally, we suspected it would burst. Our subgroup estimates showed Carson was buoyed by the same base that other candidates were drawing from: ideological conservatives and core Republicans. Because these voters had many similar candidates to chose from, their support was often transient. In contrast, Mr. Trump, we observed, drew together a demographically distinct 
 that he could rely on.
Moreover, because we can accurately estimate candidate support in the same geographies that delegates are awarded in, we were able to not only understand Trump's support levels, but also calculate how many delegates he would win in each contest. While conventional wisdom suggested he would do worse in highly Democratic areas, our estimates showed the opposite. Because these "Blue Zones" represent a disproportionate share of delegates, strong performance allowed him to rack up a substantial, and ultimately insurmountable, delegate lead.
In the end, Trump's unique coalition and a splintered field interacted with GOP delegate rules to benefit Trump in a way that our data science methods revealed early on.
While the first time-series above may look similar to aggregates of public polls, we take a different approach. First of all, we collect more data than any single poll. Since we first added the question, we have recorded conversations with over 20,000 self-identified Republicans among a broader sample of 70,000 adults in the United States.
However, polling aggregation sites average dozens of polls and tens of thousands of responses every month. In contrast, our methodology relied on only 2,375 interviews in March while generating more accurate and smoother results. By leveraging Civis proprietary data science tools, we can accurately forecast candidate support using much smaller (and cheaper) surveys than traditional methods, 
.
As we described in 
, we run tens of thousands of simulations using proprietary Bayesian algorithms that leverage all of that data to make estimates of survey responses. Previously, we used those to produce maps on the Congressional district level and the subgroup estimates that allowed us to understand Trump's unique coalition. However, the same techniques allow us to generate week by week estimates with greater accuracy than relying on surveys alone. On average, our margins of error are just +/- 2.8 percentage points - 30\% smaller than a standard survey of the same (weekly) size.
While the Republican primary may be over now, we will be waiting to see how this general election plays out.We're proud of the many great organizations we get to work with, such as our 
 with the 
. As a member of Civis's Survey Research team, I'm also proud of our commitment to best-in-class public opinion research, supported by rigorous scientific research. In addition to the data science 
 and 
, our surveys team designs and conducts research that results in thousands of phone and online interviews per week, from national surveys on public policy to precision targeted surveys of consumer habits in the markets our clients care about most.
Before these projects even begin, my role often involves working with clients to help them find the most effective way to find out what they want to know from the people they are most interested in. The work we did with The Hotline is a good case in point.
As Ola 
, The Hotline wanted to know a few things, including:
What they wanted to ask were great questions - which is why they were chosen - but there was still a little more work that needed to be done to get the most value for The Hotline from this project.
A researcher's first impulse might be to simply restate the situation and ask how much the respondent agrees:
Maybe you can see the problem: the question itself points out a 'discrepancy,' which suggests what the 'right' answer is - that is, what answer the person posing the question might expect. Decades of research in survey methodology have shown that, while people do have their own opinions, leading or loaded questions can influence the way they end up responding, in essence putting a thumb on the scale.
One improvement could be to ask about several scenarios separately, and ask how much the respondent supports the person in question being legally allowed to purchase a gun:
And yet, this is also potentially problematic. We could only allocate two questions to The Hotline and more survey time means greater expense. We also have to remember the respondent might just get tired of answering what seems like the same question over and over, paying less attention to the task or drop out altogether. But maybe the biggest problem, as prior research has shown, is that people's answers to earlier questions may influence their response to later questions. If I agree to the first of four similar questions, I may be more likely to say I agree to later questions, even if they are slightly different.
To resolve both of these problems, we decided to randomly pick one of the three questions above to ask, or a more 'neutral', non-violent scenario:
Instead of posing these questions to everyone, each person was randomly given only one of these four (either A, B, C, or D). This resolves the problem of question order because each person only responds to one question. It also resolves the problem of question framing because it allows us to compare directly those people who received a question about an abusive spouse and those who received a question about an abusive unmarried partner. Because the questions were randomly assigned to each person, we can be sure that there are no significant differences between these groups of people, which allows us to make an 'apples-to-apples' comparison. And because we included a 'neutral' control group, we can measure how much more or less a person supports restricting gun rights relative to their existing preferences about gun rights.
Now that we've conducted the survey, asking the right questions, I'll provide a bit more insight into how we analyzed it.
To analyze the data we collected, we divided the respondents into two groups: those who said they were opposed to the individual being allowed to buy a gun and those who did not (eg. those who said they were supportive or had no opinion). Then, controlling for other relevant variables, like individual's gender, age, geographic location, and race, we calculated the overall impact of each scenario on an individual's likelihood to say they were opposed. We also apply data science, using Bayesian techniques, to recover the true impact of each scenario--exactly how much of an impact it had on opinion.
In other words, we are able say that, relative to the baseline of the verbal fights question, people are 13.7\% less likely to say they support a person who is convicted of stalking from being allowed to purchase a gun and 17.9\% less likely to say they support a person convicted of abuse. Without this design we would have been unable to draw this conclusion.
So instead of just asking a question, it's critical to think about what you are trying to understand, have a point of comparison, and use data science to understand the true impact.I recently joined the team at Civis Analytics as our Director of Government Analytics and I'm excited to bring my background in information technology, analytics, and city planning to help government organizations become more data-driven. For the past ten years, I've been implementing large data analytics projects to improve the performance of programs at Federal government clients and I'm looking forward to introducing government agencies to the power of the 
 and our team of data scientists.
At Civis, our primary unit of analysis is the person. This means that the software we've developed and the insights we're best suited to provide fit perfectly with local government strategy, since local government is ultimately about serving people. Many local governments are already using data analytics to understand the constituents who reach out to them. Our focus at Civis has often been helping government find the citizens that are not interacting with government, and help give them the information they need to make the best decisions for themselves and their families.
This people-centered approach fits in perfectly with the new drive across government for further adoption of 
. Behavioral economics involves using small, low-cost changes in program design that can have huge impact, and that don't involve using the heavier-handed levers of policy, regulation, or economic incentive. For example, a text message reminding college-accepted high school graduates to complete tasks such as completing financial aid forms and taking placement tests led to an 8.6\% increase in the number of low-income students successfully enrolling in college, compared to those who received no text reminders.
 
 at employers who ask employees to "opt out" of their 401k plan are much higher than those who are asked to "opt in". These small changes in program design and outreach can have profound changes in people's lives, at very little cost to the government.
For these types of interventions to succeed, they need to reach the right people, as quickly and efficiently as possible. This is where Civis comes in. Predictive modeling has been used for many years in government to identify patterns that identify fraud, risk, or lawbreaking. However, the next step is using analytics to proactively identify citizens who simply need additional information, outreach, or tools to reach their goals or interact more efficiently with government.
Our predictive models help identify which people organizations should be reaching out to, including government organizations. For example, we have helped a state agency in their efforts to conduct outreach to families who could afford to, but are not using tax-advantaged methods to save for college, and helped cities and states 
 after the advent of the Affordable Care Act. Similar methods can be used for many other applications, such as ensuring that veterans are aware of their benefits, identifying students who might be at risk of dropping out of college, or identifying taxpayers who need to file a particular form but may not realize it. Combined with our state-of-the-art 
 and experimental design services, we can ensure that the the right message gets to the right people.
Although the speed of change can be slower in the public sector than it is in the private one, the public sector has seen a recent increase in interest in using more data-driven decision-making. Government has also seen a recent influx of talented data analysts with an interest in data science and exposure to the types of models and software tools we, at Civis, use every day. Agencies are getting savvier about data analytics all the time - they don't just want to a see a report with results, they want to see what makes a model tick under the hood.
Government is more interested than ever in working with the data themselves, and having "self-service" access to analytics products - which is just the way we like to work at Civis. For this reason, we share our data science tools, including the 
, with our clients. We are looking forward to continuing to provide the data science know-how and software tools to help government agencies at all levels really make an impact.In December, we invited nonprofits to 
 by giving away two questions on our weekly national representative survey of 2,500 Americans.
While we received many excellent submissions from great organizations, making it difficult to choose our winner, Civis Analytics selected the 
 (The Hotline) as our contest winner. The Hotline wanted to gauge public opinion on three key policy areas:
We believe that with access to data on national opinion toward these policies, The Hotline can more effectively advocate for policy changes which will protect victims of domestic violence.
And the results are fascinating:
The Hotline provides free, confidential support 24/7 to people affected by domestic violence and operates 
, a project focused on teens and young adults, in partnership with 
. The Hotline also advocates for policies that seek to protect individuals from domestic violence, including reducing gun access for individuals convicted of domestic abuse, as well as early childhood education on healthy relationships.
Given its mission, The Hotline wanted to understand more about public opinion on a federal gun policy loophole, gun access for individuals convicted of stalking, and the appropriate time to educate children about healthy relationships. First, some background: 
 prevent certain perpetrators of domestic violence from having access to firearms. However, this protection is not in place when the victim is someone who is dating, but not living with their abuser, even if the victim has the same kind of protection order or the offender is convicted of the same misdemeanor crime. This discrepancy - the "boyfriend gap"- is a loophole in federal law that domestic violence advocates would like to close.
Additionally, current federal laws do not prevent people who are convicted of misdemeanor stalking from possessing or purchasing firearms. Many people have the erroneous belief that stalking does not pose a major threat. However, research has found that 
.
Our survey for The Hotline consisted of two questions: a gun control question and a question about the appropriate school level at which to teach children about healthy relationships. You can read more about the 
.
Overall, most people are not supportive of granting gun access to perpetrators of domestic and dating violence.
Nationally, most people support limiting the gun access of perpetrators of domestic violence. However, some counties feel this way more strongly than others. Urban counties, the Midwest, and the Atlantic coast are especially in favor of these restrictions.
Civis' polling found that nationwide, public opinion does not support the "boyfriend gap". People were equally likely to say they opposed a person convicted of domestic abuse being allowed to purchase a gun when the victim was described as a spouse or a non-cohabiting dating partner.
Republicans and Democrats generally agree on gun access restrictions for individuals convicted of domestic abuse and misdemeanor stalking.
While we do see a big partisan difference in our "control" scenario--only 38\% of Republicans favor stricter gun control against the person in the baseline "verbal arguments" scenario, compared to 71\% of Democrats--this partisan split largely disappears when we ask specifically about domestic abuse situations. This shows that while Republicans are more supportive of gun rights in general, they largely agree with Democrats about gun access in the context of a domestic abuse scenario.
While most individuals believe children should be educated about healthy relationships in elementary school, some people feel this education does not belong in school at all. This map shows counties in the US shaded by the percentage of residents  who say that education about healthy relationships doesn't belong in school. Those in the Midwest, central South, and rural counties are more likely to say this education doesn't belong in schools; In more populous counties and the coasts, individuals are more likely to believe this education does belong in schools. Note that in most counties, doesn't belong is not the most likely response.
We're excited to see how The Hotline uses this data to pursue its goals of empowering survivors of domestic violence and changing the conversation around domestic violence and dating abuse. Equipped with these findings, they will continue to shape the national conversation about domestic violence and advocate for effective reform.
These results are based on a national phone survey of adults in the United States that Civis Analytics conducted from 2015-12-28 to 2016-01-08 with 3953 individuals. Respondents are sampled from nationally representative voter and consumer files. Results were weighted to the national adult population of the United States.
This post was co-authored by 
.
 Because The Hotline was interested in support for gun control in a variety of domestic violence contexts, we split our first question into four different hypothetical scenarios and then asked survey respondents whether they supported restricting the right to purchase guns in the given scenario. Our variations tested a spousal abuse scenario, a dating partner abuse scenario, a stalking scenario, and a verbal fight scenario, which serves as a baseline, or "control" against which to compare the other scenarios.
 All respondents heard the following education question:
People disagree about the right time to start teaching children about abusive relationships, including both emotional and physical abuse. Do you think this instruction should begin in elementary school, middle school, or high school, or do you think it doesn't belong in school at all?Data has been an integral part of The Hive since we launched in the fall of 2014. We knew that our efforts had to go beyond simply raising awareness of the global refugee crisis. In an effort to get new people to engage, we knew we needed to be strategic about who we targeted, how we reached them, and what we were asking people to do. We teamed up with Civis Analytics right away - and asked them to help us think and act like the Obama campaign: engage individuals, understand what they value, and activate them beyond just giving money. Smart data was the foundation for all of that.
This was an entirely new way of doing things for UNHCR - and arguably for the non-profit sector as a whole - so it took time for the organization to appreciate the impact individual-level data can have across the organization. We had to start by mapping out what exactly it would mean to use predictive modeling, so people were clear on what to expect. Once we completed that, we were able to start the multi-step process to develop the data and transform the way we target and engage audiences who aren't currently involved in efforts related to the refugee crisis.
We started by analyzing our current donors. From our current donors, Civis created lookalike models that gave us an appreciation for who was already responding and taking action to support refugees - and, of course, the much larger universe of people who were not yet engaged. We also conducted a national survey to measure people's knowledge and support of refugee aid, along with a host of related issues, and Civis used that information to build models that explored the population beyond the the lookalikes - what they know about refugees, what they might need to understand better in order to be motivated to take action, and similar.
We started all this work when awareness and interest in the global refugee crisis was pretty limited - and certainly confined to only people who were already deeply knowledgeable on the subject. Then, at the end of last summer, that all changed. The number of refugees flooding out of Syria into Europe exploded. Heart-breaking pictures of a young refugee who drowned trying to make it to safety suddenly made headline news and took over social media channels. All of a sudden, the global refugee crisis was the top story all over the world. President Obama challenged Americans to step up and help. The Pope said it was our moral responsibility as a society to aid refugees. The attention and the interest in the refugee crisis was higher than ever before.
We had to move quickly if we wanted to use this spike in interest to engage more people - but we knew that the traditional methods and messages wouldn't be enough. And we didn't have all our new ways of approaching this challenge figured out. While we had predictive data, we didn't have a lot of data about what people thought about the crisis or a baseline around engagement in the US. We needed to understand what we should say and the expected response. It was clear that Americans cared about helping refugees, but we needed to know more so we could determine how to get people more involved, to take more meaningful actions. We turned to survey research and rapid-response message testing, applied the data we had to test everything we could think of - so that we were able to test efforts we thought could be most effective, and learn the most in the shortest period of time.
Everything is informed by the data, both directly and indirectly. We have an unprecedented level of sophistication that we can apply - who we target, how we position issues related to the refugee crisis, and the ways we work with partners. Beyond our specific efforts:
And with all of this, we've been able to improve our organizational capacity and shift the way we think about our challenges. We're looking at individuals - human beings - not donors, or advocates. We can do a much better job tapping into what we know people are already doing, or comfortable doing, instead of trying to compel people to do what we think benefits our organization best. Our cause is important, but we need to think more about the people we are talking to - understanding who they are and what motivates them to ultimately engage them and support our cause.
We were making assumptions based on our gut - and they were pretty good. While the data validated our assumptions, it also revealed a whole new set of opportunities. It uncovered some surprising untapped geographic markets, hotspots in the parts of the country that we had never thought of, groups of people who don't fit the stereotype of those who would have been our targets.
Beyond target audiences, the data has expanded the ways we think about how to engage people. Instead of relying on the existing messages - the ways you commonly hear about the refugee crisis from a nonprofit or through media coverage - we now had messages that have been tested and verified by data science. Instead of speaking about the refugee crisis as an emergency situation, or telling stories of the horror refugees face or hope they retain against all odds, we can present issues in ways that Americans understand, connect with on a more personal level, or make sense of through an experience they can appreciate. 
All of these insights continue to show how data enables us to target more efficiently and effectively.
It's key you don't try and do it on your own. We are one organization, with incredible insights - but still only know some of what we should about the audiences we want to engage. The data gets better when there's more of it. So we want to collaborate and learn together, not just with other non-profits, but with brands and tech companies. The people we want to engage are more than just potential donors or advocates to a social cause, and we need to think about all the different elements of their life that are related to what we are asking them to do. It's also important that we know the data doesn't solve everything on its own - on our side, as an organization, we need to recognize that the data forces us to change our approach. If we target new people, with better messages, but only offer the same options to take action as always, we shouldn't expect a different result. The data serves as the foundation for a totally different way of operating, so we also have to think about what changes about our approach to everything from staffing to organizing to engaging individuals.
Data is a whole new way of thinking about engaging people and now's the time to take a data-driven approach for your organization.As a woman in tech, I was psyched to join my colleagues Katie Malone, Jenna Colazzi, Emily Stephens, Kate Pham, and April Chen at the 
, the world's leading gathering of women technologists hosted every year by the 
. The conference included talks by the likes of YouTube CEO Susan Wojcicki, robotics expert Professor Manuela Veloso (my personal favorite), and United States CTO Megan Smith.
While the conference was last fall, I wanted to share our recently released findings and analysis of the conference attendees. As part of our sponsorship of the conference, we had the opportunity to work with the Anita Borg Institute to evaluate the impact of GHC on the 11,702 attendees. And the findings are fascinating. You can see the full report below, but here are some highlights:
In an effort to create a complete view of each attendee's experience at GHC, we fielded before and after surveys to analyze the conference's impact on attendees' commitment to tech, their professional networks, and their attitudes toward their careers.
GHC is named in honor and memory of 
, one of the most accomplished computer scientists. Grace Hopper was a United States Navy Rear Admiral and one of the first programmers of the Harvard Mark 1 computer, used during the latter part of World War II. She invented the first compiler for a computer programming language and is credited with popularizing the term "debugging" after literally removing a moth from a computer. We at Civis Analytics are such big fans of Grace Hopper, we even have named one of our conference rooms after her!
One of the things I love about working at Civis Analytics is applying data science to contribute to great causes, like supporting and advancing women in tech.Last year, all fired up from an 
, I pitched Gabriel Burt on a radical idea. "What if we had hackweek every week?" The results have been exciting: we've produced internal tools that we use every day, released new features that went right into production, contributed new code to the open-source community, squashed of some of Tech's least favorite bugs, and even prototyped major technical projects that made their way into the development queue and been staffed with large teams.
Hackweeks align with one of our most important values at Civis: promoting autonomy. For people doing technical work, that means having voice in the direction our products and offerings take, so they don't end up as just the implementation arm of a business group. One of the best ways to do that is to give engineers permission and encouragement to try out risky ideas on their own, putting resources behind ones that pan out. That also lets us actually test hypotheses about new designs or technologies, instead of trying to guess what will work and relying on meetings and consensus. You have to make a space where it's OK for those projects to fail, which means setting aside time for engineers to experiment, to hack, outside of their regular work.
The traditional approaches have some drawbacks:
In retrospect, the solution was obvious; we wanted the structure and contiguous time of a delineated hack week, but spread out across the year like our free Fridays or like totally unstructured 20\% time would be. We could get there by giving everyone a full week at a time, and staggering the weeks so that only a few engineers are hacking at once. Each week we give a group of engineers all week to hack, telling them to set aside their normal work and cancel as many meetings as they can. Once we've gone through the whole team we start back at the top, spaced out so everyone gets a hack week four times a year.
Staggering means we can schedule around deadlines and make sure we can line up the backup to let even the most critical team members step away from their day jobs for a week. It means only ever having a handful of hack week presentations at a time, so great ideas are less likely to be missed in the shuffle and so that everybody gets a chance to practice pitching their technical ideas. Most importantly, it also lets us concentrate more support resources on a handful of people hacking each week. We've been able to go way beyond giving people a week and telling them good luck. Instead, we've built a structured accelerator to make sure hackers have the design and infrastructure support they need to complete their projects, to maximize the chances that those projects make the jump into production, and to make sure we learn as much as we can even from projects that don't go as planned.
Moving to running hack week every week has let us:
I'll dive more into our program and how it works in my next post. Stay tuned!We're always curious about how new platforms change people's political expression, and, of course, whether we can develop new instruments to measure these opinions. On social media platforms, users aren't anchored to a distinct, real identity we can match back to a voter file, which poses a real challenge for answering typical opinion research questions "who are the people that make up a candidate's supporter base?"
Instead, on platforms like Twitter, people distinguish themselves through who they connect with and what they talk about, not through their basic demographics. So as we shift our attention to non-traditional platforms for opinion research, we've realized that we also need to shift how we ask these basic opinion research questions.
Today, we're releasing our analysis of the GOP primary raging online (and offline) based on the seven candidates who were on stage for the main Republican debate on January 14, 2016 (while Christie and Bush recently dropped out, we still included them here for comparison). To complete our analysis, we asked the question 
 And we were fascinated by what the data uncovered:
This work is not to be confused with our 
 we released last week, which focused on the content of tweets rather than followers' interests.
We aimed to identify the interests that are most strongly associated with followers of a particular candidate relative to followers of any of the candidates. It is important to note that these are not necessarily the most frequent interests of the followers of a candidate. The most frequent interests for a candidate tend to be generic interests like television and politics. Our aim was to highlight differences between followers' interests, and so the interests we identify can even be ones that a fairly small fraction of a candidate's followers have, if those are interests that few of the other candidates' followers have (e.g., progressive politics for some of the more centrist candidates, or specific TV shows). Also note that these are interests associated with one candidate's followers relative to the other Republican candidates' followers -- not relative to the entire population of Twitter users, etc. And so, for example, conservative politics is less likely to show up as a top interest.
To find the interests of Twitter followers, we used the awesome 
. Using a custom machine learning framework, Macromeasures provides a hierarchical list of interests for individual Twitter accounts. Each interest includes a score indicating the strength of the interest based on the machine learning model. For this analysis, we included only interests with a score of 4 or above, corresponding to Macromeasures' "High" level of interest. This left us with an average of about 19 identified interests per follower. Note that we included interests from all levels of the hierarchy, and so the results include some similar interests at different levels of generalization (e.g., political issues, abortion rights, pro-life activism, pro-choice activism) when candidates' followers are strongly associated with both the general and specific variants.
We took the following steps to create the lists of interests that are strongly associated with the followers of each candidate.
The metric by which we rank follower interests is based on the Binomial distribution, which models the number of successes for a given number of trials (e.g., the number of times one will see heads if one tosses a coin a certain number of times). For the number of trials, we compute the number of users who have a particular interest in our sample. For the number of successes, we compute the number of these users who also follow the given candidate. We could then easily estimate the conditional probability of a user following the candidate given that they have the interest and follow one of the candidates (subject to the bias due to oversampling as described above). However, since this would tend to rank rare interests more highly, we compute the lower bound for a 95\% confidence interval for the Binomial distribution's probability of success parameter, using Wilson's method as implemented in 
. This helps discount rare interests, giving us a conservative estimate of how strongly associated the particular candidate is with the particular interest. We can then rank all interests by this value to find a list of most strongly associated interests.
This methodology was inspired by a 
.
For ease of visualization, we categorized interests ourselves and assigned colors to each. This was subjective, and in some cases, the assignment to categories is unclear (e.g., we decided not to label Washington D.C. and Las Vegas as regional interests since those locations seem much more likely to be of interest to a broader population than Ohio or New Jersey).  We also replaced specific brands with generic labels (e.g., "a news publisher").Some of our work on social media analytics was highlighted in a recent 
, and it gives us a great opportunity to talk about some of the methods we use for making sense of the Twitter firehose.
Civis Analytics takes a different approach from many companies in that we have a general methodology for imposing interpretable structure onto the unruly beast that is a Twitter conversation. We don't just monitor keywords or hashtags that relate to themes we already know (or assume) are important - we let our algorithms decide what the most important topics of conversation are based on how often they come up and how strongly they cluster together. Letting the data speak for itself helps us ensure that we aren't missing important themes in the conversation, or influencing our findings based on our prior assumptions. Before I get into some of the methodology and findings, we'll a look at one of the most eye-opening topics that emerged.
When we set our algorithm loose on millions of tweets about the US presidential primaries, it learned that there was a "topic" of conversation that was strongly associated with the words below:
This topic piqued our interest especially because it hit an all-time high after the Iowa caucuses:
What this topic is actually about becomes clear once you start looking at tweets that are strongly associated with it, for example, 
. That's right, it's all about people threatening to leave the country (if their nightmare candidate is elected). That these sorts of tweets make up such a big part of the conversation this election season is interesting, but I'll leave it to you to determine whether it says more about the candidates or the mood of the electorate.
Some follow-up analysis may help to sway your decision. For your consideration, here's the breakdown by candidate of people's threats to move 
:
And, since Mr. Trump is by far the biggest driver of this topic, here is the breakdown of locations to which people threaten to move, should he be elected:
Predictably, given its geographical and linguistic convenience, Canada is the most common refuge. Other North American and/or English-speaking countries are also well-represented on the list. Perhaps more surprisingly, a fair number of people threaten to move to other US states such as Alaska or Hawaii if Trump is elected; it's an open question why exactly they think this would improve their personal situation. And a few Twitter users are contemplating more extreme measures (moving to Mars, Pluto, or North Korea). Only time will tell if they have to make good on those threats!
Our methodology for analyzing Twitter conversations can be applied in almost any domain, but let me go into a bit of greater detail about our analysis of the 2015-16 US presidential primary as an example. We analyzed about 15 million tweets spanning the time frame from May 1, 2015 until today, in which one or more of the declared presidential candidates were referenced. We then used unsupervised learning to identify the most salient themes in the social media conversation, and the most important segments of users participating in the discussion.
Civis uses neural networks to identify latent topics in the conversation; some tweets may be associated with multiple topics, others with just one, or even with no general topic that is important enough to track independently. While the model doesn't tell us the "name" of a topic, it does tell us what words and tweets are most strongly associated with it, which makes it fairly straightforward to assign interpretable labels to them.
For example, in the presidential campaign analysis, many of the 44 topics identified by our model had to do with the candidates themselves. This is clear from the high activation that candidates' names and hashtags have with the topic (and the other words that show up in the list can be revealing about candidate perceptions!).
Ted Cruz' birthplace clearly figures prominently in tweets about him, as do his 
 impressions and his non-traditional 
. Trump's topic is dominated by supportive hashtags and contains less specific terms (in part because he is discussed so frequently and in so many contexts). Rubio's topic shows evidence of interest in his family's Cuban background and traffic citations, as well as an 
 he threw in Iowa. And when people talk about Chris Christie on Twitter, they typically aren't focusing on his experience as a federal prosecutor.
Some other topics identified by the model have to do with major issues that have been debated by the candidates or events that happened during the campaign. Of course, immigration policy and border security has been a major theme of the Republican primary. Funding for Planned Parenthood has been discussed in both contests, as has Hillary Clinton's use of a private server for e-mail during her tenure at the State Department. The focus of these topics is immediately apparent from the sort of words associated with each:
Among the other major themes in the presidential primary conversation are one about general campaign announcements, one on poll results, one on the primary debates, and one consisting entirely of profanity. (We'll omit the list of associated words for that last one.)
Figuring out what people are talking about on Twitter is helpful, but we can take things a step further by segmenting the users who are actually engaging in the discussion. Civis uses a method for automatic clustering of Twitter users based on their relationships with others in the social network that defines a conversation. As with the identification of topics, actually assigning labels to these user groups (or 
) is typically fairly straightforward.
The diagram below illustrates the three user communities that our algorithm identified as most separable in the discussion of US presidential candidates. In the diagram, each node represents a user account, the color indicates the user's community, and the size how many followers the user has. Nodes that are closer together, generally, are more similar in the types of users that follow them and the types of users they follow themselves.
Since the topic under discussion is political, it is probably not surprising that the user groupings that emerge are politically-based. There is a Conservative cluster including accounts such as the NRA, the Cato Institute, and most of the GOP presidential candidates; a Progressive cluster with accounts like the Nation, Occupy Wall Street, and Bernie Sanders, and a group of accounts we've labeled as the Media. The Media cluster includes accounts like The Wall Street Journal, POLITICO, and (yes) The Onion, but also some more centrist organizations and politicians (including Hillary Clinton and Jeb Bush).
Now that we have categories associated with some of the most important and active users in the conversation, we could do all sorts of crosstabbing and analysis of these communities across topics, time and space.
The presidential primary campaign has been a wild ride so far, and we've had fun applying our models in this area, but the methodology can be applied to any topic or brand to gain insights and strategically join conversations.Advertising during the Super Bowl is a huge investment. Brands spend upwards of \$5 million on a 30-second spot because it's the biggest TV advertising platform of the year. It's an opportunity to reach over 100 million viewers with messages about brands, products and services - but what kind of return on investment (ROI) do these ads generate? How can we quantify whether an ad actually causes someone to change their mind about something?
We can answer those questions with data science.
Before Super Bowl 50 we chose four ads to test across different industries using Civis Analytics' proprietary multi-level bayesian creative testing algorithm to see which ads directly influenced the attitudes and intentions of the viewers. 
The result of this analysis can be boiled down to two numbers. The first is the ad's 'Probability ad works' to increase brand favorability or intent to purchase given the experimental result. The next thing we look at is the ad's actual observed 'Impact' on brand favorability or intent to purchase overall and among subgroups.
Here's what we found:
Helen Mirren's no-nonsense public service announcement against drunk driving actually increases the likelihood someone says that they'll have a Bud the next time they have a cold one.


It is attention-grabbing but not necessarily appetite-inducing.


Puppy-Monkey-Baby probably doesn't increase your desire to try Mountain Dew Kickstart, which was probably already pretty close to zero.


Ryan Reynolds makes you like Hyundais. Admit it. Also, admit that 
 is your favorite movie.




Civis Analytics believes that when we provide numbers to inform a business decision, those numbers should be immediately actionable. That's why we boil down the results of Civis Analytics' rapid message testing to a few meaningful metrics.
 The 'Probability ad works' metric helps us quantify whether our observed evidence is likely to be true or random noise. After observing an ad's impact on a survey respondent's brand favorability or intent to purchase we can determine the probability that the ad had a positive impact. This is incredibly important when testing ads because the actual impact on an attitude or behavior from an ad is usually small. Our methods help distinguish between the signal and the noise and help clients make decisions under uncertainty.
 The bars for each group show the average treatment effect (positive or negative) of being exposed to the ad. Positive bars (green) show the observed improvement in favorability or intent to purchase. Just like in clinical trials we're comparing the difference between those who received the treatment (saw the ad) and those who received a placebo (did not see the ad). And just like in clinical trials this methodology allows us to make claims about causality rather than just correlation. To read more about this check out the Wikipedia page for 
.
P.S. Civis Analytics can help tell you when and where to air your spot as well! Learn more about the 
.Here at Civis Analytics, we love open source. We use Ruby on Rails, AngularJS, Docker, and Go (to name a few projects), and we're happy to contribute back to the community and release our own open source projects.
We're pleased to announce 
 where we will publish our open source projects and list our policies.  We use the BSD 3-clause license and a 
 for our open source projects. These policies allow us to share our work using a standard license and provide an inclusive and respectful environment for people who participate in our projects.
We've already open sourced and talked about 
 and 
 in past blog posts.
Our newest project is one that builds, packages, and deploys Go projects called 
. As always, we welcome your contributions and hope you find these projects useful.
We recognize the importance of contributing to open source and are proud to encourage our team to participate by submitting pull requests, filing issues, or creating new projects. We've been inspired by companies like 
, 
, 
, and others to publish open source projects. The success of existing open source projects and supporting new ones is worth working on and we all look forward to contributing to open source in 2016 and beyond.Security is hard. You're busy writing code, but you also want to keep your application secure, so you're doing double-duty developing new features 
 keeping an eye on vulnerabilities. You follow Hacker News and Reddit, but you know any good security strategy revolves around defense in depth, and you're looking to add additional, automated tiers to help keep an eye on security for you. Fortunately, Ruby has some great tools to help you out.
 is one such tool. It provides patch-level verification of your Gemfile, auditing your gems for security vulnerabilities so you don't have to. It easily integrates into your continuous integration workflow, letting you focus on building software and trust that your build will fail when something needs attention. We use it every day, and couldn't imagine maintaining a complex software application's dependencies without it.
When 
 was announced, we found ourselves looking for a similar tool to audit Ruby and RubyGems. To our surprise, we couldn't find one. So we built it.
 was written to complement bundler-audit, providing complete coverage for your Ruby stack. It behaves like bundler-audit, and integrates in the same way. For example, from our 
:
Now when an advisory is released, our build fails. We can immediately assign someone to work on upgrading our version of Ruby or RubyGems, ensuring a prompt response. In the meantime, we can get our build passing again by telling RubyAudit to ignore the advisory:
This provides an automated tier that reinforces our other approaches to security, helping us stay on top of security advisories.
We've open-sourced RubyAudit on 
 and published the gem to 
. We encourage you to add it to your Gemfile, and welcome issues or pull requests.
As with many open-source projects, we do great things by building on top of those that came before. RubyAudit would not exist without the hard work of the 
 team, specifically bundler-audit and 
.Last week, 
 released their annual list of 
 and sure enough, Data Scientist tops the list. Since our team is largely made up of Data Scientists, we thought we'd get to the heart of what makes being a Data Scientist such a great job. And what better way to do it than to ask the Data Scientists directly.
Here's some thoughts from our team on why they love what they do:
"As a data scientist, I really enjoy the mix of math, statistics and computer science required to get the work done. Doing data science at Civis is even more exciting, because I know that my work actually influences major decisions made by our clients." - 
, Applied Data Scientist
"I love how varied my work is, and how it's always pushing me to learn new things. As a data scientist, I have to get creative every day in looking for solutions to problems. And crafting a solution to some problem is extremely rewarding, because for many of the problems we solve here at Civis, no one has solved them before-I love that feeling of pushing on the boundaries of what we know." - 
, Data Scientist
"The tools of data science seem to be useful for a much wider range of problems than we currently know. Honing these tools and discovering new uses for them gives me a wonderful feeling that there's more to what's possible than we imagine today." 
, Data ScientistWe were lucky enough to be a customer through the services division at Civis Analytics when we then learned about the Civis Media Optimizer application and could optimize all of our media buys. As they were gearing up to launch the application on the 
, we opted in to become a beta partner to see if the tool could really optimize our buys. And sure enough, it does!
There are two reasons the Civis Media Optimizer is a good fit for Discovery. First, the type of planning we do, focuses on the audience first. Using the Civis Media Optimizer, we can find viewers based on past viewing behaviors, which we believe is one of the best ways to find new viewers. Second, our type of buying has evolved to rely on data science to optimize our TV buys. We've learned that in addition to looking at the largest networks, we should also look into unexpected networks to find those new viewers. It doesn't matter where we find our viewers, what matters is that we've found them. And often times, they aren't necessarily watching in the most expected places.
We used a number of planning tools to determine ad spend before the Civis Media Optimizer. Historically, these tools would look at a specific audience and identify the networks that tend to over index against that audience. From there, we would allocate GRPs against those network suggestions. While this worked, it was not nearly as dynamic and extensive as the Civis Media Optimizer application. With Civis, we get network, daypart and specific program recommendations for a wider range of networks. I haven't come across another tool that provides that level of specificity when it comes to making recommendations.
The media team uses it to optimize our paid campaigns and owned assets - the networks within the Discovery portfolio. Getting the team up and running was easy - it's a very intuitive tool that's easy to use and quickly provides recommendations.
We recently launched a campaign to drive viewership for a specific program on one of our networks. In planning the campaign with the Civis Media Optimizer, we discovered many of our viewers were watching unexpected networks that we could then advertise on. By advertising based on those recommendations, we saw a 10\% increase in tune-in conversation from 2014 to 2015 for that series.
The most surprising thing was the dayparts and networks that the application recommended. It's not about how and where we reach our viewers - it just matters that we reach them. In the past, we'd focus on five or so networks, but when you buy based on audience, it's not about which network you advertise on, it's about finding your viewers wherever they are.
Jump right in and get started! There are so many different levers you can pull when optimizing - play with the variables and see what happens. Also, don't just stop at one A/B test. A/B/C test and compare audiences to see what's best. The application is very intuitive so don't be afraid to try different combinations.
Also, the team at Civis Analytics is always happy to help and is so easy to work with. You'll find the application is so easy to use that you won't need their help, but if you do, they're always there.
It all comes back to content - it doesn't matter what screen you are watching on. As a network, it's critical that we create content that is valuable to our viewers. As long as we continue to do that, our consumers will continue to watch.




	
Barack Obama recently gave his final State of the Union address, and since we're interested in analyzing text data at Civis Analytics, I figured I ought to see if I could discover anything interesting. Rather than trying to understand the conversation on social media as we've done in 
, I decided to take a somewhat longer view, comparing the text of this year's speech to the texts of all of the previous addresses, starting with George Washington's first address in 1790.
It turns out that one can use data science to get some pretty interesting insights out of State of the Union addresses with just some very simple text analysis methods.
Here's a quick visualization for that last point.  I'll explain it in more detail 
.
In the rest of the post, I'll explain 
 before presenting 
 with analyses of 
.  I'll then focus in on 
 and how they related to some 
, before offering a few 
.
A lot of this post is about comparing different State of the Union addresses to
see how similar they are.  Here's the (relatively simple) text analysis methodology for doing that.
I took the data set of State of the Union address texts and performed the following steps:
This results in a matrix of similarity scores between addresses, which can be
visualized as follows.
In this similarity matrix visualization, each address corresponds to a row and a column. The similarity between address 
 and address 
 is shown at the cell at row 
 and column 
 (or row 
 and column 
). Darker cells indicate higher similarities, with the diagonal being maximally dark because it corresponds to an address's similarity to itself.  Note that the cells above the diagonal are a reflection of those below it. This makes it easier to compare a single address across time by looking across a single row or column.
Click 
 for a version with a label for each address.
There are probably many observations to be made about the similarity matrix above, but the thing that seems most salient is that there are large blocks of similar speeches, perhaps representing important eras of American history: 1815 to 1912 (pre-WWI), 1923 to 1932 (Coolidge and Hoover), 1946 to 2016 (post-WWII). (Note: these blocks probably also to some extent represent changes in dialect, both as American English changed over time and since the State of the Union changed from being frequently written to being delivered orally.)
The pre-1815 speeches varied quite a lot from each other, though each presidency makes up a little block. Jefferson's addresses in particular form a dark block of similar cells along the diagonal in the upper left of the plot.
Focusing on post-WWII addresses, the post-Reagan addresses appear to make up a block, perhaps because they shift away from talking about the international issues such as the Cold War and focus a lot on jobs and families. (I'll try to interpret this shift a bit more below.)
There are also some outliers in the above plots that are interesting to explore. For example, Bush's post 9/11 speech, largely about terrorism, is dissimilar to everything except Bush's subsequent speeches. Carter's 1981 address is the longest address at over 33,000 words, many times longer than most speeches since 1900 (e.g., Barack Obama's 2016 address had about 5,200 words). The 1981 address was a written rather than spoken (
), and though I normalized for the length of speeches in our analyses, its extreme length probably resulted in partial overlap with a lot of other addresses.
We can also "zoom in" by restricting the matrix plot to only include addresses from a particular time period. Note that this causes the mapping from similarity scores to colors to change a bit because the general level of similarity is a bit higher for smaller time periods. This may allow finer-grained distinctions to be made. In the plot below, we'll zoom in on the post-WWII period.
To help better understand the structure of the matrix visualization, I computed the mean of the log entropy scores for each word during various time periods (e.g., pre-WWI). I then ranked words for several time periods in attempt to get the most salient or interesting words for those time periods. For lack of a better term, we'll call these the most "salient" words.
For example, there is a stark contrast in the salient words before and after Franklin Delano Roosevelt's (FDR) presidency, which spanned some of the most difficult years the nation has faced because of the Great Depression and WWII. There appears to be a turning point in the language of addresses around WWI or WWII. Much of the language that shows up as particular to the period of time prior to this turning point pertains to the growth of the country, its relationship with colonial powers in Europe, treaties, territories, etc. During and following FDR's presidency, the language shifts to focus on government programs, jobs, the economy, the Cold War, energy. There also appears to be a larger focus on statistics (e.g., "millions" and "billions" show up as salient words), which at a glance appears related to increased discussion about jobs and government revenues and expenditures.
We can also zoom in on just President Obama's speeches.
Obama's addresses are all relatively similar to each other. However, not surprisingly, similarity is generally highest between addresses in subsequent years. Looking at the words that are strongly associated with Obama's addresses, we see a focus on jobs, kids, college, clean energy, terrorism, Iraq, and Afghanistan. Comparing the top words for his first term and second term, the most notable thing seems to be a shift away from talking about Iraq and toward talking about terrorism and Afghanistan.  (Note that "al" and "qaeda" show up as different words because I didn't do any detection of multiword expressions.)
Focusing just on Obama's 2016 speech, note the use of the word "voices", which appeared 10 times in singular or plural form (e.g., "democracy breaks down when the average person feels their voice doesn't matter"). The string "voice" only appears 94 times total in the other 229 addresses.
We can also look at which specific words make Obama's 2016 speech similar to previous speeches. To do this, I took the log entropy-weighted word vector for the 2016 speech and computed the elementwise product with the vectors for each previous speech, respectively. I then found the words for each speech with the largest magnitude for that product. These are essentially the salient or interesting words that overlapped between the 2016 speech and the previous speech. To avoid information overload (if we aren't there already), the table below just shows the results going back to President Jimmy Carter's addresses, and just the top three overlapping salient words. One thing to note is that similarity generally decreases as we go back in time, as can be seen in the similarity matrices plotted above (e.g., the 2016 speech was most similar to Obama's other speeches as well as the other speeches in the last 20 years or so).
This suggests that the similarity between Obama's 2016 speech and his previous speeches was because of his discussion of kids, college, jobs, and terrorism. Its (more moderate) similarity to George W. Bush's addresses was due to discussion of terrorism, whereas the similarity to Bill Clinton's addresses was due to college, jobs, and technology. We can even go back and compare Obama's 2016 speech to Carter's speeches. Though there is less similarity there compared to more recent speeches, there is some interesting overlap in discussion about energy and the Arab world.  Across this time period, we also see some overlap in the use of modern colloquial language (e.g., "got", as in Obama's 2016 statement that "we've actually got to cut the cost of college").
We can also find some interesting trends in the discussion of particular topics.
From looking at the tables of words above as well as the words that were salient
in addresses from the past 40 years, a few topics jumped
out at me, so I decided to take a closer look by plotting topical frequency
over time.
Individual words are rare, and so plots of word frequencies can
show a lot of variance, but grouping related words together can give us a clearer picture.
While unsupervised learning techniques such as 

can be used to automatically find groups of words, here I decided for simplicity
to manually group small groups of closely related words, as follows.
The results of these analyses are the plots 
.
They show the percentage of the total number of words in each
address that belong to each topical group.
Note that this analysis doesn't use log entropy weighting or stopword removal 
.
Please also note that a lot has happened
since Jimmy Carter became president, and so for brevity I'm going to omit some
really important trends (e.g., the end of the Cold War).
The plots also take the words out of context, and
in an effort to keep them focused and avoid polysemy, I have omitted potentially
related words (e.g., the word "power" could be put in the energy-related topic,
but it would also include the sense of "power" related to influence).
Despite the shortcomings of the simple methodology here, we see some interesting
trends.  Each topic has a fairly distinct peak during one of the presidencies
on the timeline.  Bill Clinton devoted a relatively large fraction of his speeches to
the "families" topic compared other presidents.  George W. Bush spoke a lot about
terrorism, and Barack Obama spoke the most about the jobs topic.
The energy topic in particular shows an interesting trend: a few of Carter's addresses focused
a lot on energy (e.g., due to the oil crisis), and then it was mentioned less
frequently for many years until Obama's recent addresses.
We also saw this trend to some extent in the previous section, where the salient words
that led to similarity between Obama's 2016 address and Carter's addresses included
the words "oil" and "solar".
In this post, I've presented some pretty simple but hopefully compelling (hey, you read this far) analyses of State of the Union addresses. The State of the Union addresses represent the president's outlook on the past, present, and future of America, and historical analyses provide us with a glimpse of how the country is evolving. The analyses also help to put President Obama's recent address in a broader context.
Of course, while this is super interesting, an analysis of political addresses over many scores of years aren't particularly "actionable" (e.g., "Mr. President, other presidents who mentioned jobs and families were also interested in this product"). However, similar historical analyses can be performed on other text data, which may help us gain insights about other types of conversations (e.g., analyses of tweets over days or hours instead of years, or analyses of customer feedback over time, etc.) and thereby help us make practical decisions. Each new data set brings new challenges (especially when there is text), and whether the data involves the future of America or a more practical goal, we at Civis Analytics are interested in using the latest and greatest data science methods for modeling, visualization, etc. to address those challenges.
 A similar, recently published analysis by 
 was brought to our attention after releasing this post. If you're interested in a really nice deep dive into the history of the State of the Union, please check it out.Civis Analytics helps organizations across sectors use data science to improve outcomes. While working across multiple engagements and sectors, we've determined the most successful organizations invite their entire team to participate in building a data-driven culture by setting every employee's sights on central metrics. Many of these successful organizations complement big data with Google Sheets, as they allow employees outside the data team to easily bring their own data points into the mix, and in turn, contribute to a data-driven culture.  This is why organizations (including Civis Analytics!) use them so often, and why we built a seamless connection to Google Sheets within the Civis platform.
There are hundreds of millions of Google Drive accounts, and the popularity is only growing. Organizations leverage the collaborative nature of Google Sheets to make concurrent changes in a single document that reflect inputs from various teams. While Google Sheets typically do not contain the big data that is the bread and butter of an organization's operation, they contain the critical elements of metadata that provide context for big data - benchmarks, dictionaries, and more. Their collaborative nature makes them a great way to gather data from large groups of people (especially when using Google Forms), and also to distribute aggregated information back out.
With our integration, we're making it easy for organizations to incorporate Google Sheets into their data flow: Civis users can import Google Sheets, directly populating Redshift tables. As with all platform functions, these imports can be scheduled or triggered by other tasks, allowing platform users to build workflows for non-technical team members to set their data in motion. Additionally, users can export query results to Google Sheets, allowing for quick and easy report creation and information distribution.
To put this in context, I'll share two examples:
In my previous job, I led the data operation for a political campaign in North Carolina, and used the Civis Platform for the majority of my work. I synced in data from disparate databases to unify information on field organizing, fundraising, emails, and digital activity. While I could query at any time to count contacts or dollars, Google Sheets allowed our team to share an understanding of whether we were on track to win on Election Day. Department directors worked with their regional counterparts to enter into and edit goals in Google Sheets. Using the Civis Platform, I scheduled an import of these goals every Friday at 5:00 PM, followed by a single query that merged metrics and the corresponding goals. My team iterated on this process by writing their own queries and exporting the results to Google Sheets to share aggregates with regional leads, who used past work to guide future goal creation.
Civis Analytics currently works with 
, a large non-profit focused on global poverty, to expand their audience. Their digital agency places online ads to find new activists who will work to end global poverty, and the resulting data syncs into Civis. When new activists sign up via these digital ads, their sign up comes with valuable metadata that changes dynamically, related to the message, vendor, and cost of the ad. The agency simply updates a Google Sheet when changes occur, and the Civis users seamlessly incorporate this metadata through scheduled imports. A scheduled script then joins the agency's metadata to the email database, and exports back out to Google Sheets clear reporting on cost and message success.
For those like me who already capture innumerable data points in Google Sheets, import them and start querying! For others, consider Google Sheets as an avenue to welcome other parts of your organization into a data-driven culture. And as always, automate - or in the words of the infamous infomercial, "Set it and forget it!"The last two posts in this series have been about getting a 
, and then 
, for example, which algorithms are working best and why. The upshot was a better handle on my workflow, but I'm left with a lot of free parameters of my algorithms to tune, and messing around with my workflow often leads to spaghetti code that becomes less and less understandable/easy to experiment with as I go. Enter the 
 and 
 objects: two tools that effectively allow me to pour gasoline on my data science fire, tightening up the code and doing parameter scans in just a few lines of code.
First up is Pipeline. There are a number of tools that I've chained together to get where I am now, like SelectKBest and RandomForestClassifier. After selecting the 100 best features, the natural next step is to run my random forest again to see if it does a little better with fewer features. In this case, I have SelectKBest doing selection, with the output of that process going straight into a classifier. 
 packages the transformation step of SelectKBest with the estimation step of RandomForestClassifier into a coherent workflow.
Why might I want to use Pipeline instead of keeping the steps separate?
This last point is, in my opinion, the most important. I will get to that point very soon, but first I'll get a Pipeline up and running that does SelectKBest followed by RandomForestClassifier.
I make a list of steps, each of which is a transformer (like SelectKBest) or, for the last one in the list, an estimator (RandomForestClassifier), and then turn that list into a Pipeline. Then the Pipeline is a single coherent workflow, with the transformed data from SelectKBest being seamlessly passed along the RandomForestClassifier. Depending on exactly what I want to do in a given case, I could have many transformers strung together, with or without an estimator at the end.
By the way--I've slightly changed the way that I am evaluating my model, using 
. It gives me more information than cross\_val\_score, which I was using before, although it's a little more involved to use (I am responsible for doing the training/testing split now, whereas cross\_val\_score did that automatically).
Now to GridSearchCV. When I decided to select the 100 best features, setting that number to 100 was kind of a hand-wavey decision. Similarly, the RandomForestClassifier that I'm using right now has all its parameters set to their default values, which might not be optimal.
So, a straightforward thing to do now is to try different values of k (the number of features being used in the model) and any RandomForestClassifier parameters I want to tune (for the sake of concreteness, I'll play with n\_estimators and min\_samples\_split). Trying lots of values for each of these free parameters is tedious, and there can sometimes be interactions between the choices I make in one step and the optimal value for a downstream step. In other words, to avoid local optima, I should try all the combinations of parameters, and not just vary them independently. If I want to try 5 different values each for k, n\_estimators and min\_samples\_split, that means 5 x 5 x 5 = 125 different combinations to try. Not something I want to do by hand.
GridSearchCV allows me to construct a grid of all the combinations of parameters, tries each combination, and then reports back the best combination/model.
GridSearchCV seems a little scary at first, because the parameter grid is easy to mess up. There's a particular convention being followed in the way that the parameters are named in the parameters dictionary; I need to have the 
 of the Pipeline step (e.g. feature\_selection, not select; or random\_forest, not clf), followed by two underscores, followed by the name of the parameter (in sklearn parlance) that I want to vary. To put this all together in a painfully simple example:
But once I've got the parameter grid set up properly, the power of GridSearchCV is that it multiplies out all the combinations of parameters and tries each one, making a 3-fold cross-validated model for each combination. Then I can ask for predictions from my GridSearchCV object and it will automatically return to me the "best" set of predictions (that is, the predictions from the best model that it tried), or I can explicitly ask for the best model/best parameters using methods associated with GridSearchCV. Of course, trying tons of models can be kind of time-consuming, but the outcome is a much better understanding of how my model performance depends on parameters.
I should also mention that I can also use GridSearchCV on just a single object, rather than a full Pipeline.  For example, I can optimize SelectKBest or the RandomForestClassifier on their own and that will work just fine. But since there can sometimes be interactions between various steps in the analysis, being able to optimize over the full Pipeline is really useful. It's also trickier to do, which makes it a good example for teaching.  Last, GridSearchCV will automatically cross validate all steps of the analysis, such as the feature selection-it's not just the final algorithm that should be cross-validated, but the upstream transforms as well!
This brings me to the end of this series, about end-to-end data analysis in scikit-learn and pandas. My goal in these posts is not to show a perfect analysis, or even one that demonstrates all the steps one might try, but instead to focus on the process. If I can get something up and running quickly, even if it's imperfect, I'm in a much better position to understand later on how much my refinements are indeed improving the analysis. At the same time, there are definitely best practices and tools (like Pipeline and GridSearchCV) that will make my life much easier as my work expands. Having a great set of tools in the python data science stack, and knowing when and how to deploy them, leaves me free to spend my time and energy on the most interesting, important and difficult-to-automate tasks-like trying to 
.Today, the 
 featured some of our polling data and proprietary algorithm results and their implications for the Republican primary race - a mostly under-the-hood look at Donald Trump's coalition.
We wanted to give a bit of a different look at the same data at a local level, showing how support for the Republican candidates has changed over the past five months - maybe even in the district in which you live.
This map shows a week by week look at which Republican primary candidate has a plurality - more support than any other candidate - for that week, in every Congressional district across the country, over the past five months.
Here are a few things it's telling us:
January is a long month in primary season, so we're just as curious to see how things shake out in the coming weeks ahead of the Iowa caucus and New Hampshire primary.
Right now, most of the polls you're probably reading about in the news are reporting a national or single-state topline of the race - usually with a margin of error of ~5\% or higher.
What we do is a bit different than what you've been reading:

We've gathered an enormous amount of survey data on the GOP primary so far. Back in August, we released 
 from an initial poll of 757 self-identified Republicans from a total sample of 3,007 Americans (you can read more about the findings in 
). Since that initial survey starting August 10, 2015, we've collected over 10,000 more survey responses to the GOP Primary horserace question on our ongoing weekly national tracking survey of 2,000+ respondents (if you'd like to add your own question, learn more 
).

To build the maps you're looking at, we're running tens of thousands of simulations using proprietary Bayesian algorithms that leverage all of that data to make estimates of survey responses in small geographies or demographic subgroups (if you're interested in learning more, check out 
).
Using these methods we're able to confidently generate estimates within 8.7 percentage points at the Congressional level which is 5.2 times better than what we could do with surveys alone.This is the second post in a series about end-to-end data analysis in Python using scikit-learn Pipeline and GridSearchCV.  In the 
, I got my data formatted for machine learning by encoding string features as integers, and then used the data to build several different models. I got things running really fast, which is great, but at the cost of being a little quick-and-dirty about some details. First, I got the features encoded as integers, but they really should be dummy variables. Second, it's worth going through the models a little more thoughtfully, to try to understand their performance and if there's any more juice I can get out of them.
I'll start with revisiting the way that I transformed my features from strings to integers. Recall that the strings were generally identifying categorical data, like the water source of a well or the village where the well is built. A problem with representing categorical variables as integers is that integers are ordered, while categories are not. The standard way to deal with this is to use dummy variables; one-hot encoding is a very common way of dummying. Each possible category becomes a new boolean feature. For example, if my dataframe looked like this:
This type of dummying is called one-hot encoding, because the categories are expanded over several boolean columns, only one of which is true (hot). I'll write a one-hot-encoder function that takes the data frame and the title of a column, and returns the same data frame but one-hot encoding performed on the indicated feature. I'm using the scikit-learn OneHotEncoder object, but pandas also has a function called 
 that does effectively the same thing. In fact, I find get\_dummies() easier to use in many cases, but I still find it worthwhile to see a more "manual" version of the transformation at least once.
Now I'll iterate through a list of the columns that I want to one-hot encode, transforming each one as I go, with the final output of that process being a dataframe where all the categorical features are encoded as booleans.
One note before I code that up: one-hot encoding comes with the baggage that it makes my dataset bigger-sometimes a lot bigger. In the countries example above, one column that encoded the country has now been expanded out to three columns. You can imagine that this can sometimes get really, really big (imagine a column encoding all the counties in the United States, for example).
There are some columns in this example that will really blow up the dataset, so I'll remove them before proceeding with the one-hot encoding.
In practice, I found that dummying my features didn't make a huge difference in performance when I got to the modeling stage, although this is the kind of thing you generally don't know before trying it.
I found that dummying had a huge effect on my dataset size in this case--I went from 39 features to over 3 thousand! And that takes into account aggressive trimming of the features that blew up the most. Having so many features invites problems with overfitting, slow and memory-intensive training, and I almost certainly don't need all 3 thousand features to capture the patterns in my dataset. This is a perfect use case for feature selection, which is supported in scikit-learn by e.g. SelectKBest(), which will do univariate feature selection to get the k features (where k is a number which I have to tell the algorithm). Making a guess, I can ask for the top 100 features, which doesn't make my performance much worse and speeds things up a lot:
Now I'll turn my attention back to the machine learning algorithms--there can be theoretical reasons to suspect that a particular algorithm will do better or worse. I found in the last post that a random forest classifier did the best of all the models I tried, beating a logistic regression (by a lot) and a decision tree classifier (by a slimmer margin). This doesn't come as a surprise to me, and here are a few reasons why:
With these points in mind, it makes sense that my random forest did so well on this task, although one of the catches with random forests is that they have lots of parameters to optimize. How many trees should there be? How does each tree get trained? How many features get used in training each tree? There usually aren't formulaic answers to these questions, and part of the craft of machine learning is tuning these parameters to get the best performance that I can out of my model. But with so many parameters, which sometimes interact with each other in complex ways, parameter tuning can be a huge hassle. In the 
, I'll talk about an extremely powerful pair of tools in scikit-learn, the Pipeline and GridSearchCV, that allow crazy powerful parameter tuning in just a few lines of code.A couple of weeks ago, I had the opportunity to host a 
 at the Open Data Science Conference in San Francisco. During the workshop, I shared the process of rapid prototyping followed by iterating on the model I've built. When I'm building a machine learning model in scikit-learn, I usually don't know exactly what my final model will look like at the outset. Instead, I've developed a workflow that focuses on getting a quick-and-dirty model up and running as quickly as possible, and then going back to iterate on the weak points until the model seems to be converging on an answer.
This process has three phases, which I'll highlight in an example I created to predict failures of wells in Africa. In this blog post, I'll show how I got the raw data machine-learning ready and build a few quick models. In subsequent posts, I'll revisit some of the choices made in the first model, effectively cleaning up some messes that I made in the interest of moving quickly. Lastly, I'll introduce scikit-learn Pipelines and GridSearchCV, a pair of tools for quickly attaching pieces of data science machinery and comprehensively searching for the best model.
The example problem is solving the "Pump it Up: Mining the Water Table" challenge on 
, which has examples of wells in Africa, their characteristics and whether they are functional, non-functional, or functional but in need of repair. My goal is to build a model that will take the characteristics of a well and predict correctly which category that well falls into. A quick print statement on the labels shows that the labels are strings:
The machine learning algorithms downstream are not going to handle it well if the class labels used for training are strings; instead, I'll want to use integers. The mapping that I'll use is that "non functional" will be transformed to 0, "functional needs repair" will be 1, and "functional" becomes 2.  When I want a specific mapping between strings and integers, like here, doing it manually is usually the way I go.  In cases where I'm more flexible, there's also the sklearn LabelEncoder.
There are a number of ways to do the transformation here; the framework below uses applymap() in pandas. 
 the documentation for applymap(); in the code below, I have filled in the function body for label\_map(y) so that if y is "functional", label\_map returns 2; if y is "functional needs repair" then it should return 1, and "non functional" is 0.
As an aside, I could also use apply() here if I like. The difference between apply() and applymap() is that applymap() operates on a whole dataframe while apply() operates on a series (or you can think of it as operating on one column of your dataframe). Since labels\_df only has one column (aside from the index column), either one will work here.
Now that the labels are ready, I'll turn my attention to the features. Many of the features are categorical, where a feature can take on one of a few discrete values, which are not ordered. In transform\_feature( df, column ), I take features\_df and the name of a column in that dataframe, and return the same dataframe but with the indicated feature encoded with integers rather than strings.  This is something I'll revisit in the next post, where I talk about dummying out categorical features with OneHotEncoder in sklearn or get\_dummies() in pandas.
Just a couple last steps to get everything ready for sklearn. The features and labels are taken out of their dataframes and put into a numpy.ndarray and list, respectively.
The cheapest and easiest way to train on one portion of my dataset and test on another, and to get a measure of model quality at the same time, is to use sklearn.cross\_validation.cross\_val\_score(). This splits my data into three equal portions, trains on two of them, and tests on the third. This process repeats three times. That's why three numbers get printed in the code block below.
I have a baseline logistic regression model for well failures. There's an assumption implicit in this model (and the other classifiers below) that classification is the correct approach to take here. Classification is designed for unordered categorical tasks, like predicting whether my favorite ice cream flavor is chocolate, vanilla or strawberry. Regression gives a continuous output which also implies a built-in order to the answers that it gets; an example would be predicting my age or my income. The task of predicting well failures could be modeled either way; it has discrete categories for answers (functional/functional needs repair/non functional) but there's also an ordering to the categories that a classifier isn't necessarily going to pick up on. I have the choice of modeling with a classifier and potentially getting slightly worse performance, or building a regression but needing to add a post-processing step that turns my continuous (i.e. float) predictions into integer category labels. I've decided to go with the classification approach for this example, but this is a decision made for convenience that I could revisit when improving my model down the road.
I started with a simple logistic regression above (despite the name, this is a classification algorithm) and now I'll compare to a couple of other classifiers, a decision tree classifier and a random forest classifier, to see which one seems to do the best.
And the winner appears to be the random forest, not really a surprise but you'll have to wait for the next post to learn why the random forest is such a strong algorithm.
This brings me to the end of the "getting started" portion of this analysis. I now have a working data science setup, in which I have:
In the 
 I'll clean up some of the technical debt that I've accrued by moving so quickly toward getting a model working.Employees at Civis have many interests; three at the top of the list are data, Chicago and biking to work. At the intersection of those three is the 
, which details some of the rider information for the Chicago bicycle-sharing program. It is a fun dataset to play around with, even if it has a limited number of fields. It gives basic demographics (sex and age) for Divvy subscribers, as well as clues about all trips made on Divvy bikes, including date and departure location. From this data set, it is relatively easy to extract some simple information, such as the average age of a Divvy subscriber--35.7--or learn that male riders outnumber females more than three to one. However, a common task at Civis Analytics is creating additional features from simple data. So let's start digging into the data and see if there is anything else interesting.
I usually use Divvy to get to and from work, but is this how most other people use Divvy? To begin, let's look at departure times from each station. Limiting the data to just weekdays, we can see how many rides happened during which hours of the day. As expected, there are spikes around the AM and PM rush hours (7-9 AM and 4-6 PM).
That makes sense, so let's move to the stations themselves. If we map the number of rides to the stations, we can determine the most popular in terms of number of rides per the number of docks over the first half of the year. As you can see the stations at the center of the city get the most traffic but no real patterns are emerging yet.
Let's go back to the rush hour element of this problem. Let's tweak the map to show the percentage of rides that occur during the AM rush and the PM rush. With this certain patterns start to emerge. But what we really want to see is which stations people are using to get to work and what stations are mostly used for people arriving at work. Let's subtract the AM rush hour percent from the PM rush hour percent to get a sense if a station is being used as a destination or departing point during rush hour (for example if a station has 40 percent of its rides starting during the PM rush and only 10 percent during the AM rush that would indicate the station is predominantly used as a place people arrive to work). You can see these three graphs in the animation below:
Now we get a sense of which locations people are arriving at to go to work (with minimal math!) As you might expect, the main concentration of Divvy arrivals are in the heart of downtown. However, we can also see a path of Divvy stations extending out of the center of the city that seem to be destinations as well. So one final step, we will overlay another dataset to figure out what might explain these points. Below is the same map with data of 
 locations from the 
.
Finally we have a data story that makes sense. Now we have a better understanding of the data and some additional features we can use for predictive modeling. Thinking a little bit about the data involved and extracting additional usage from all the available data is the first step of effective modeling.
If you are interested in recreating these visualizations, you can find the code 
.With the release of the Gear VR virtual reality headset by Samsung and Oculus, it feels like the future is here. It's easy to see how a number of industries are going to be disrupted by this new media format over the next few years by virtual reality, including video gaming, film, and marketing - imagine an architect letting you tour around a design instead of just showing you a blueprint.
But what about data science? The applications are much less clear than in entertainment and marketing, but it's likely that virtual reality will enable some interesting new data visualizations that 2D images, even interactive ones, don't provide.
You might be thinking, "Hey wait, is this just going to be like a silly-looking 3D bar chart?" Making good data visualizations is 
. There are a ton of design and user experience issues that need to be figured out for VR, just like 2D interactive visualizations, which have come a long way through projects like D3.js, ggplot2, seaborn, plot.ly, and many others. We're not going to get practical VR visualizations in one post, but let's take a few tiny steps in that direction.
Oculus has done some amazing work setting up 
. For example, to build full-fledged interactive VR apps, one can use two of the most popular game engines (Unity and Unreal). That's probably the way to go in the long run for VR visualizations since they allow for much more interactivity, but Oculus also has included a very nice little app in the form of Oculus 360 Photos that allows you to view static 360-degree images. "Static" or "non-interactive" are perhaps not great terms here since a static image in VR wraps completely around you. You can look in any direction, and you'll see a different part of the scene. However, you can't move around (like an app), and the scene doesn't change or move (like a video).
The 360-degree photos can also be stereoscopic, meaning that there's one view for your left eye and another for your right, creating a sense of depth to the images, similar to a 3D movie (but at least to me, much more convincing). (Aside: unfortunately, a lot of VR content currently available, particularly video, is not stereoscopic. Hopefully better tools and cameras will make this cheaper and easier.)
I'm going to focus on stereoscopic images, but hopefully some of the ideas are applicable to videos and apps. So... a stereoscopic 360-degree image? That must require some crazy proprietary format that only exists for the Gear VR and will never be used for anything else, right? Wrong. Actually, one of the 3D-to-2D projections that works with Oculus's software, the equirectangular projection, was invented around 2,000 years ago by 
, and images using this projection can be saved as PNG files, invented slightly later in 
.
In fact, such equirectangular projections are not so hard to find. Many world maps use this projection, like 
 from 
 (others are 
).
Loading up an equirectangular PNG file into the Gear VR is pretty easy. See 
).
An aside about map projections: equirectangular projections, and other 2D map projections, cause visualize distortions. In fact, the best possible projection is to some extent a 
. For example, in an equirectangular projection, Africa appears much smaller than it really is because it is along the equator, and Europe appears relatively larger because it's further from the equator. When viewing the globe in 3D, however, there are no such distortions. Of course, if you load up a world map in Gear VR, you're looking at Earth's surface as if from the inside of a sphere, but it's a much more accurate (and impressive) representation of our planet than a 2D image.
OK, back to data visualization. How can we render an equirectangular projection of a 3D visualization of some data? Again, my first thought was that it would require some fancy piece of 3D modeling software, which can cost 
. However, the open-source community comes to the rescue. The latest release of 
 (2.76b) actually has 
 for rendering equirectangular projections (
 are probably a better projection for VR, and Oculus's viewer software supports them, but Blender doesn't at the moment, to my knowledge). Also, a lot (perhaps most) of the things one can do in Blender's GUI are accessible through Blender's amazing Python API. It's easy to make a scene with some basic lighting and the right camera setup for VR rendering within the GUI, save it (as a 
 file), and then use a Python script to load that file up, add cubes, spheres, etc. for data, and finally render a PNG image as output. Blender also has support for outputting stereoscopic image files in a format that Oculus's software supports (one equirectangular projection for each eye, either side-by-side or up-and-down, in a single PNG file).
First, let's just plot some points as cubes. The base 
 just has a couple basic light sources and a camera slightly offset from center and a few settings for stereoscopic equirectangular output (see notes below). The following Python code generates some artificial data points from a 3D normal distribution and places a cube according to the implied x, y, z coordinates. It can be run with 
.
That gives us a nice little 
 that can be loaded into the Gear VR.
Artificial data is nice for a simple example, but what about something a little more real? Well, we're doing a lot of research on social media at Civis, and one of the datasets we've put together for development has tweets and profile information for Twitter users who tweet about data science, "Big Data," machine learning, etc. One thing we're exploring with such data
is how to cluster users into communities and then visualize the space of users in a useful way. An approach that seems fairly effective is to do the following:
The t-SNE algorithm ends up grouping together similar users because it tries to put users with similar sets of friends and followers, as represented by the adjacency matrix, near to each other in 2D space.
The t-SNE algorithm (and scikit-learn's implementation - open source for the win again!) is perfectly capable of finding a 3D manifold rather than a 2D one, so I tweaked our code to output a CSV file with x, y, and z coordinates for each of 1,000 users tweeting about data science, and then threw that into Blender. More precisely, I made a Blender scene with the appropriate camera settings and some basic lighting, and then ran the following code. (Note: the communities used for coloring data points are clusters extracted via the 
.)
The output is another stereoscopic equirectangular image that can be loaded up in Gear VR, but since not everybody has a VR headset (yet!), I'm also including the next best thing, an animated GIF (the frames were rendered by adding some camera movements to the modified scene) that 
 captures the excitement of being immersed in the data science
social network through VR.
Is that stereoscopic 3D image (or the animated GIF version) a useful visualization? No, of course not. It doesn't even have a way to show who's who, or who's connected to whom, etc. Nor can one search for particular users, or then click on nodes to see Twitter profiles, tweets, etc.
However, the sense of immersion in the VR visualization does seem to have some intangible value, like you can reach out and touch the data, though I won't go into it for fear of sounding completely nuts. Moreover, there is potential to convey visual information more efficiently in a 360-degree wraparound 3D medium than on a flat 2D screen. From the data science and analytics perspective, imagine an immersive VR dashboard: it would be like having 20 big screen TVs surrounding you (How else are you going to view all your Big Data?). OK, maybe that's a bit much, but hopefully you can imagine the possibilities.
It's hard to say where VR will take us. In the short term, while it looks like some the technical tools needed to make virtual reality data science applications are in place (e.g., the Oculus APIs, Unity, and open source tools like Blender), there are a ton of usability issues that need to be figured out, and it'll be a while before frameworks built specifically for data visualization show up (e.g., like D3.js for VR). However, the future is bright... and putting on a VR headset makes you think you're already living in it.
Select "Cycles" Render" in the "Info" editor (at the top of the screen by default)
In the "Properties" editor...
Select the camera and...
Go to the compositing interface (drop down in info bar) and...
There's a lot more details and ideas in the blender documentation and 
.In 2013, I was a disgruntled graduate student (is there any other kind?) fantasizing about leaving academia for the "real world" when I read 
 New York Times article about companies born out of the Obama campaign. One sentence, about Civis Analytics' work with The College Board, stuck out to me- "Civis will identify 'kids who are low-income, high-achieving but not meeting their college potential.'" I thought, one, that work sounds amazing and two, I'm going to apply.
A couple months later, I was working on the College Board project myself, and have since been lucky enough to work with a wide range of our nonprofit clients- from large foundations like the Bill \& Melinda Gates Foundation, to smaller (and equally awesome) nonprofits like US First. Our nonprofit clients make up a core part of our business, and I am very thankful that I get to work on helping these organizations make data-driven decisions every day.
Therefore, in the spirit of Thanksgiving and Giving Tuesday, we at Civis Analytics have decided to give back to the nonprofit community as a whole by holding a contest to give away some of our expertise (and sweet, sweet survey responses). Starting today, we're opening submissions for our "Ask America Anything" Contest where one nonprofit organization will win a two-question survey and analysis.
We like to think we are pretty good at surveying (and some 
 seem to 
). While we are known for some of our political surveys, we actually run surveys year round on a wide variety of topics for our clients. Even better, anyone can pay to add questions to our rolling survey that we send out weekly to a representative sample of 2,500 Americans. We take care of the survey, the sampling, and the weighting. In just one to two weeks, you get reports (and maps!) on demographic breakdowns for how different types of people answered your question.
In this case, we aren't asking for money, just a couple sentences on your organization, one or two questions you would ask 2,500 Americans if given the chance, and how you would use that information in 300 words or less. Knowledge is powerful, and having the answers to something you've always wondered about can help you make better decisions. So tell us what you've been wanting to know.
If you are a nonprofit organization interested in participating, enter your submissions 
 by December 7, 2015. We will contact a winner by December 14, 2015. Entries will be judged by the appropriateness of questions to your organization's stated mission and the likelihood that survey responses will be helpful to your organization.
We're looking forward to hearing what you'd like to Ask America!
*To read the official rules of the Ask America Anything Contest, please visit 
.One thing I love about working at Civis Analytics is that we have a fantastic learning culture. We share our technical knowledge within our teams on Hipchat, host brown bag presentations and even have a weekly Journal Club where we discuss academic papers. I wanted to expand that learning culture even more to teach and inspire others by showing what you care for and do outside of the office.
When I started back in 2013, I was somewhere around the 30-35 person in the company but now we have tripled in size and have expanded our diversity within the type of job roles. I knew there were those "others" hiding their awesome talents/skills/knowledge all because we might not sit next to each other in our open work environment or at lunch. So lightning talks were an excuse to also get to know others-even if it's only for those five minutes.
To give you a bit of insight on what I mean when I talk about lightning talks, they are a series of concise presentations, usually five to ten minutes, about pretty much anything. Our fellow Cives have already presented on a variety of topics and I've been very pleased learning some interesting facts such as:
I'm glad that we've set aside one hour a month where team members have an opportunity to share some insight that others can learn from. It's only been two months since we started, so I can't say they've dramatically changed things, yet, but I'm happy we continue to foster our culture of learning and providing interesting water cooler topics as our company continues to grow.
Here's the full list of sessions to date:
Interested in learning more? Connect with us on twitter @civisanalytics and we'd be happy to share some of the things we've learned.
P.S. If you are interested in joining our team, where you can learn data science from some really smart people in addition to Greek Stoicism or Crazy Beers, we're 
!Last month, I had the privilege of attending 
, a day-long executive education program hosted at Northwestern's Kellogg School of Management. I was generously invited by virtue of our company's selection as a Top 100 Finalist for the 2015 
, for which Civis Analytics had advanced out of more than 500 nominees. The Practical Innovator featured senior Kellogg faculty members speaking on a variety of topics, ranging from "Innovation Best Practices" to "Strategy and the Practical Innovator" and "Creating Winning Customer Value Propositions".
Every presentation I attended was fascinating, and content was geared towards companies of all sizes; from maintaining a culture of innovation at larger companies or corporations, to defining your value proposition, business plan, and acquiring funding as a startup, and finally how to grow successfully from a small company to large one.
The last topic particularly resonated with me - as a member of Civis Analytics from its inception, I have seen us transition in just over two years from a consultancy of about 15 people to a product and services company with 100 employees - and still growing. The speaker during this topic, Kellogg professor 
, highlighted five successful strategies for transitioning from small to large, which I've seen Civis Analytics adopt in its own specific ways over the past two years:
In our earliest days as a smaller consulting services-based company, we were eager to solve any challenges or opportunities that came our way (and fit our social mission). We learned what problems were inherent and common for companies looking to implement data-driven solutions, as we continually built a base of knowledge across verticals such as higher education, media, telecommunications, healthcare, non-profits, and consumer packaged goods. However, to avoid spreading ourselves too thin as we grew in size, it was critical for us to identify the verticals and applications in which our value proposition of data-driven solutions made easier (through both our product and services) is strongest, and which products and features we would focus on developing in our platform before coming to market - which brings me to the next topic...
Our earliest consulting work helped provide us with an increased understanding  of market opportunities across different verticals, while giving us more in-depth area expertise to supplement our individual-level, data-driven approach to problem solving. However, in order to truly scale our services, we have to identify patterns in the delivery of our solutions, and productize them for future iteration. This has resulted not only in the creation of our main product, the 
, but also individual applications such as the recently launched 
. Moving forward, we will continue to innovate and develop additional applications on the platform to solve specific problems, similar to how Civis Media Optimizer solves for TV optimization.
Building partnerships has been a critical component in fueling Civis' growth to date - whether it be a capability partnership like our OEM agreement with Tableau for best-in-class business reporting within our platform, or a market partnership like we have as an 
 of Amazon Web Services, whose tools we use to power our data science platform. Moving away from owning every business challenge and moving towards strategic partnerships with peers who specialize in key areas allows us to focus our resources on what we do best while still growing our business overall. Similarly, we've also been teaching our services clients how to use our platform more and more, as they transition from consulting clients to product users.
When I joined Civis Analytics in early 2013, we had a handful of extremely talented employees who were the core of our consulting practice - but we needed to surround ourselves with equally and more talented people in order to take our business to many levels beyond where we were. In order to ensure that we adhered to a high standard of employee talent, we developed a process of testing candidates with internally-developed exams designed to assess job-specific skills. These exams have helped us add new employees with a vast array of backgrounds and skills. It's a wonderful thing to be surrounded by extremely smart people, all with differing skillsets and points of view to bring to the table. Establishing this process requires patience, but is paying off in the long run.
In the earliest days of Civis Analytics, our reputation was built mostly from the successes of the 2012 presidential campaign. However, in order to reach new customer bases and verticals, we've had to build a strong brand and reputation outside of the political sphere, and build up a base of institutional knowledge and abilities that remains with the company as it grows. There's no better example than our 
 of Civis Media Optimizer for the type of brand awareness and recognition we're trying to build.
As we continue to grow and evolve as a company, it will be important to maintain and complete these transitions in each of the five areas. Additionally, as we become larger in size as well as product and services scope, it will be especially important to maintain a culture of innovation across our departments -- to this end we've placed a large emphasis on cross-functional communication for developmental priorities, and have established team members who are tasked with incorporating feedback across the different departments and teams here.
It's been quite an exciting two-plus years here at Civis Analytics, and I'm looking forward to seeing what the future brings, and how our strategies evolve and develop even further.A year ago, we blogged about 
, providing a high-level FAQ about interviewing at Civis Analytics. Today we'd like to talk about a specific stage in our interview process: the Hangout Interview.
At Civis Analytics, our engineering interview process is divided into three stages: a preliminary phone interview, a technical assessment, and the on-site. This is not to say that everyone goes through an identical process, or that every interview will have precisely three stages, but it's a good approximation. For our technical assessment, we'll ask you to build something, then to show it off. We want to see how you approach a problem, and we'll ask you to make changes on the fly. Our goal is for you to show us your best work, so we have you use your own environment and whatever language and tools you're most comfortable with. Then, we have you share your screen via Google Hangout so we can follow along.
There are many articles about video interviews, be it via Google Hangouts, Skype, or another technology. But the advice these articles provide is generic, dated, and doesn't address a lot of the issues we've seen candidates experience. Our Hangout is a 
 interview, which means we have different standards. To that end, we've put together a few tips to help you focus on the assignment instead of stressing over the medium.
 Most articles on video interviews tell you to "dress to impress". Even on 
, people have suggested dressing up. We don't care. Be comfortable. We won't judge you either way, whether you wear a suit or a t-shirt. If "be comfortable" is too vague, you won't go wrong with jeans and a nice shirt. This includes your surroundings. Time spent worrying about a messy bookcase is time better spent on engineering.
 Make sure you can connect and that your speakers and microphone work 
 the interview is scheduled to start. Remember that video interviews are bandwidth-intensive, and find a location with reliable Wi-Fi, or use a wired connection. Wear a headset if you have one, even the cheap one that came with your phone, so you have your hands free to type. Those are minor things, but if your interview starts 10 minutes late because you weren't prepared, that's 10 minutes you don't have to impress us, and we can't always let things run long.
 It's best if you can share your entire desktop, not just a single window. You don't want to have to stop and change what you're sharing when you transition from an editor to a terminal or a browser. Worse, it can be very confusing to an interviewer when you switch to a window that's not shared and we can't follow along with what you're doing.
 Be aware of your open tabs, your bookmarks bar, other applications, files on your desktop, your wallpaper, etc. If it's up there, we'll see it. Close your e-mail and IM, and hide any personal or confidential information, applications to other companies, etc. Don't forget about notifications and reminders. If your OS provides a "Do Not Disturb" mode (
), turn it on. Your phone, too. This will also help keep you focused on the interview.
 As with an in-person or a phone interview, make sure you explain what you're doing as you go along. Not only does it help us follow along, but part of any interview is gauging fit. We want to understand how you think, and know that you're somebody we'll work well with. Some people find that talking through things helps settle their nerves, too.
 With Google Hangouts, you share all monitors when you share your desktop. This can make it very difficult to follow along with what you're doing. We all love our second monitor when developing, but think of a Hangout as a cross between a presentation and a pair-programming exercise, where our attention is focused exclusively on one window at a time.
 Don't use a coffee shop. If you have a roommate, ask them to be quiet. Don't forget about ambient noise. If you live by a train station or a hospital, make sure you close your windows, or consider finding an alternate location. If your cat walks across the keyboard, it happens, but you shouldn't have to break to take your dog for a walk. This is an interview, and you want to be able to focus so you can do your best.
 Seriously. A lot of interviews are asking you questions, trying to weed out candidates by how they respond under pressure. We just want to see what you can do. Take some time to build something that allows you to show off what you'll bring to the role. The best candidates we've interviewed have been those that were excited to be showing off what they created.
Can't wait to show off what you can build? We're 
.Since joining Civis Analytics this year, I've had the opportunity to work with different organizations through our nonprofit practice area. Civis's commitment to doing work that has a positive social impact--including our significant involvement with nonprofits like the Gates Foundation, Enroll America, and the American Red Cross--was one of the reasons I chose to work here. We often find that many nonprofits face similar challenges as they begin to incorporate data into their strategic decision-making. In the spirit of helping nonprofits do more with the data they already have, I've gathered some key pieces of advice for small organizations to keep in mind as they gather and work with data.
Moving from a data-lite organization to using data to inform all of your strategic decisions can seem intimidating. But the truth is, you don't need to have "Big Data"--massive datasets with thousands of variables--to draw quantitative, actionable insights. You probably already have data on your work that can be utilized more effectively. And, diving into the world of quantitative analysis doesn't mean your industry experience will be irrelevant; data science is just one tool that can make you a more effective and efficient organization.
Using cloud data storage can be a preferable solution to managing an in-house server for many organizations, especially smaller nonprofits. Cloud storage means you can access data on the go and you are not reliant on your own IT team to keep your servers functioning. We recommend using  
 for organizations that are interested in transitioning to cloud storage. AWS only charges you for the data storage you actually use, which is great for small teams operating on a budget. The 
 is a great way for nonprofit organizations to get the benefits of cloud storage with very little set up. Civis has combined the best tools available through AWS--including the Redshift fast analytics database and more than 20 AWS products--to build a system that is reliable, private, fast, and secure. Find out more about how we use AWS 
.
When accessing data online and hosting data remotely, especially when you are working with personally identifiable information, data security is of the utmost importance. A nonprofit's reputation can be severely damaged by a security breach--which can affect people's donation and volunteering decisions for years. Multi-factor authentication, encryption in-transit and at-rest, and requiring complex passwords that are changed regularly can help make a computing environment more secure. When working with protected information, you will need to meet additional legal guidelines for data usage.
When working with any sort of data, it's important to follow standard conventions, like:
The mark of a truly data-driven operation is that people at all levels of the organization are using data to inform their everyday decisions. This is only possible if analysis is available to everyone, from the Executive Director to your volunteers or donors. Try to disseminate information in a way that is easy to understand and, more importantly, easy to act on--specific recommendations and findings that are relevant to the decisions that everyone is making in their day-to-day jobs. This has two benefits: first, aligning everyone in the organization around effective strategies, and second, encouraging more responsible data collection throughout the organization, as people see the value and payoff to having clean and accurate data.
To find out more about how Civis can help your organization better use data at any step of the process, 
.Recently, I was invited to give the alumni address to the graduating cohort of the 
 (yes, that's a large image of me on their site). Startup Institute is an immersive eight-week bootcamp I attended before joining Civis Analytics that helped me further develop some of the technical product design skills I had built in grad school and previous jobs. But, more importantly for me, it gave me the opportunity to focus on and craft my career in technology.
My talk centered around the feeling that so many of us who have come to Civis Analytics-and startups in general-share: I don't feel like I fit into "normal" work environments. Am I a 'weirdo'? The answer is a loud and proud "yes!" Being a little different is something to be embraced. Who wants to be normal anyway?
What is a 'weirdo'? Unlike the connotation we all grew up with, our definition of 'weirdo' in the Civis Analytics world, is actually a good thing. It means:
My fellow millennials and I are often 
 for this 'weirdness'. But, it's not just the millennials. As Civis Analytics continues to grow rapidly, we find great people coming to us from business, 
, non-profits... And they've all got that something special: a passion for the people they work with and the work they do.
And I'm not the only one who thinks about the culture I work in and the people I work with. 
, CEO of 1871 (Civis Analytics is an alum!), also spoke-about the 5 P's needed to succeed in a startup: Passion, Preparation, Perspiration-yes, sweat, Perseverance and Principles. While maybe not as kooky as what we call a little weird, we think alike - we're driven by the work we do.
Watch my talk 
 (warning: iPhone videography) and follow along with slides 
). Howard also posted photos of the event on his 
.
Ready to embrace your 'weirdness'? Consider joining us: 
.Yesterday, we introduced 
 to help marketers fix TV advertising.
This morning, we took out a full-page ad in the 
 - an open letter to Madison Ave. from our CEO. Here it is:





We're introducing 
 to help you save it.
"TV as an advertising medium is broken."
You hear it every day - consumers are cutting the cord, shifting to mobile, and demanding wider content options on demand. And it may all come true - eventually.
But, the fact is that 87\% of American adults will turn on their TVs this week.
 Families and friends gather to watch live events, dramas, comedies, sports, and more. Many even think we are in a modern-day TV renaissance.
And the reason is quite simple - Americans still love TV. They love blogging, debating, tweeting and, most of all, watching it. And marketers love TV as a result - it's still the single best channel to get a message out to a large audience quickly.
With the renewed engagement on TV, this means viewers also watch lots of advertising - your advertising. This year, companies will spend \$79 billion dollars on TV advertising alone, about 42 cents of every ad dollar.
But TV, at least right now, is also a marketer's burden. In this era of big data, companies want to target their exact audiences - the specific people they care about - and not the broad, imprecise demographic segments that are increasingly ineffective and outdated. Automakers want prospective car buyers (not 'Men'), consumer packaged goods companies want people with healthy food preferences (not 'Women'), sporting goods retailers want people interested in sports (not 'Youth') - and so on. New data streams and models make it possible, however, solutions using this data either have not scaled or are simply attempting to duplicate digital methods. This means TV ad strategies have not caught up - and companies are fed up.
That's why today we are launching 
, the first-of-its-kind ad-planning platform for TV. Civis Media Optimizer empowers advertisers to, for the first time, pick the exact mix of programs that maximize reach to their specific audiences for their budget. It's TV ad planning finally done right.
To show why it works, I'll take you back to our history in politics, where I served as the Chief Analytics Officer for the 2012 Obama Campaign. Before 2012, politics was in a very similar situation TV is in today. Billions of dollars worth of ads and personnel resources were being spent the same way they had been for decades; ignoring the fundamental issue of understanding who to focus on and how to effectively reach them. Our campaign's lesson, as far as advertising is concerned, is simple: Do the work to understand individuals based on data, and meet them where they are.
The same goes for TV advertising. With Civis Media Optimizer, we start with your specific individual-audience data (built by us or you), match it to anonymized consumer viewing habit data sets using a privacy-protecting third party, and provide you back the data within an in-browser user interface. The platform's algorithms then sort through the billions of combinations of shows to pick the exact combination that maximizes reach for your target audience(s). Your planners can then quickly and iteratively 'optimize' their media buy for your budget. You get the reach of TV with the precision of digital.
Discovery Communications and GMMB are among the media companies and ad agencies using the platform today to build better, more efficient buys.
"When it comes to campaigns and elections, every advantage counts," said Jim Margolis, partner at GMMB. "Working with the tools that Civis has developed gives our clients an edge in both effectiveness and efficiency that can mean the difference between winning and losing a campaign."
"When partnering with Civis, GMMB media planners are able to provide even greater efficiencies - we've seen 15\% improvements - through their analytical tools and data integration," added Daniel Jester, Managing Director at GMMB. "These tools provide sophisticated audience insights, deeper cost analysis, and more targeted media solutions for our buyers. They are a leader in the tech and data analytics industry that is revolutionizing how media is purchased."
We've seen your agencies provide stunning creative, insightful messaging, strategic direction, and more. But that's just not enough anymore. Your clients are starting to demand that every choice they make - not only in advertising, but in everything - be driven by data. They know it because they have to survive, and they'll demand it of you - their agencies - because otherwise they won't survive in the 21st century.
We're data-driven, and we're telling you not to quit TV. We've built 
 to help you preserve it.A while back we decided that all new functionality in the 
 would be implemented as a combination of API endpoints and front-end code. At the same time, we decided that all endpoints will be available to our customers. That is, if you can use a piece of functionality via the Civis UI, you can leverage the underlying API for programmatic access, too. This has been a huge force multiplier, as you can read about in 
. It also means that our Engineering Team can't break our API contract just because it suits the needs of the UI.
One solution is to rely upon API versions, which were built in to the Civis API from day one. But even with versions, we still need to know whether a change constitutes a breaking change. We consider it a breaking change to:
Published guidelines are good, but they can be fallible. We prefer to leverage automated solutions that codify our best practices, and make explicit and enforce our policies. With that in mind, we wrote 
. Swagger::Diff compares two 
 specifications and identifies any breaking changes. For example, to compare the Swagger Project's expanded petstore example against the minimal petstore example, verifying the expanded example is backwards compatible with the minimal example:
Swapping the order of the arguments, it compares the minimal example against the expanded example, flagging the endpoints, parameters, and attributes that are missing from the minimal example:
Swagger::Diff also integrates directly with our test suite. People might forget to run a check, but continuous integration (CI) does not. Coupled with an API endpoint that returns our current Swagger specification, it allows us to simply and easily compare two versions of our API via a 
:
Every time we push code, our CI tests the development branch against the production specification, and fails with a message detailing backwards-incompatible changes if any were introduced. This guarantees that no backwards-incompatible changes will accidentally slip into our API.
We've open-sourced Swagger::Diff under the 
, published the gem to 
, and made the source available on 
. We encourage you to add it to your CLI toolset, project's CI, or test suite, and we welcome issues or pull requests.
Do you enjoy building APIs? We're 
.One of the best learning experiences of our internship at Civis Analytics was the intern class' close reading of Edmond Lau's book 
. This four-week book club gave us the chance to learn from the experience of Edmond Lau, and incorporate the lessons he learned about being a high impact engineer. The process of discussing the book with the other interns also gave a deeper understanding of the material and how it might apply to the internship. The book really shaped our mindsets towards developing software.
During my internship, I was responsible for working on a few different tasks at once. While I wanted to do a lot of different things, I still wanted quality. The question of what to work on when was persistent. Lau suggests a few ideas for this, and I'll share how these worked out for me in practice.
Lau suggests you create a task list, a reasoned, ordered stream of activity. The best thing to do, he says, is to regularly update this stream based on the leverage of each action, or the value added divided by time spent. This can, at times, mean addressing things that aren't urgent, but important.
Implementation
I used 
 at the beginning of everyday for 15-20 minutes. For each task, I would spend a few minutes amount of time detailing the idea before moving on to the next. This included verbose descriptions of current bugs in a branch, tagging someone for code review, and listing new concepts to explore for potential solutions. This last one is important.
This strategy paid off as, little by little, I learned more about features in say, 
, that I would go on to use in branches later that day and later that month. Ultimately, it gave me more context for the tools in our stack. This habit personally conflicted with my sense of doing the most urgent task first, but adding in these more long-term goals helped pace the day better for myself and made me a more cohesive engineer.
Lau spends a lot of time talking about optimizing your workflow, which leads to faster iteration and more productivity. This involves taking a step back and seeing which phases of your development process slow you down. This is important as slow steps will take up much more time than necessary, interrupting your flow of thoughts. I have, in particular, worked hard on one point on optimizing workflow: mastering the programming environment.
Some examples he uses are mastering your text editor/IDE, getting familiar with shell commands, and making sure your debug flow is faster. I spent quite a bit of time mastering my text editor, 
. (Side note: while this post is vim-specific, many of the tips could be helpful for any command line text editor, including Emacs)
A quick step to getting a nicer vim environment is just editing your ~/.vimrc. There are many examples online and it greatly improves the experience from a curt, black-and-white text editor to a more descriptive, colorful editor.
The bigger step, for those that want to dive deeper into the world of vim, is to fill in a lot of functionality gaps that are important in a modern text editor. I previously used 
 for projects that I deemed too large for vim. I tried to make Sublime more vim-like with Vintage mode, but, frankly, it's awful. So, what was I missing in vim that Sublime gave? Various things: including fuzzy search, file search, a file tree, and fast shortcuts, among others little things. Combined, these things make vim a pain to work with on projects with a significant amount of navigation required. The problem with vim is that it does not come pre-installed with any of this functionality. Rather, you need to go out and find these plugins. So, first things first, a plugin manager. Some popular options are 
 and 
 (this takes a bit more effort). Next, find the plugins that fill in the functionality gaps that you have. The great thing about Civis is that everyone is eager to help each other, which is why I got most of the plugins that I currently use from another developer there. Some quick and easy must-haves include 
, 
 (or its more performant counterpart, 
), and 
. For those that are welling to spend some more time, I also highly recommend 
. To actually install these plugins for Vundle, you just put Plugin 'github-user/github-repo,' or another naming format that it supports, in your .vimrc and you're ready to go.
I find that I work faster with vim commands. Iterating quickly not only allows you to contribute more to the company, it also allows software engineers to learn more quickly.
The Effective Engineer helped me take a longer-term view of the software development process and helped me justify long-term investments in my coding productivity. The book emphasized the importance of tooling and the long-term ROI that is possible when a coder invests time in improving their development process. Lau places an emphasis on reducing iteration speed, the time it takes to run an experiment, see the results, and get back to experimenting. Because of Lau's book, I started investing much more in tools like Sublime Packages, superior testing frameworks, and keyboard shortcuts that make me a more efficient developer and reduce my iteration speed.
The book also caused me to question many aspects of the software development process that I previously viewed as unassailable truth, and weigh their pros and cons more objectively. Lau questions practices like coding reviews, automated testing, and paying down technical debt, pointing out that each of these practices trades off quality and iteration speed. He points out that sometimes iteration speed matters more. This has changed my approach to testing in my personal projects. Instead of viewing each test as equally valuable, I now work first on the tests for the most critical and most used parts of the codebase and add other tests later when the utility of the software is validated.
Do you have any tips on how to become a more effective engineer? Share them with us on Twitter 
.MySQL, which we host on 
, is one of the most important parts of our stack to optimize for performance. I found that in order to make one query against a table with a few million rows run quickly enough, I needed to add a composite index across six columns. I was a bit surprised to see this error come back from MySQL:
A bit of reading through the MySQL documentation informed me that:
What does MySQL do with those extra 428 bytes? Who knows! What is clear is that the index I needed covered too many columns that were too large, and MySQL was not going to create that index.
I was trying to create a composite index of five VARCHAR(255) columns and one INT(11). Because the collation on the table was UTF-8 and because MySQL assumes three bytes of storage for each Unicode character
, that puts us at 255 characters * 3 bytes * five columns + 4 bytes for that last INT(11). That's 3829 bytes - 757 too many!
Luckily, a look through one of those VARCHAR(255) columns showed that it only contained a few distinct values. Changing that column to an ENUM brought it from 765 to just two bytes, which puts us at 3066 bytes total - under the limit by a whole six bytes!
MySQL can create and drop indexes rather quickly, but performing the migration to alter the column type on a table with that many millions of rows is not something MySQL does well. Fortunately, for Rails users there is the wonderful 
 which adds robust, no downtime migrations for large MySQL tables. Using the gem, I was able to perform the lengthy migration with no downtime, create the multi-column index on my newly-shrunk table, and drop one particularly nasty query's runtime by 95\%.
So if you ever bump into an error about long key lengths when trying to create an index, take a look at the columns you're indexing and see if you can't shrink them down a bit. See if you can switch any VARCHARs to ENUMs or if your VARCHAR(255)s really need to be 255 characters wide. If you're feeling lucky, you could try applying an ASCII-only collation to your Unicode VARCHAR columns, but you never know where Unicode characters can crop up! In the final stretch of a major client project back in 2014, a fellow data scientist at Civis Analytics whispered a modest proposal to me: "We should be able to build all these models from massive amounts of survey data - and actually get some sleep." A reasonable idea - and one that we ended up solving with the Civis API. While the interface of the 
 gives me the tools (which you can use too!) to solve client problems and build workflows, with the API, I can script the Civis platform to process more data and build more models, faster, than I would ever have time to do by hand.
I work as a data scientist in the Applied Data Science department, building analytics solutions for clients in a variety of industries and sectors. At first, I thought the hardest problems I would be solving at Civis Analytics would be around modeling methods and algorithms - and trust me, those problems are hard! - but even more challenging is how to design, scale and execute whatever methodological solution I come up with. No matter the project, my toughest questions are about scale and workflows: Can I build a predictive model on this much data in less than an hour? Can I scale this process to build hundreds of models as our business ramps up? How can I empower the rest of my team to use my work? Can we plug these Civis workflows into processes that happen elsewhere?
I rely on the Civis API every day to develop workflows and pipelines that solve these scaling problems for my team. Many of the problems I face are already solved by the large suite of tools already in the platform. However, when I encounter a new problem, I can write a script that calls existing functionality through the API and supplements it with whatever custom solutions I write myself.
My current challenge is building an automated modeling pipeline that can handle approximately 200,000 new data points generated from rolling, weekly surveys.
In the not-so-distant past, we'd have anywhere from two to a dozen full-time data scientists working on this sort of pipeline. One person would check the data, another would model, another would analyze, and maybe some more people would build reports.
All these smart data scientists should be able to do more than operate just one part of our business. And with the Civis platform, I can orchestrate that entire workflow by myself. It means that the rest of our data scientists can build excellent workflows of their own, handle more clients, and produce more data science insight. We can even collaborate and share our solutions across projects and clients.
The platform is our force-multiplier, and one aspect of it in particular has been instrumental in making my life easier: the API. With the API and some basic python skills, I can develop chained workflows that start automatically and on a schedule. Those workflows can operate on more datasets than I would have the ability to set up by hand.  And best of all, I can set them to run even when I'm not in the office or when I'm working on something else, so I don't need to repeat the same tasks over and over again.
Part of my code for creating and building a model in python with a set of parameters would look something like this:
I can specify everything from the type of model used, an option to algorithmically find interaction terms and even an option to limit the cases I want to model in my Amazon Redshift tables. Once this model has finished building, I can then apply predictions from this model with code that might look something like this:
This code makes modeling incredibly easy to scale. It is straightforward to pipeline and extend to cover whatever use cases you need. In fact, all platform functions can be programmatically kicked off and their results returned. As a result, building a single custom workflow for a particular client does not monopolize all our person hours, meaning that we can handle more work than ever before.
The tools in the platform, coupled with the flexibility of the API, lets you customize your solutions - so that once you set it up, you can catch up on some shut-eye.As featured in the New York Times, the 
 conducted a survey of the Republican Primary of 757 self-identified Republican or Republican-leaning adults across the country. You can read more about our findings in 
 by Michael Barbaro, Nate Cohn, and Jeremy W. Peters from Sunday August 23, 2015 and our methodology in 
 by Nate Cohn on August 26, 2015.
We wanted to share a bit more on the survey we conducted, the topline findings, and the methodology used to conduct this poll.
Self-identified Republicans or self-identified Independents that lean Republican were asked the following question:
Civis Analytics conducted 3,007 live telephone interviews of adults in the United States contacted on landline telephones from August 10th to August 19th. Among respondents of this survey there were 757 self-identified Republican or lean Republican adults. These respondents were asked their candidate preference for the GOP primary.
Sample for this survey was pulled from nationally representative voter and consumer files provided by TargetSmart Communications.
The results were weighted to two universes for purposes of comparison. The first method weights respondents to the Republican Primary question to be representative of registered or modeled republicans in the United States according to voter files provided publicly by Secretaries of State. This method can only be achieved when sampling from voter and consumer lists. The second method is closer to the way most public polls using random digit dialing weight their surveys. Using this method, the the entire sample (all 3,007 respondents) is weighted to be representative of the adult population and respondents who qualified for the Republican Primary question are simply analyzed as a subset of the broader adult population.
The margin of error for this survey is 4.2 percent based upon the standard-error for a sample of 757 respondents plus additional error introduced through weighting.
You can read more about the methodology and findings in Nate Cohn's article in 
.
The Upshot article highlighted a simple regression analysis conducted by Civis Analytics Data Scientists about the relationship between candidate support and past voting history. We've provided the original analysis and charts for reference below.
This first chart shows average vote share (\% Trump, \% Bush, etc.) by historical election participation at the individual level, as measured by the voter file. The dashed line in the graph indicates the average number of elections that adults in our respondent pool have voted in. The dotted line in the graph indicates the average number of elections that 2012 Republican Presidential Primary voters have voted in.
The following chart indicates the difference in vote share for each candidate between 'average adult' respondents and 'average primary' respondents with standard-error bars for reference.
Keep on the lookout for more posts from the 
 about interesting work we are doing and our approach to research methods!At the beginning of July, I traveled to Austin for 
, the annual conference dedicated to scientific computing in Python. Python is a powerful, easy-to-use programming language which has become very popular in the data science community. I was at the conference because the data science department at Civis Analytics uses Python to write all of the predictive modeling which goes into the new Civis platform - it's been a wonderful tool for us.
Why do we like Python at Civis Analytics? Python's expressiveness lets us write software faster. More than just the initial creation of software features, we've found that Python is a good language at all stages of our code's lifecycle - creation, testing, review, deployment, maintenance, and improvements. The popularity of Python also means that resources are readily available to help developers through any problems. Python programmers are friendly and helpful people, and there's a great store of knowledge available in books and websites to anyone using the language.
The other advantage to working in Python is the open-source community represented by the attendees of the SciPy conference. We at Civis are able to take advantage of the accumulated knowledge of the machine-learning research community contained in 
, high-performance numerical tools in 
, and the powerful tools for working with tabular data in the 
 library, among many others. The availability of this professional-grade, open-source software lets Civis's data scientists focus on building new tools for organizations to take advantage of all of the data available to them.
While at the SciPy conference, I had the opportunity to talk about some of our experiences with writing Civis's modeling software. Watch the video here:
One of the earliest choices you need to make when developing in Python is which version of the language to use. Despite the fact that Python 3 was released in 2008, adoption has been slow. By this point, however, it's clear that new projects should use Python 3 (and Civis's data science department has been doing so for over a year). There's no one "killer" feature in Python 3 compared to Python 2, but there's a lot of little things that all add up to make Python 3 much nicer to use. You get better handling of concurrency with asyncio, the improved "forkserver" context for multiprocessing, function annotations for enhanced bug checking and more helpful IDEs, and more. The only thing that might require you to use Python 2 is if you need to use third-party software which isn't Python 3 compatible. All the larger, actively-maintained libraries are Python 3 compatible, but there's still lots of older libraries which aren't. If you're forced to use Python 2, you should at least make sure that your own software is Python 3 compatible. Statements such as 
 and libraries such as 
 make this easy.
Another thing to remember (and this is vital for any software project) is to build in comprehensive testing from the ground up. If there's any part of your software which isn't being tested, it's probably broken. If you're lucky, it's broken in a way which doesn't affect the output you care about... how lucky do you feel? I recommend you make liberal use of libraries such as 
 (Python standard library) and 
 (a helpful third-party package). Make sure to check out the 
 module for a powerful way to divide your testing into small, fast portions. Every time you find a bug, add a new test to make sure it never comes back. Never merge any new change until it passes all of your tests!
Finally, one of the biggest lessons I've learned from my experience with building the Civis modeling library is that sometimes you just have to go back and re-write. When you're building a new software library, it's not always clear at first how all of the parts are going to fit together. Moreover, you sometimes make decisions which are the easiest solution to your immediate problem, but which make future work more difficult. If your software is something which will only be used once, or never changed, that's not a problem. But if you want your software to be reusable, extensible, and easy to maintain, sometimes you've got to change what you actually wrote to what you wish you'd written.
And don't forget to check out all of the other wonderful talks and tutorials on the 
.Earlier this summer, 
 Civis - the data science platform. Our end-to-end, ready-to-use data science platform in the cloud enables organizations to make data-driven decisions. It's what we use for our work, and we're excited for it to be available for all organizations to use the platform with their data.
Many of you have been working with our team of data scientists, solving data science problems on your own and using Civis Console to view reports, share findings with your team, and put the power of your data to work. 
 is our next generation platform, that gives all users the ability to do the entire data science process from end-to-end in the cloud. You can unify your data into one one place, use matching algorithms to make use of all your data, make predictions with machine learning that lead to data-driven decisions.
While many of you are familiar with the power of our platform, we wanted to share some of our favorite features in Civis that weren't available in Console.
For our current clients, we're starting to upgrade you to the new platform - don't worry; it's a quick and easy process - and you'll be hearing from our support team in the coming weeks. You can also reach us at 
 if you have any questions.
If you are interested in learning more about Civis to help your organization make smarter, data-driven decisions, you can 
.To me, an important part of being an effective engineer is utilizing the best tool for the job. Most times you'll need to orchestrate a bunch of them together. At Civis Analytics, we use Ansible for a myriad of dissimilar configuration management tasks: spin up AWS CloudFormation templates, install application dependencies, configure AWS CloudWatch alarms, etc. We've been using Ansible for a while; we're quite happy with it and have contributed back to the project. To authenticate Ansible to AWS, you can use an AWS service called Identity and Access Management (IAM). This service provides role-based access controls from one centralized console. Recently, we adopted a new-ish AWS IAM Best Practice: Use 
.
Using Multiple AWS Accounts for different kinds of resources is becoming an increasingly popular option, and AWS is really embracing it. While having multiple accounts completely isolates your different environments and resources, it also presents an administrative problem: how to distribute and control access. You don't want to manage multiple users for the same person in different accounts, nor do you want to give out shared credentials. IAM provides the usual swath of access control objects: Users, Groups, Roles and Policies. Users can have their own permissions or 'assume' a role and temporarily gain authorization to access whatever that role can. Roles can be used by users from different accounts, and multiple users can assume the same role.
Now, the federated access that IAM roles provide are temporary. The credentials expires a short time after they are requested. How does this work? When requested by an authorized user, the Security Token Service (STS) creates temporary IAM credentials that the user can send with subsequent requests to utilize those elevated permissions. STS also allows the use of Multi-Factor Authentication.
For Civis Analytics, using IAM Roles made a heck of a lot of sense, but we had to crush a few implementation challenges. We had multiple codebases that were not designed to perform a role assumption on their own. We figured the best way to utilize IAM Roles would be to feed the temporary credentials in via an already in-place mechanism: environment variables. Many of our AWS calls use the Boto library, the Python-flavored SDK for AWS. Boto looks in a few places for AWS credentials and looks through these in a defined order. It checks the environment variables first. If we set our new keys there, we can guarantee Boto will use those credentials. It's important to note that a lot of other tools follow this same paradigm (aws cli, ruby aws sdk, etc).
So, we use the AWS CLI to assume a role. After successful completion, a new set of credentials are returned to you: A SecretAccessKey, SessionToken, and an AccessKeyID. You must then manually export these into your environment. If you used the method described in the above paragraph you also need to perform some manual cleanup in the form of removing the temp credentials when they have expired.
That gets tedious fast, simply because the keys are ephemeral. So the obvious choice was to script this; to have a single command make the call, parse and export the result to the environment. Alongside that, the script needs to handle two other cases: expired role credentials and the potential overwriting of original credentials. The latter is due to the ability to store your credentials in environment variables in the first place, which would be overwritten by our process.
As I mentioned earlier, we've open sourced the script and it is now available on 
. Please give it a try, and open an issue or pull request if you have suggestions on how we can improve it.Recently, I had the opportunity to speak at Data Science Pop-up Chicago where I was able to share insights on some of the work we do here at Civis to solve social problems.
Our history at Civis starts with the 2012 Obama campaign and I took a look at some of the learnings from the campaign to share how we apply individual-level predictive modeling to different organizations. In the below video, I outline a few short case studies across healthcare, non-profits and universities to showcase how data can be used for social good. Enjoy!We've had a great week here at Civis. Since Monday, we celebrated our 
, made a splash at 
, spoke at the Amazon Web Services summit, and launched the new 
, an easy-to-use, end-to-end, powerful and customizable, data science platform in the Amazon cloud.
 had this to say about our launch:
With the fanfare of the launch, the summit, and the media coverage starting to simmer down, our 
 are excited to get back to work democratizing data science and making the 
 even better. Read the 
 or 
.Here at Civis, we're very interested in studying how events shape public opinion. Survey research is one great way to track public discourse, but ambient data from social media provides an excellent look at the organic conversation around an issue or event. We've been using Twitter to track US discussion on access to healthcare for months now, and the overall conversation seemed to be shifting in a productive direction, toward jobs and implementation and away from anti-administration partisan bickering.
Last Thursday our Twitter feeds went into overdrive over the Supreme Court ruling on the Affordable Care Act (ACA), which upheld ACA authorized federal tax credits for Americans in all 50 states, not just those living in states with their own healthcare exchanges. It inspired us to dig in and see what this meant for the trends we'd observed.
We've been tracking the conversation around healthcare for almost a year now, with a focus on distinct tweets in English and only in the US. Our analysis focused on two main points: what conversations are happening about healthcare, and which communities are participating in each discussion.
We found that there were several distinct conversations happening around healthcare. Here are the topics we found:
To understand the conversation, we sampled millions of tweets from the Twitter firehose that met basic search criteria (keywords and phrases) we considered part of the healthcare discussion. We excluded retweets from the set, and retained only independently-authored material. In order to make that giant set of unstructured data more manageable, we used a technique called 
 to learn what topics are discussed most frequently. LDA learns a set of human-interpretable topics (technically, frequency distributions over words) that can be used for high-level analysis and visualization. We still need to assign human-readable labels for each topic once they are learned, but this is typically easy to do by looking at the words and tweets most strongly associated with a topic.
In early June, prior to the announcement of the Supreme Court's ACA announcement, the partisan discussion had increasingly moved towards a focus on the growth of healthcare-related jobs. The Healthcare Jobs topic had grown to about a quarter of all Twitter activity on the issue, up from around 10\% at the beginning of the year.  It looked like we might be on our way to recognizing that ACA was not such a job-killer after all!
The second thing we wanted to understand was which distinct communities were participating in the conversation. To do this, we built communities using the 
. In simple terms, this approach looks at the follower relationships across the full spectrum of Twitter handles and clusters them into distinct groups. We found that there are four groups participating in this conversation: Centrists, Healthcare Thought Leaders, Progressives, and Conservatives. Conservatives are 'farther away' and more isolated  from the rest of the conversation than all of the other groups. Here's what the communities look like:
Well, one significant way the Twitter conversation around healthcare changed after the ACA decision was that it became a lot more active - no surprise there! On the average day in the first half of 2015, we saw about 18,000 tweets per day on healthcare; on June 25th, we saw a spike of 130,000 tweets on healthcare.
But the increase in volume itself is not particularly remarkable or interesting. We also wanted to know how the topics that were being discussed in the wake of the decision had changed. To that end, we constructed a daily time series of the activity on our LDA topics. This graph shows the relative activity of each of our seven topics over the previous ten days.
To complement this, we looked at the relative share of the conversation each topic occupied and saw that the share of the conversation about healthcare jobs had gone way down. It was replaced by partisan commentary about the ACA and the Supreme Court ruling.
It certainly looks like something is going on here, given the tails on many of these graphs after the June 25 announcement. We can validate this by using a method called Bayesian change point analysis to pick out those words and topics showing a marked change in distribution during the time in question. Our initial suspicions were confirmed, with big losses for words like '\#healthcare' and '\#hiring' and big gains for 'Obamacare' and 'SCOTUS.' We also see clear change points in the conversation for the following discussion topics:
So it seems that the big shift in the conversation following the ruling was a significant drop in the share of postings about healthcare jobs and personal healthcare issues. These were replaced by an increase in tweets about the Supreme Court ruling (obviously) and a rehashing of vitriolic rhetoric around 'Obamacare.'
The next question we wanted to answer was whether or not this change in the conversation has led to a regression to partisan bickering. While it's too early to tell if any change is going to last, we can certainly look at the difference between today and what had been going on recently.
To do this, we decided to take a look at sentiment around healthcare tweets. What we found was disheartening, albeit unsurprising. The overall sentiment of healthcare tweets began plummeting on the day of the announcement.
This drop in sentiment is largely due to a change in the dialogue from talk about implementation and employment opportunities to political partisans complaining about the ACA.
To further hammer this point home, we can look at the standard deviation on the sentiment, which skyrocketed. This means that the sentiment for the average tweet got further away from the mean. For non-statisticians, that means that people weren't converging around good ideas, but moving farther away from each other as time went on.
Finally, we wanted to take a deeper look into who was participating in the discussion so we returned to the communities of users that we identified using the Louvain algorithm. Similar to what we saw on the content side, we found that participation increased among political partisans and decreased among healthcare thought leaders. The table below describes the change by group:
On Twitter, while the bulk of the discussion continues to be driven by healthcare domain specialists, political factions have ramped up their activity.
For fun, here is a list of the terms most strongly associated with each community's tweets in the day following the ACA announcement.  Deciphering the favorite hashtags of each group is left as an exercise for the reader...
In many ways, the conversation last week represented a regression, with political partisans drowning out a healthy discussion about economic opportunity under the new ACA. Before the SCOTUS ruling, the discussion had been focused on job prospects for nurses and other health-care professionals. Thought leaders were engaged in discussions about where the health-care market was headed. That moment waned, and for now, we're witnessing the same rhetoric we heard in the summer of town-hall meetings in 2009.
*This post is co-authored with Derrick Higgins.Back in February, I got the chance to speak at O'Reilly's 
 about some of the unique data science work that we do here at Civis.  We recently got a video of the talk and thought this would be a great time to share it.
Over the last year, we've spent a good chunk of time thinking about the ways in which data scientists and social scientists can work together.  While we don't always think the same way, use the same tools, or even speak the same mathematical languages, here at Civis we're determined to leverage the strengths of both disciplines to understand, predict, and change human behavior for the better.  In my talk, I go through some of the challenges and benefits of data science/social science collaborations and share some success stories from our projects.  Here's the video, with a quick summary from the abstract below.  Enjoy!Data sets are growing rapidly. But in spite of this abundance of data, most organizations are still struggling to leverage all of it to understand their customers, figure out new markets, build their operations, and more. Data is the key to making these decisions effectively, but that data is buried deep in dozens of unconnected systems. There's too much latency as data moves from point to point. Teams are stuck working at the lowest common denominator of their infrastructure. Resources and time are limited, but teams everywhere are struggling to use this incredible resource to become truly data-driven. It's a mess.
The cutting-edge data science that Civis Analytics' clients need in order to run their organizations smarter involves huge datasets, computationally-intensive algorithms, and breakneck timelines. So when Civis Analytics set out to build the tools underneath our data science product and services offerings, we had to have a data warehouse solution that could handle everything needed for advanced data science now and in the future. And importantly, we knew we needed it in the cloud so that it could serve both the largest and smallest organizations out there and so that on-demand pricing could enable Civis and its customers to experiment with new ideas without having the hurdle of sunk costs. We found what we needed when we selected Amazon Redshift as the lynchpin of our technology, supplemented with EC2, DynamoDB, EMR and the other products available through 
Traditional, legacy data warehouses require significant time, specialized knowledge, and substantial resources to administer. The cost of building and maintaining self-managed, on-premise data warehouses is very high. But Amazon Redshift is a fully-managed data warehouse solution in the cloud built for analytics, so it's fast, simple, scalable, and cost effective.  As Civis expands, we easily bring up new Redshift databases in matters of hours. AWS services like DynamoDB, RDS, and EMR are also critical tools in our toolbox - whether we need quickly process transactions, scan giant amounts of data, or leverage computing power to run some of our most complex modeling algorithms.
The technology foundation that AWS provides allows Civis Analytics to put powerful, data-driven decision-making into the hands of our customers. We're bringing data science to the world with a robust offering of applied data science services layered on Civis' rapidly growing set of powerful technology products.  First, Civis Analytics helps customers to unify and normalize their disparate data sources, simplifying and automating data housekeeping. We then help teams to put their data science skills to work building powerful predictive models, collaborating around data as a unified team, and using that collective insight to make strong data driven decisions which drive their organization forward.
Amazon's inexpensive and nimble data and computing solutions simultaneously have the power and scalability to tackle some of the world's most complex data-wrangling challenges. AWS has provided us with the wide variety of services Civis needs to do great data science and we have assembled them into a data science platform for both startups and enterprises alike. That's why we're proud to be 
 partners.

We are so happy to announce our partnership with 
 today. We'll explore how the work Civis does can integrate in to Discovery's global portfolio of television networks.
Our teams are joining forces at a time when data and new technology continue to change the way viewers consume content. Together, we will interpret more accurately rapidly-evolving viewership patterns and better inform marketing, media, development and scheduling decisions for all of Discovery's networks.
From the press release:
Read the 
. 
.People at Civis come from a variety of backgrounds, and several of us left academia to work here. Myself, I earned a PhD in physics from 
, then spent four years doing cutting-edge cosmology research at the 
 before coming to work with Civis. I still love physics, but coming to work at Civis was definitely the right decision for me. The data science research at Civis combines the best features of academia with the best features of industry.
When I started at Civis, I was surprised at how similar my day-to-day work experience felt to the academic work environment that I was used to. In both cases, you're surrounded by really smart people working on the same (or related) projects. People are friendly and helpful. The most important thing about your work is the quality of the final product, not which specific hours you're sitting at your desk. There's even freedom to shape your own research agenda here - possibly more or less freedom than you would have in academia (much of one's experience in academia depends on one's faculty advisor / research group and/or grant status).
Of course, there are a lot of differences from academia as well. The output is certainly different. At the University of Chicago, the goal of my work was to learn and publish new and interesting information about the Universe. Here at Civis, we're solving data science problems for all sorts of organizations, and the more practical aspect of the work here is a big plus for me. Another of the differences that I noticed was a different standard of openness. In academic research, I was used to being able to talk about nearly everything that my group was doing. In private industry, you can't be quite as open, both because clients usually don't want their business to be public knowledge, and because knowledge of what we're doing and how we're doing it is Civis's biggest asset.
Another similarity between Civis and the academy is the meta-nature of the challenges. I'd say the biggest challenge we face here is the same that one encounters in any knowledge work: figuring out which question to answer. There's a dozen different projects that I could be working on right now, but which is the most useful or most urgent? One of the great things about working at a small and growing company like Civis is that everyone here has a lot of opportunities to shape the future of the company, and picking which questions to work on is a big part of that.
Many of the skills and habits that I learned in academia transferred directly to my work at Civis. For example, I mentioned that the biggest challenge in this work is deciding which question to answer. You've also got to make sure that questions are framed in the right way. Is it clear what would count as an answer to the question? Would the answer be helpful for Civis, or for our clients?
Once you have the right question, it's not enough just to find the answer to a question. You've got to test your answer to make sure it's right. If your model works on one set of data, great! How about different data? Does it handle data that's very different from what you used to create it?
Finally, once you have a solution, does it apply more broadly than the initial question? As much as possible, we build on past work. It's good to be able to solve one problem, but better to be able to generalize that solution to future problems. As part of that process, we build up a store of software which is accessible to everyone at the company. All the code that we've written in the past is available to everyone who works here to help them solve problems in the future.
Civis also makes employee education a priority. We all learn a lot just doing our "ordinary" jobs, but beyond that, we put part of our time into learning new skills that we're interested in. For example, I'm studying neural networks, because it's not something I've had the chance to use before, but something that I think is really cool. We even have weekly journal club meetings where we discuss new research in statistics and machine learning, an event which will be familiar to anyone with an academic background.
I think one of my coworkers put it best when I asked him a similar question in my interview. He told me that working at Civis is what working in academia should be like, and I agree.Our founder, Dan Wagner, was recently featured at 
 in a 
 with Amazon CTO, 
. Watch the full video, or read an excerpt below:
: "So do you actually build stuff?"
: "We do!"
: "So tell us the kinds of things that you build?"
: "We left the campaign immediately. The lights went off, the curtains came down and we were 15 kids clutching our Macs walking down the street. And we began to get introduced to these companies that had a generic set of problems: an [...] IT implementation over here [...] and the decision making apparatus of that company over here, powered by spreadsheets and conditional formatting. There was a vast distance between these two things.
As we began to get introduced to these companies, they said, "well, how do we apply the same type of expertise at our company?" A college president approached me to say, "We are dealing with 40\% attrition of kids that are dropping out. How can we figure out who these kids are ahead of time, build a predictive model that might say who they are, and then build intervention programs?"
The exercise in terms of the data goes beyond just a prediction: you have to pull all the data into one central place, you have to clean it, you have to impute for missing values, you have to match all the databases using record linkage from different spots, you have to run a linear and non-linear algorithm to predict who might drop out in the future, validate it on data, score the data set, provide it back to the university, run the intervention and see what what happens. 
 is software.
But you can't do a lot of that stuff with the on-premise IT available at big organizations. We began by implementing some basic ETL software, pulling the data into Redshift, aggregating the data and running basic linear models.  Over time, we've added functionality to our platform and made it available to these companies."Last week, after 
 about our work on predicting the uninsured went out the Upshot posed an interesting question to Civis and Enroll: could we run a simulation wherein all states expanded Medicaid?
To understand how we did this, it helps to take a step back to understand a bit about our methodology. We interviewed thousands of people in 2013 and 2014 to ask them about their insurance status. In the last two years, we've analyzed hundreds of characteristics to confirm if there was or was not a mathematical relationship between possessing that characteristic and being uninsured. Out of that process, we have come up with the ~40 or so that gave us the best prediction. Many of these characteristics make immediate sense - age - while some of them are more surprising - such as whether or not the person had voted in a recent Midterm election, or the employment status of their spouse.
One of those important variables, not surprisingly, was Medicaid. By analyzing the survey responses, we found that individuals living in a Medicaid expansion state were statistically more likely to say that they had insurance, even when controlling for other factors such as income, age, and gender. The math also told us roughly the size of the difference.
To re-run the simulation, we just applied that 'difference' to people in every state, without changing anything else. In essence, what we are saying is: if everything about a person was exactly the same, but they happened to have Medicaid in the place they lived, how would that change the likelihood that that particular individual had insurance? We then rolled those numbers up from individuals to counties and states.
As the 
 mentions, this is a thought experiment. We can only take past data on how Medicaid was expanded as our basis. But we know that Medicaid was not expanded randomly, and there are other things correlated with whether or not a state expanded Medicaid, and how successful that expansion might be, that we have not - and cannot - adjust for. So while the specifics are estimates the overall picture is clear: Medicaid made a significant difference.
Want to learn more about our work in predicting insurance? 
. Please also take a look at 
 section to learn more about individual-level predictions we've done in other industries. And, of course, we are always hiring: 
 you'd like to work with us solving interesting problems with data and analytics.Last Wednesday, Annie and I headed up to Carnegie Mellon University (my alma mater! Go Tartans!) for the 
 to find the next batch of Cives. Because we have a tendency to (over)analyze any data we can find, here are some interesting facts about our trip.
According to CMU's Career and Professional Development Center, 400 students registered for the fair. As students came up to Civis' table, they could either approach Annie or me, and almost everyone with whom we spoke left a resume, which was helpful for  data-collection.
Because Civis is committed to building a community of 
, we were curious about how two women of color would be received. While we'd prefer to run an experiment in which we randomly assigned who got sent to which fair, small sample sizes make that sort of thing hard to analyze; instead, we're just going to offer some descriptive statistics and make some semi-speculative leaps.
Much to our delight, we saw a rough parity in the number of female and male candidates, which matched the composition of the school.
The applicants we received were substantially more Asian than the school, which was notable in part because Annie and I are both Asian women-but we're not sure how much of that was us versus the composition of the fair.
Indeed, the most interesting thing we learned wasn't actually about how effective Annie and I were together; it was about how different the two of us were. For example, here's the breakdown of resumes we received by recruiter.
I'd like to point out, for the record, that I actually missed 30 minutes of the fair (12.5\%) to have lunch with a professor, so we're not exactly apples-to-apples here. But even after you make that adjustment, Annie still talked to many more people than I did.
But I wasn't just standing around! In fact, we suspect that my conversations were, on average, longer than Annie's because students had more questions for me. Because I'm a CMU alumna, I got a special badge, and lots of students were particularly interested in how my time at CMU prepared me for Civis. Those were questions that Annie was never asked.
If we dig into this a little more, we see that not only did Annie and I talk to different numbers of people, we talked to different types of people.
Whereas Annie talked to twice as many men as women, I talked to twice as many women as men, and whereas Annie spoke mostly to undergraduates, I spoke mostly to graduate students. Maybe this was because Annie looked a little younger, so undergrads felt more comfortable approaching her. Or maybe because I was a female alum, women felt more comfortable talking to me.
Finally, a note on homophily, which refers to the tendency of similar individuals to cluster together: this explains how most of your friends are probably in the 
 and why you're more likely to 
. Homophily got a bit of a moment in the sun after researchers Fowler and Christakis published a paper on how 
: researchers who re-examined their work argued that the "contagion" effect, which Fowler and Christakis argued 
 weight gain, could be 
.
Annie and I may have another data point to add to this debate:
I work in the Data Science R\&D department. Annie works in Applied Data Science. Somehow, both of us managed to convince a disproportionate number of our candidates to express interest in our departments. Was this homophily?--maybe people who were interested in research approached me, and people who were interested in more applied work approached Annie. We're not so sure. We didn't wear or say anything to identify our department. Moreover, we've collaborated on projects before, so we use the same vocabulary and discuss similarly technical subjects (albeit in different contexts). Plus, we recruited (roughly) the same proportion of engineering candidates, so it wasn't as if one of us seemed significantly more technical than the other.
So what's going on? Our working theory is that we just both love our jobs and find it easy to get other people interested in our jobs too.
If you're interested in working at Civis, you can 
. Both Annie Wang and I would be happy to talk to you (though, statistically, it appears I'm less approachable).When it comes to tattoos, Florida's male inmates are five times more likely to get a dog tattoo than their female counterparts. Female inmates prefer cats two to one over men. I know this because one of the great things about working for Civis is finding and working with interesting datasets and a favorite of mine has been from the 
 which keeps public records about various offenders. One bit of information that is recorded is tattoos, and the availability of such a great dataset deserved a closer look. Below is a graph of the popularity vs gender ratios of common animals (including some mythical) in tattoos.
Because there are many more men than women in the dataset, an equal weight is given to both groups, so averaging the popularity between men and women we see that dragons are the most popular (mythical) animal appearing in tattoos. As far as non-mythical beasts, the state animal, the panther, is well represented, coming in as the second most common tattoo animal.
However the graph above does leave out one animal. Valuing popularity of tattoos equally between men and women, by far the most popular animals to appear in tattoos are butterflies. Nearly 15\% of women with a recorded tattoo have butterfly tattoos. Despite its popularity with women, it is not a big hit among men. The female to male ratio for butterfly tattoos is nearly 32:1. Here is the same graph including butterflies.Here at Civis, we love models. But not the ones that wear swimsuits or walk down runways: the kind of curves we care about are mathematical representations of human behavior, and we employ models to predict everything from political beliefs to consumer preferences.
Imagine that it's your friend's birthday, but you don't know what present to buy for her.
If you're anything like us, you might first try to find data on what gifts other adults like and, from that, make a prediction about what your friend will like. A simple way to do that is through a cross-tabulation, which compares responses across two or more dimensions:
While cross-tabs are great for comparing a few dimensions, they become overwhelming if you have dozens. (What if preferences differ by age, marital status, region, income, and occupation?) More importantly, once you add many dimensions, how can you make sure that there are enough people in each combination of dimensions to be confident about your prediction?
Instead of having to interpret many complicated crosstabs, we can use models to give us estimates of how likely someone will prefer a given gift. More flexible and precise than crosstabs, models take into account hundreds of dimensions at once and reveal hidden relationships across them.  When you build a cross-tab, you have to select exactly which dimensions you will analyze; by contrast, models rely on a combination of human and machine intelligence to "pick out" which characteristics have the strongest relationship to the outcome of interest (in our case, someone's preferred gifts) and leverage mathematical relationships to generate predictions from those characteristics from them.
One of our clients is a company that connects consumers with green energy providers. Recently, they asked us to help them build a model to predict how likely someone is to sign up for clean energy if asked. The company had a huge mailing list, and prior experience had shown that the vast majority of people who received their mail weren't going to sign up. In a world of limited resources, how could they identify the most likely people to sign up and make sure to reach out to them?
That's where modeling comes in. Looking at records from past mail campaigns, Civis analyzed which people were most likely to respond to their mailings. The idea is that if we can identify patterns in the data we have about who responds, we can expand them to predict future responses.
A conventional approach towards this problem is to segment existing data into groups by age, gender, or other demographic information and then look at response rates across those groups. That can be a good first step, but to build a more precise model, we want to consider a greater number of characteristics. In our case, we looked at academic and business literature to see what traits might be associated with energy preferences. This led us to gather data on everything from weather patterns to average commute time to home ownership in addition to the typical demographic data. As we gathered this data, we also had to clean all of it and conduct checks to make sure the data was captured accurately.
Once we had the data, we used our custom-built software to construct a few different models and compare their performances. In the end, our best models confounded some of our initial intuitions: for example, we were pretty sure that household income would be related to energy preferences, but we found out this relationship changes based on where you live. Also, it's not just about household income: education and marital status also play a large role. Our final model leveraged these insights and more.
After we had a model, we applied it to all potential mail targets and created a score that assigned them each a likelihood of responding to a future mailing. Then, we divided the mailing list we sent to our client into 10 groups based on how likely people were to respond: Group 1 were the people were predicted were least likely to respond and Group 10 were the most likely. By sorting their list with the model scores, our client could prioritize their top targets.
Working with our client, we analyzed the results of our model by sending mail and recording the response rates. We didn't send mail to every group (we're just testing, after all), but we did send mail to five:
Exactly as our model predicted, the individuals in Group 10 responded at the highest rate--double those in Group 8 and five times those in Group 4 (unfortunately, we can't show the actual response rates, but rest assured that our bar graph starts at 0!).
This next plot shows how much higher the response rate is compared to the expected response rate of a "control" list that any company might use. Our top decile responded about 3 times more often than the recipients that were in the control list.
Ultimately, our model helped the renewable energy company identify those most likely to respond to their mail and, indirectly, promote the use of clean energy in the US! Another victory for modeling!We recently said goodbye to our inaugural class of summer Interns and Fellows here at Civis.  The class of '14 worked hard with our Engineering, Data Science R\&D, Applied Data Science, and Data Research teams helping solve tough problems for non-profits, political campaigns, and companies.
We accepted students from all over the country, ranging from rising juniors to completed PHDs and representing 13 different universities: Brown, Michigan, Yale, Penn, Loyola Chicago, Harvard, Northwestern, George Mason, Georgetown, Lake Forest, Claremont McKenna, Florida International, and NYU.
Our team spent their days building models, developing and deploying features for our software tools, analyzing massive datasets, roving the office for good snacks, and of course, going to baseball games.
We brought everyone up to speed with an early crash-course on SQL - care of our many teachers in Data Research - then got to work. Each of our interns logged hours of training on Python, R, Git, and Unix shell scripting. Interns and fellows were given  substantive work along with the trust and space to work on projects that challenged them.
We'll miss this summer's team but we're excited to welcome our next class of interns this Fall, Spring and Summer. We have open positions for the following roles:
If you are local to Chicago or Washington, DC, we're currently accepting applications for Fall and Spring. Internships for Summer '15 are open to applicants across the country and will take place in either Chicago or Washington, DC.
Read more about our programs and submit your application 
.Piaggio's Question: What combination of attributes in our ads will positively impact the perception of our brand? Our survey design helped the scooter-maker identify the right ads for distinct consumers.
, the makers of Moto Guzzi and Vespa, needed to identify the best online ad but testing Piaggio's desired combinations using traditional A/B testing was too difficult.
In regular A/B testing, clients can only test simple comparisons. Civis Analytics used conjoint analysis to test the full range of options Piaggio's creative team developed - from color of scooter to message and background - against the full set of their desired consumers.
Instead of delivering one winning ad, Civis was also able to assign winning content by consumer segment. Piaggio implemented a data-driven ad campaign that told a story driven by consumers instead of gut feeling.
For the uninitiated, conjoint analysis looks something like this:
In practice, we were able to test, via surveys, the following advertisements with both scooter owners and prospective scooter owners.
Good survey design leads to impactful conclusions. Not only did our survey give Piaggio a winning ad "overall," but we were also able to assign winning content by consumer segment:
While affordability was the most stable feature, appealing to prospective owners ("new leads") and current owners alike, this matrix of features allowed Piaggio to deliver the best ads to consumers based on preference.
Beyond allowing us to test these combinations, our survey design linked stated ad preference with an action: in this case, "test drive a Vespa" at a local dealership. This was our answer to a possible discrepancy between stated preference and revealed preference. Piaggio was able to trace customer preference for an online ad to their most desired offline outcome: a customer in one of their brick-and-mortar stores.Here at Civis we like to use the best tool for the job, so integrating different tools and stacks is essential.
Docker is the tool-du-jour for portable code.  It allows us to maintain a rich environment for statistical programming, and easily call that environment's executables from our webapp.
Props to Docker for a great online tutorial. 
. It's a good way to get your hands dirty, but alas, it's not real.  If you're looking to take the next step, try following the example I've shared below.
In this example, we'll:
If you're on Linux, installation should be straightforward.  I'm on Mac OS X and that's pretty easy too.  Installation instructions can be found on Docker's website.  Mac users will also need to install boot2docker, a lightweight virtual machine.
Check your installation by running:
As explained in the installation instructions, this downloads the hello-world image and creates a bare-bones container.
I want to run a simple app that demonstrates random incidence via the 
.  Rather than writing that from scratch, I found a GitHub repo 
 that almost does what I want.
I do want to make a few changes.  Namely:
To do so, I forked the repository in GitHub.  These changes aren't really the point, but you can view them 
.
clone the repository:
First, check to see your current system python version:
Mine is 2.7.6 and I want my script to run in Python 3.  An easy way to do this is to pull a pre-built image that already has what you need.  Shared images can be found on 
.  (You'll need to create an account to use Dockerhub, or you can use your GitHub account).
On Dockerhub, I searched 'python' and found 
.  That seems useful, but before I set that up, try:
Python isn't installed on the hello-world image.  Now pull a docker image that does have Python 3.
Now I can run python in the container from that image.  For example:
This command says "run the docker image called 'python' and in that container execute the command 'python -version'."  You can see that the container has Python 3, while my own machine still has Python 2.
The 'python' image also has a number of other basic utilities in it, so we can get a feel of what's there.  'ls' shows my file system's directory,
shows the directories for the python container.
There are a few ways to get your Python code to run in the 'python' container.  One is to mount a local directory to that image, but like most of our Dev team here at Civis, I'm on OS X.  There's an incompatibility there - in short "bind mount can't work on a remote machine, so if your client is on OS X and your daemon on boot2docker, -v /host/some/path:/container/some/path will look for /host/some/path on the boot2docker host, not you mac." (see 
 for more information).
Instead, we'll build a container that has our Python code in it.  To do so, we'll first create a Dockerfile, which contains a series of commands to build the container.
From your git Python repo, make a Dockerfile like this:
and save it as "Dockerfile"
This creates a new docker image, starting from the 'python' image.  It copies current dir into /src for the image, then ???
The -t flag supplies a tag for the resulting image.  Without it, you'll get an image id.
Remember to make sure 'DOCKER HOST' is set in your environment, as specified in the Docker installation instructions.
Let's test drive our new container.
Good.  Our new container has Python 3.  And now, let's run birthday.py in the new container, supplying arguments (num\_people, num\_successful\_trials):Here at Civis, we like to have fun.  And what's more fun than coming up with new and creative ways to measure pi?  For our recent one-year anniversary party, we hosted a 
 in our shiny new office. Joerg Rings and I decided to use the occasion to teach people about a common computational technique called the Monte Carlo method by measuring pi with a smartphone, some computer vision code, a shoebox, and a handful of lentils. Confused? Read on.
Imagine that we stick a circle inside of a square that is just big enough to hold it:
If you brush away the cobwebs from that dusty part of the brain where high school geometry resides, you might remember that pi captures the relationship between the area of a circle
and its radius:
Similarly, the area of a square is given by the square of the length of its sides:
Putting these two equations together, we get a simple formula for pi:
But how do we measure the area of the circle and the square if we can't use a ruler and those two formulas?  Here's where we turn to Monte Carlo methods.
 make clever use of random numbers to do calculations that would otherwise be quite difficult. John von Neumann and Stanislaw Ulam originally invented them during World War II to simulate the scattering of neutrons in radiation shielding materials (Ulam's uncle was a gambler who liked to frequent the Monte Carlo Casino in Monaco, which gave these methods their name). Instead of solving the difficult equations of motion of the neutrons, they simulated their movement by picking random numbers and shifting the motion of the neutron each time it was supposed to scatter. Monte Carlo methods are used everywhere in the sciences, and we put them to work every single day for the Bayesian statistical work we do to forecast elections.
In our case, we want to calculate the ratio of the areas of the square and the circle. Let's imagine that we had a handful of pebbles and we scattered them randomly on top of our diagram above. A bunch would land in the circle, and a bunch would land in the square. Because the square is bigger, you'd expect more to end up inside the square than inside the circle. By simply counting the number of items that land inside the square and then counting the number that land inside the circle and taking their ratio, we should get a pretty decent estimate of the ratio of the areas. From our formula above, that should be 
. And all we had to do was use a little randomness and simple counting.
Scrounging around the office, we came across an old shoebox and a package of dried lentils.  Perfect.  We threw a handful of lentils into the shoebox, put the lid on, and shook it up.  Since we're way too lazy to count things by hand, we took a photo of the scattered lentils with a smartphone, sent it to a computer, and then taught the computer to find and count the lentils with python and the 
 library.
The python code loads the image and then runs a 
 that automatically picks out circles in the image and figures out if they're inside the circle or the square.  After tweaking the code a bit, this is what we got:
The lentils outlined in green are inside the circle, the ones in blue are inside the square but not the circle, and the ones in red are outside of both.  Taking the ratio, we estimate pi as 3.591, which is off by around 14\%.  Repeating the process many times throughout the course of the science fair and averaging the results, we ended up, at the end of the night, with an estimate of 3.180, just 1\% from the true value.  Not too shabby.
Take a look at the source code 
, or download our 
 yourself and give it a try.  And if you're really interested, read some more about 
, another Monte Carlo method for estimating pi that was first formulated in 1777.Data scientists have to handle data from many different sources. Many of these sources however aren't very useful until they can be combined together (list of potential customers, volunteer signups at a campaign rally, public records, etc). When an organization is trying to understand people, it needs all the information it has about individuals linked together in a single record.
Often, we have one canonical data source - a voter file, or customer base - and need to match new data. We have to do be able to do this very quickly for two different use cases. First, for web apps that may need to look up a users' friends in real-time, and second for matching massive new datasets to our existing ones. We also need this capacity to be scalable - we're not running millions of records through 24/7, but when we are we want it done quickly.
This is a classic problem of record linkage. Besides the problem of how to decide if a pair of records are identical, the main challenge is that it is prohibitive to exhaustively compare all pairs. In order to find links in a reasonable amount of time, we need to quickly limit our search space. The most effective way to do this is by blocking: grouping records together based on a rough summary (a token). There are a number of sophisticated ways to generate these tokens. This IEEE 
 by Prof Peter Christen provides a useful survey of some of the newer methods. Clearly, there is a tradeoff between constraining the number of records returned (Pair Quality), and ensuring that the best-matching record is actually returned (Pair Completeness). Notably, he finds that traditional blocking performs well for person-based data. While some methods have slightly higher completeness, they suffer greatly in Pair Quality.
We chose Traditional Blocking for its balance of performance and simplicity. In order to improve Pair Completeness, we create entries in multiple blocks for each record, based on different combinations of fields. Some examples:
If too many records share a token, we don't bother storing it. If we don't have more specific information when we're trying to match a record, we won't be able to distinguish between them anyway. On the other hand, for the tokens which are unique, we can find that record even if other information has changed...for example, a person with a relatively unique name regardless of where they have moved in the country.
This blocking narrows our search from the entire US population down to, on average, a couple hundred records. We then use a custom algorithm employing 
 and field-specific probabilities, derived from the population, to select the best possible record or determine that there is no match.
While we use a lot of Ruby at Civis, it was clear that we would need something faster for this application. Based on its reputation as a fast, programmer-friendly language, we chose Go. Golang is built for speed, and since we started with go1.1 it's 
 
 
.
While small databases may be stored in-memory, larger record sets require a database. We selected AWS's Dynamo key-value store. It allows us to store >800M records and retrieve any individual one in <3.5ms. The pricing is primarily based on maximum available throughput, which can be adjusted several times per day. While there is no official AWS Go library, there is community has created the 
 has done a good job of adding support. While it's great to have community-based libraries, we've found it important to follow the Go 
 of forking the ones we use. Without a standard library management system (like Ruby's Bundler), it can otherwise be a bit too easy for external library changes to break your code.
Finally, for running the application, we use AWS's Elastic MapReduce (EMR), Amazon's on-demand implementation of 
, the open-source platform for 
 tasks. We use the EMR infrastructure in two parts of our system. First, we use it to transform large record sets into key-value stores. Each record is mapped to several tokens, and then each token is reduced to a list of records. Each of these steps is a Go program. Second, we use it as a simple (if brutish) way to parallelize our bulk match jobs. In the Map/Reduce framework, we effectively just have a Map step (each record is mapped to its best match) with no reduce step. When our clients create a matching job with millions of rows of data, we create temporary clusters to run against the Dynamo stores, and only pay for them until the job is complete.
A few billions rows later, our infrastructure has proven solid. We're continually working on improvements, such as eliminate unnecessary tokens, improving the pairwise scoring of records, incremental dynamo updates, and more efficient storage (ie, go's blob encoding instead of JSON)How can we predict voter outcome to help manage our outreach resources? We used our Golden program to perform 60,000 daily simulations to remove polling biases and keep us as accurate as possible during the 2012 Obama re-election campaign.
Campaign decision makers work in a data rich environment. Internal metrics, consultant pollsters, and public polling inundate campaigns, but reconciling disparate data points into a comprehensive understanding of the political landscape can be a challenge. But uncertainty cannot be a road block: in the absence of clear and consistent information, political decision makers still must act.
Data scientists and analysts, now at Civis Analytics, developed an election forecasting algorithm (code named "Golden") while working for 
 during the 2012 election cycle. The algorithm ingests internal predictive modeling, consultant polling, and public pollster toplines, and in turn generates state-level estimates of candidate support, orderings of relative competitiveness, and an overall likelihood of victory on Election Day.
While much of predictive analytics fixates on the micro-level, some of the most crucial choices decision-makers face are macro-level. In a presidential election, deciding which states in which to invest and the proper mix of resource allocation between them is often the difference between victory and defeat. Rigorous election forecasting helps make sense of divergent data points and leverages uncertainty for more informed decision making.
On October 4th, 2012, the day after the first Presidential debate, the Obama campaign leadership was faced with an unenviable question: after speculation and some unfavorable initial polling following the debate, was the President still on track for victory? Members of the internal Analytics Department, now data scientists and analysts at Civis Analytics, were tasked with answering this question.
Over the previous months, the analytics team had developed an election forecasting algorithm that could reconcile the large number of internal surveys the campaign was conducting with the polling results of external consultant and public pollsters. The "Golden" model had two major facets. First, for each pollster and poll, estimates of partisan bias and expected error were generated. When new polling results were released, the model could efficiently situate the new piece of information in a larger political context. For example, if a pollster with evidence of past Republican bias released a poll showing the President trailing, the model would automatically recognize that the poll may be pointing to a political reality less pessimistic than its topline suggests. As opposed to seeing each new piece of polling information as disjointed and discrete, this information could then contribute to building a holistic image of the race. Second, the "Golden" algorithm would use this base of reconciled public opinion to simulate the Election Day results 62,000 times each evening. These simulations were then interpreted to generate state-by-state estimates of support, rank competitiveness of states, and give an overall likelihood of victory.
This was crucial in October of 2012. After two nights of internal polling, the analytics team found support for the President stabilizing in a winning position. However, public pollsters continued to see declines in support each night their calls were in the field. The algorithm was able to confidently account for these pollster effects and reassure campaign management that the President's strategic position was strong and that there was no need for panic. The above comparison between OFA and Gallup's support estimates contrasts the smooth, stable, and actionable predictions from the analytics team, with the noisy public polling toplines reported in the final weeks of the cycle.
Beyond being a tool for reconciling different measures of public opinion, "Golden" became a core driver of the campaign's resource allocation decisions. When 2012 campaign leadership decided to adopt the analytics method of resource allocation based on the "Golden" algorithm, a principle approached now Civis CEO Dan Wagner and stressed "if we lose this election, a lot of that will be on your shoulders," but that "we're going to trust you, because there is reasoning behind this algorithm and that reasoning makes sense
." In the aftermath of President Obama's re-election victory, campaign manager Jim Messina reflected that the 62,000 simulations of the election run in "Golden" each night was "how I spent the \$1 billion dollars
."
In the first few days after the first Presidential debate (approximately one month before Election Day), the "Golden" algorithm showed stable levels of candidate support and accurately predicted the ultimate outcome of all 50 states. Those forecasts, refreshed and refined daily throughout the rest of the cycle, never presented an alternate view of the electoral landscape. The final "Golden" estimates in the days before the election were within 1.1\% of the actual results in every battleground state.
When compared with other polling aggregation algorithms, the "Golden" algorithm consistently had a lower average error and smaller partisan bias (left). While the algorithm is robust to noisy results from public pollsters, a major driver of the model's success was the internal polling and modeling conducted by the analytics department. For example, the final OFA analytics polling results had an average error of 1.6 points, as compared with Rasmussen's average error of 4.6 points. Here, the ability to center the model's estimate on high-quality polling and modeling proved crucial.
Now at Civis Analytics, the analysts and data scientists responsible for creating the "Golden" forecasting algorithm have worked to adapt and generalize the tool. National political organizations can view their Civis modeling in the context of public polling and other opinion research metrics to help them distribute resources across senate, gubernatorial, and congressional races. Statewide political groups can similarly fine tune resource allocation across state legislative races. The same methods can be used to track and reconcile public opinion on a range of other issues including attitudes towards climate change and education reform.We are a community of engineers, data scientists, and statisticians, and are looking to add to our growing team. We are smart, fun, and a little bit weird. If this sounds like you and you've already submitted an application, here are some tips for next steps in our process.
Wonderful! Our hiring leads review every application. If you're a good fit, you'll be contacted by our Growth Manager with next steps, either a position-specific exam or phone interview. Most of our exams take four hours and are intended to show your approach to work as well as the right answer. If your performance is strong, we'll invite you to Civis in Chicago or DC for an in-person interview. You can meet us in-person, and we can tell you a little more about the work we do.
In terms of the interview, we're interested in your skills and how they apply to Civis: have you worked with data before? How have you used it to solve a real-world problem? We also hope you're familiar with our mission and our work: please review our press clips in advance. Your education and previous work experience should give you all the skills you need.
We believe in quantifying results and use our exams to remove some of the bias inherent in interview processes. We're also eager to find good matches for our open positions - the exams are a big help towards that end.
Our office is casual. Wear anything you'd feel comfortable wearing to work.
If appropriate, you might also consider an internship. We're committed to teaching within our organization and can help you hone your skills.
All candidates who receive offers undergo a standard employment and background check.This past June, we were thrilled to be awarded Best New Startup at the third annual 
.  Our data scientists and software developers were honored to take home the statue.
The Moxies celebrate Chicago's most innovative digital entrepreneurs and are presented each year by Built in Chicago. According to Built in Chicago founder Matt Moog, an incredible 175 new tech companies have been founded in Chicago since last year's awards.  This year's ceremony was sold out, with over 850 attendees gathered to hear the results of more than 360,000 votes.  If you were one of those votes, thank you! Our red horse trophy now greets all visitors to the Civis Analytics home base.This May Civis Analytics celebrated its first birthday! In honor of our raison d'etre--data science--we celebrated by having a grand science fair. Clad in Civis lab coats, staffers led various booths showcasing their analytic prowess in relatable terms.

Senior Data Scientist, Derrick Higgins, asked participants to compete against a computer in predicting words from politicians' speeches.

Here Data Scientist Paul Wyatt demonstrated sampling bias by having guests estimate the weight of candy. Volunteers were asked to choose 5 pieces of candy from a bag of 100, weigh the 5 candies, and then use that measurement to estimate the weight of the entire bag. Routinely, the guesses were too high as they did not account for various sizes of the different candies. Large candies are easy to see and to grab, while small candies fall neglected to the bottom of the bag.

Data Scientist Elaine Lee used the 
 API to map out peoples' routes around the city of Chicago after they pick up their coffee from their favorite coffee shop.

Friend of Civis Chicago Mayor Rahm Emanuel also stopped by. Here Director of Engineering 
 and Data Scientist 
 asked the Mayor who he thought was the worst band in the world. The Mayor offered to include his favorite band instead. Monitors displayed Kelly's Tableau visualization of attendee responses compared to the band's actual popularity ranking, based on data that Jenny scraped from 
's public API.
Senior Data Scientists 
 and Joerg Rings also implemented a Markov chain Monte Carlo method for calculating the value of pi, using Python and image recognition. See 
 for the details on how they did it.

CEO 
 awarded Mayor Emanuel with his own Civis Science Fair ribbon.
Of course, the party wasn't all big data and predictive analysis--DJs spun tunes while guests partook of libations and 
 tacos. 
 also provided some great photo-ops for guests and staffers alike.
